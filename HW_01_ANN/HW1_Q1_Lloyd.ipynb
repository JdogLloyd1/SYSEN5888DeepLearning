{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2827fa83",
      "metadata": {},
      "source": [
        "SYSEN 5888 Spring 2026\n",
        "\n",
        "Jonathan Lloyd\n",
        "\n",
        "Homework 1, Question 1\n",
        "\n",
        "\n",
        "Goal: Build and train an artificial neural network for a binary classification task. \n",
        "\n",
        "Tools: Pytorch\n",
        "\n",
        "Data: The input data and their corresponding binary labels are provided in the data file, hw1data.dat\n",
        "The input data contains 1000 two-dimensional data points that lie within a square of area one. The input data and labels should be loaded by reading the data file using any choice of library. Please note that for binary classification, the -1/1 labels need to be converted into 0/1 labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ff4b273",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (1000, 3)\n",
            "First 5 rows:\n",
            "[[ 0.73662472  0.50544176 -1.        ]\n",
            " [ 0.71066494  0.56503663 -1.        ]\n",
            " [ 0.10533493  0.06889585  1.        ]\n",
            " [ 0.95860447  0.16390308  1.        ]\n",
            " [ 0.42369288  0.51051878 -1.        ]]\n",
            "First 5 rows relabeled:\n",
            "[[0.73662472 0.50544176 0.        ]\n",
            " [0.71066494 0.56503663 0.        ]\n",
            " [0.10533493 0.06889585 1.        ]\n",
            " [0.95860447 0.16390308 1.        ]\n",
            " [0.42369288 0.51051878 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Import data file\n",
        "hw1data = np.loadtxt(\"../HW_01_ANN/hw1data.dat\")\n",
        "\n",
        "# Quick look at the data\n",
        "print(\"Shape:\", hw1data.shape)\n",
        "print(\"First 5 rows:\")\n",
        "print(hw1data[:5])\n",
        "\n",
        "# Convert data labels -1/1 to 0/1\n",
        "# if data label in column 3 (index 2) is -1, reset to 0\n",
        "hw1data[:, 2] = np.where(hw1data[:, 2] == -1, 0, hw1data[:, 2])\n",
        "print(\"First 5 rows relabeled:\")\n",
        "print(hw1data[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c96ccd",
      "metadata": {},
      "source": [
        "Architecture: Define a Sequential model, wherein the layers are stacked sequentially and each layer has exactly one input tensor and one output tensor. Please build an artificial neural network by adding the following layers to the Sequential model using the configuration below.\n",
        "- Input - Shape 2\n",
        "- Dense - Units 5\n",
        "- Dense - Units 1 - Activation Sigmoid\n",
        "\n",
        "The initial random weights of layers can be defined by specifying weight and bias initializers. For each of the above layers, initialize the kernel weights from a Xavier/Glorot uniform distribution and set the random seed to 99. Additionally, initialize the bias vector as a zero vector. The activation function defines the node output given a set of inputs. An appropriate choice of activation function is required to allow the artificial neural network to learn a non-linear pattern. The activation functions for the first dense layer can be chosen from some of the commonly used activation functions like Rectified Linear Unit (ReLU), Hyperbolic Tangent (tanh), and Sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc78a88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Model Class\n",
        "# Architecture: Input(2) -> Dense(5) -> [activation] -> Dense(1) -> Sigmoid -> Output\n",
        "class Q1SequentialModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, input_activations='relu'): # Default ReLU\n",
        "        super(Q1SequentialModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        activations_options = {'relu': nn.ReLU(), 'tanh': nn.Tanh(), 'sigmoid': nn.Sigmoid()}\n",
        "        self.input_act = activations_options[input_activations]\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.output_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.input_act(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.output_act(x)\n",
        "        return x\n",
        "\n",
        "    def _init_weights_biases(self):\n",
        "        # Initialize kernel weights with Xavier/Glorot uniform, Seed=99\n",
        "        torch.manual_seed(99)\n",
        "        for layer in [self.fc1, self.fc2]:\n",
        "            nn.init.xavier_uniform_(layer.weight) \n",
        "            # Initialize biases with zeros\n",
        "            nn.init.zeros_(layer.bias) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd0a8de",
      "metadata": {},
      "source": [
        "Training: The model is compiled by specifying the optimizer, the loss function and metrics to be recorded at each step of the training process. For binary classification, it is a common practice to use binary cross-entropy as loss function. Popular deep learning libraries provide support for several optimization algorithms. Some of them are Stochastic gradient descent (SGD), RMSprop, ADAM. Please choose accuracy as a metric during model compilation. Finally, train the artificial neural network by fitting the input data and labels with each of the aforementioned optimizers and their respective configuration as given in the table below. The neural network should be trained until convergence is achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37235eb2",
      "metadata": {},
      "source": [
        "Deliverables: Please report the training accuracy after the training process is carried out for *every combination* of activation function and optimizer.\n",
        "Plot the loss curves to determine the number of epochs required to achieve convergence.\n",
        "Report the hyperparameter tuning step.\n",
        "Predict and report the binary classification results for the data point [0.8, 0.2] with the trained artificial neural network.\n",
        "Discuss the influence of particular parameters on different optimizers.\n",
        "It is recommended that the final results be reported in a tabular format as shown below. Please also make sure to submit your working code files along with the final results.\n",
        "\n",
        "<table>\n",
        "<thead><tr><th>Optimizer</th><th>Activation function</th><th>Required epochs</th><th>Training accuracy (%)</th><th>Prediction for [0.8, 0.2]</th></tr></thead>\n",
        "<tbody>\n",
        "<tr><td rowspan=\"3\">SGD<br>(Learning rate = 0.01, Momentum = [0.0, 0.1, 0.5, 0.9], discuss the impact of momentum values on the convergence behavior of the SGD optimizer)</td><td>ReLU</td><td></td><td></td><td></td></tr>\n",
        "<tr><td>Tanh</td><td></td><td></td><td></td></tr>\n",
        "<tr><td>Sigmoid</td><td></td><td></td><td></td></tr>\n",
        "<tr><td rowspan=\"3\">RMSprop<br>(Learning rate = [0.0001, 0.001, 0.01], discuss the effect of learning rates on learning curves, Epsilon = 10^-6)</td><td>ReLU</td><td></td><td></td><td></td></tr>\n",
        "<tr><td>Tanh</td><td></td><td></td><td></td></tr>\n",
        "<tr><td>Sigmoid</td><td></td><td></td><td></td></tr>\n",
        "<tr><td rowspan=\"3\">ADAM<br>(β₁=[0.85, 0.9], β₂=[0.95, 0.99], discuss the functions of the parameters β₁ and β₂)</td><td>ReLU</td><td></td><td></td><td></td></tr>\n",
        "<tr><td>Tanh</td><td></td><td></td><td></td></tr>\n",
        "<tr><td>Sigmoid</td><td></td><td></td><td></td></tr>\n",
        "</tbody>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fdd82a93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Useful across models\n",
        "activation_function_selector = ['relu', 'tanh', 'sigmoid']\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Helper function to plot loss curve\n",
        "def plot_loss(curve, act, opt, lr, mom, eps, beta1, beta2):\n",
        "    plt.plot(curve)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    if opt == \"SGD\":\n",
        "        plt.title(f\"Binary Cross Entropy Loss Curve. Activation {act}, Optimizer {opt}, Learning Rate {lr}, Momentum {mom}\")\n",
        "    elif opt == \"RMSprop\":\n",
        "        plt.title(f\"Binary Cross Entropy Loss Curve. Activation {act}, Optimizer {opt}, Learning Rate {lr}, Epsilon {eps}\")\n",
        "    elif opt == \"ADAM\":\n",
        "        plt.title(f\"Binary Cross Entropy Loss Curve. Activation {act}, Optimizer {opt}, β_1 {beta1}, β_2 {beta2}\")\n",
        "    else:\n",
        "        print(\"Optimizer not recognized\")\n",
        "    # Ensure the save folder exists\n",
        "    output_dir = \"Plot JPGs\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    # Compose the filename based on optimizer and parameters\n",
        "    if opt == \"SGD\":\n",
        "        filename = f\"{output_dir}/LossCurve_{opt}_{act}_LR{lr}_Momentum{mom}.jpg\"\n",
        "    elif opt == \"RMSprop\":\n",
        "        filename = f\"{output_dir}/LossCurve_{opt}_{act}_LR{lr}_Eps{eps}.jpg\"\n",
        "    elif opt == \"ADAM\":\n",
        "        filename = f\"{output_dir}/LossCurve_{opt}_{act}_B1{beta1}_B2{beta2}.jpg\"\n",
        "    else:\n",
        "        filename = f\"{output_dir}/LossCurve_{opt}_{act}.jpg\"\n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8963438",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test training sequence\n",
        "sgdlr = 0.01\n",
        "sgdmom = 0.0\n",
        "testmodel = Q1SequentialModel(2, 5, 1, activation_function_selector[0])\n",
        "testmodel._init_weights_biases\n",
        "sgdoptimizer = optim.SGD(testmodel.parameters(), sgdlr, sgdmom)\n",
        "train(testmodel, hw1data, loss_function, sgdoptimizer)\n",
        "# accuracy\n",
        "# prediction\n",
        "# plot\n",
        "# add line to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88396539",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SGD Optimizer\n",
        "# Learning rate = 0.01\n",
        "# Momentum = [0.0, 0.1, 0.5, 0.9]\n",
        "# Activation function = [ReLU, Tanh, Sigmoid]\n",
        "# For each run, report required epochs, training accuracy, and prediction for [0.8, 0.2]\n",
        "\n",
        "# Define model settings \n",
        "sgd_learning_rate = 0.01\n",
        "sgd_momentum_selector = [0.0, 0.1, 0.5, 0.9]\n",
        "\n",
        "\n",
        "# Training Loops\n",
        "for i in activation_function_selector:\n",
        "\n",
        "    # Initialize model, weights, biases\n",
        "    model_i = Q1SequentialModel(2, 5, 1, activation_function_selector[i])\n",
        "    model_i._init_weights_biases()\n",
        "\n",
        "    # Optimize using SGD with learning rate and momentum parameters\n",
        "    for j in sgd_momentum_selector: \n",
        "        SGD_i_j = optim.SGD(model_i.parameters(), sgd_learning_rate, sgd_momentum_selector[j])\n",
        "        train(model_i, hw1data, loss_function, SGD_i_j)\n",
        "\n",
        "        # Save plot as jpg\n",
        "\n",
        "        # Training Accuracy\n",
        "\n",
        "        # Predict classification for [0.8, 0.2]\n",
        "\n",
        "        # Save results to dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3cfb38",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RMSprop Optimizer \n",
        "# Learning rates (Learning rate = [0.0001, 0.001, 0.01]\n",
        "# Epsilon = 10E-6\n",
        "# Activation function = [ReLU, Tanh, Sigmoid]\n",
        "# For each run, report required epochs, training accuracy, and prediction for [0.8, 0.2]\n",
        "\n",
        "# Define model settings\n",
        "rms_learning_rate_selector = [0.0001, 0.001, 0.01]\n",
        "epsilon = 10e-6\n",
        "\n",
        "\n",
        "# Save results to dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "847445af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADAM Optimizer\n",
        "# beta_1 = [0.85, 0.9] \n",
        "# beta_2 = [0.95, 0.99] \n",
        "# Activation function = [ReLU, Tanh, Sigmoid]\n",
        "# For each run, report required epochs, training accuracy, and prediction for [0.8, 0.2]\n",
        "\n",
        "# Define model settings\n",
        "beta_1 = [0.85, 0.9] \n",
        "beta_2 = [0.95, 0.99] \n",
        "\n",
        "\n",
        "# Save results to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3403993c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Loss Curves\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f4ff8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict and Report Results Table\n",
        "\n",
        "# Combine dataframes and clean up "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd878524",
      "metadata": {},
      "source": [
        "Discussion Section\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
