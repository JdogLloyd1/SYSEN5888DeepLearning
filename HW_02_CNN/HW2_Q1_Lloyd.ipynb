{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b02aba61",
      "metadata": {},
      "source": [
        "SYSEN 5888 Spring 2026\n",
        "\n",
        "Jonathan Lloyd\n",
        "\n",
        "Homework 2, Question 1\n",
        "\n",
        "\n",
        "Goal: Building a convolutional neural network (ConvNet) to classify images of fruits and vegetables into their respective classes\n",
        "\n",
        "Tools: Numpy, PyTorch, Torchvision\n",
        "\n",
        "Data: Fruits-360 on Kaggle https://www.kaggle.com/moltean/fruits \n",
        "\n",
        "Task: Load dataset, scale 100x100 images to 75x75, normalization and data augmentation, define training and testing datasets (85%/15% split), batch each dataset into sizes 1000, shuffle seed 42, define sequential ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c3ccab48",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Colab Server details if running outside of Colab Online UI \n",
        "'''\n",
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "print(result.stdout if result.returncode == 0 else \"No GPU detected (CPU runtime)\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ff591cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "## IMPORT API KEY - UNCOMMENT APPLICABLE LINES WHEN RUNNING DIFFERENT KERNELS\n",
        "# Import KAGGLE_API_KEY from .env \n",
        "# KAGGLE_API_TOKEN = os.getenv(\"KAGGLE_API_KEY\")\n",
        "# Define directly - delete key before uploading to GitHub\n",
        "KAGGLE_API_TOKEN = \"KGAT_4be0dffb1cb77ca36a6657e84134eaa8\"\n",
        "# When running in Colab - define in web using Colab Secrets \n",
        "# KAGGLE_API_TOKEN = userdata.get(\"KAGGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ee068ae6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# update any packages in Colab server\n",
        "%pip install --upgrade numpy pandas kagglehub torch torchvision IPython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8f9b5a86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import kagglehub   \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Import dataset from Kaggle\n",
        "# Kagglehub reads KAGGLE_API_TOKEN from environment (set in cell above)\n",
        "os.environ[\"KAGGLE_API_TOKEN\"] = KAGGLE_API_TOKEN\n",
        "# On Colab: use /content to avoid cache disk limits; locally omit output_dir to use cache\n",
        "path = kagglehub.dataset_download(\"moltean/fruits\", output_dir=\"/content/fruits-360\")\n",
        "\n",
        "# Locate Training and Test subfolders (Fruits-360 dataset structure)\n",
        "path = Path(path)\n",
        "# Kaggle zip may nest Training/Test inside a subfolder (e.g. fruits-360/Training)\n",
        "# Search recursively for Training directory\n",
        "train_path = None\n",
        "for p in path.rglob(\"Training\"):\n",
        "    if p.is_dir() and (p.parent / \"Test\").exists():\n",
        "        train_path = p\n",
        "        break\n",
        "if train_path is not None:\n",
        "    test_path = train_path.parent / \"Test\"\n",
        "else:\n",
        "    # Fallback: check direct children\n",
        "    if (path / \"Training\").exists():\n",
        "        train_path, test_path = path / \"Training\", path / \"Test\"\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find Training/Test folders. Dataset root: {path}\\n\"\n",
        "            f\"Contents: {[d.name for d in path.iterdir()] if path.exists() else 'path does not exist'}\"\n",
        "        )\n",
        "\n",
        "# Load image datasets with placeholder transform (will add scaling/normalization in next steps)\n",
        "print(\"Loading datasets\")\n",
        "train_dataset = torchvision.datasets.ImageFolder(str(train_path), transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.ImageFolder(str(test_path), transform=transforms.ToTensor())\n",
        "\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "print(f\"Training samples: {len(train_dataset)} | Classes: {len(train_dataset.classes)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c8b1ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Image Preprocessing\n",
        "\n",
        "# 1. Image scaling (100x100 -> 75x75):\n",
        "image_size = 75\n",
        "\n",
        "# 2. Image normalization values for RGB to [-1, 1]\n",
        "normalize_means = [0.5, 0.5, 0.5]\n",
        "normalize_stds = [0.5, 0.5, 0.5]\n",
        "\n",
        "# 3. Data augmentation and normalization for training; only normalization for validation/test\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=normalize_means, std=normalize_stds)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=normalize_means, std=normalize_stds)\n",
        "])\n",
        "\n",
        "# Replace raw datasets with transformed datasets\n",
        "train_dataset = torchvision.datasets.ImageFolder(str(train_path), transform=train_transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(str(test_path), transform=test_transform)\n",
        "\n",
        "# 4. Define training and validation split (85%/15% from train_dataset)\n",
        "total_train = len(train_dataset)\n",
        "val_size = int(0.15 * total_train)\n",
        "train_size = total_train - val_size\n",
        "\n",
        "SHUFFLE_SEED = 42\n",
        "torch.manual_seed(SHUFFLE_SEED)  # set random seed for reproducibility\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SHUFFLE_SEED))\n",
        "\n",
        "# 5. Define dataloaders with batch size 1000 and consistent shuffle with seed\n",
        "batch_size = 1000\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda worker_id: np.random.seed(SHUFFLE_SEED))\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(\"Loading success\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35609e4",
      "metadata": {},
      "source": [
        "Architecture - MODEL 1: \n",
        "\n",
        "Define a Sequential model, wherein the layers are stacked sequentially and each layer has exactly one input tensor and one output tensor. Please build a ConvNet by adding the layers to the Sequential model using the configuration below. For each of the layers, initialize the kernel weights from a Glorot uniform distribution and set the random seed to 99. Additionally, initialize the bias vector as a zero vector. In this architecture, you may use different dropout values [0.1, 0.3, 0.5] and report the impact of dropout values on model performance.\n",
        "![alt text](<model 1 arch.png>)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1f22d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: per architecture table (2 conv layers, BatchNorm, Dropout [0.1, 0.3, 0.5], Dense 256 -> 251).\n",
        "# Assumes input size 75x75; Glorot uniform, zero bias, seed 99.\n",
        "NUM_CLASSES = 251\n",
        "\n",
        "class Model1(nn.Module):\n",
        "    # Sequential ConvNet: Conv2D(64)->ReLU->MaxPool, Conv2D(128)->ReLU->BN->Dropout->MaxPool, Flatten->Dense(256)->ReLU->Dense(251).\n",
        "    # NOTE: forward() returns raw logits (no softmax). This is the recommended PyTorch pattern when\n",
        "    # using nn.CrossEntropyLoss, which internally applies log-softmax. Apply torch.softmax(logits, dim=1)\n",
        "    # at inference time only if you need class probabilities.\n",
        "    def __init__(self, num_classes=251, dropout=0.1, in_channels=3, input_h=75, input_w=75):\n",
        "        super(Model1, self).__init__()\n",
        "        # Layer 1–2: Conv2D 64, (3,3), no padding, ReLU; MaxPool2D (2,2)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        # Layer 3–6: Conv2D 128, (3,3), no padding, ReLU; BatchNorm; Dropout; MaxPool2D (2,2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(128, eps=0.001, momentum=0.01)  # 0.01 in PyTorch = 0.99 in Keras (weight on running stats)\n",
        "        self.drop2 = nn.Dropout2d(p=dropout)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        # Layer 7–9: Flatten; Dense 256 ReLU; Dense 251\n",
        "        self.flatten = nn.Flatten()\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, in_channels, input_h, input_w)\n",
        "            dummy = self.pool2(self.drop2(self.bn2(self.relu2(self.conv2(self.pool1(self.relu1(self.conv1(dummy))))))))\n",
        "            flat_size = self.flatten(dummy).shape[1]\n",
        "        self.fc1 = nn.Linear(flat_size, 256)\n",
        "        self.relu_fc = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu_fc(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def _init_weights_biases(self):\n",
        "        torch.manual_seed(99)\n",
        "        for m in [self.conv1, self.conv2, self.fc1, self.fc2]:\n",
        "            if hasattr(m, \"weight\") and m.weight is not None:\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "            if hasattr(m, \"bias\") and m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef88fe5",
      "metadata": {},
      "source": [
        "Architecture - MODEL 2: \n",
        "\n",
        "The performance of the CNN model is notably impacted by the number of convolutional layers it employs. In the preceding design, two convolutional layers were integrated. Kindly introduce an additional convolutional layer (as depicted in the updated architecture below) and elaborate on the roles of convolutional layers.\n",
        "![alt text](<model 2 arch.png>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0257bffc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2: per architecture table (3 conv layers 64->128->256, BatchNorm, Dropout 0.3, Dense 512->251).\n",
        "class Model2(nn.Module):\n",
        "    # Sequential ConvNet per table: Conv(64)->ReLU->Pool, Conv(128)->ReLU->Pool, Conv(256)->ReLU->BN->Dropout(0.3)->Pool, Flatten->Dense(512)->ReLU->Dense(251).\n",
        "    # NOTE: forward() returns raw logits (no softmax). Use nn.CrossEntropyLoss during training; apply\n",
        "    # torch.softmax(logits, dim=1) at inference time only when probabilities are needed.\n",
        "    def __init__(self, num_classes=251, dropout=0.3, in_channels=3, input_h=75, input_w=75):\n",
        "        super(Model2, self).__init__()\n",
        "        # Layers 1–2: Conv2D 64, (3,3), no padding, ReLU; MaxPool2D (2,2)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        # Layers 3–4: Conv2D 128, (3,3), no padding, ReLU; MaxPool2D (2,2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        # Layers 5–8: Conv2D 256, (3,3), no padding, ReLU; BatchNorm; Dropout 0.3; MaxPool2D (2,2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(256, eps=0.001, momentum=0.01)  # 0.01 in PyTorch = 0.99 in Keras (weight on running stats)\n",
        "        self.drop3 = nn.Dropout2d(p=dropout)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        # Layers 9–11: Flatten; Dense 512 ReLU; Dense 251\n",
        "        self.flatten = nn.Flatten()\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, in_channels, input_h, input_w)\n",
        "            dummy = self.pool1(self.relu1(self.conv1(dummy)))\n",
        "            dummy = self.pool2(self.relu2(self.conv2(dummy)))\n",
        "            dummy = self.pool3(self.drop3(self.bn3(self.relu3(self.conv3(dummy)))))\n",
        "            flat_size = self.flatten(dummy).shape[1]\n",
        "        self.fc1 = nn.Linear(flat_size, 512)\n",
        "        self.relu_fc = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        x = self.bn3(x)\n",
        "        x = self.drop3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu_fc(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def _init_weights_biases(self):\n",
        "        torch.manual_seed(99)\n",
        "        for m in [self.conv1, self.conv2, self.conv3, self.fc1, self.fc2]:\n",
        "            if hasattr(m, \"weight\") and m.weight is not None:\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "            if hasattr(m, \"bias\") and m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c947bbbc",
      "metadata": {},
      "source": [
        "Training: The model is compiled by specifying the optimizer, the loss function and metrics to be recorded at each step of the training process. The ADAM optimizer should minimize the categorical cross entropy. The ConvNet model can be trained and evaluated with the previously created data generators. The training step size can be calculated by dividing the number of images in the generator with the batch size for training and testing data, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18670b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data storage \n",
        "results_dataframe = pd.DataFrame(columns=[\n",
        "    'Model Name', 'Dropout', 'Epochs', 'Training Accuracy (%)', \n",
        "    'Validation Accuracy (%)', 'Test Accuracy (%)', 'Final Test Loss'\n",
        "])\n",
        "\n",
        "# Maximum epochs to run training\n",
        "# Start with 50, go down to 20 if too time consuming\n",
        "MAX_EPOCHS = 50\n",
        "\n",
        "## Helper Functions\n",
        "\n",
        "# BASIC TRAIN\n",
        "def train(model, loader, criterion, optimizer):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "    \n",
        "    Parameters:\n",
        "        model (nn.Module): The neural network model.\n",
        "        loader (DataLoader): DataLoader for training data.\n",
        "        criterion: Loss function.\n",
        "        optimizer: Optimization algorithm.\n",
        "        device: Device to run the training on.\n",
        "    \n",
        "    Returns:\n",
        "        epoch_loss (float): Average loss for the epoch.\n",
        "        epoch_acc (float): Accuracy for the epoch.\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for inputs, labels in loader:\n",
        "        \n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        \n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "        \n",
        "        running_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# BASIC EVAL \n",
        "def evaluate(model, loader, criterion):\n",
        "    # Compute 3 accuracy percentages \n",
        "    \"\"\"\n",
        "    Evaluate the model on validation or test data.\n",
        "    \n",
        "    Parameters:\n",
        "        model (nn.Module): The neural network model.\n",
        "        loader (DataLoader): DataLoader for validation/test data.\n",
        "        criterion: Loss function.\n",
        "    \n",
        "    Returns:\n",
        "        epoch_loss (float): Average loss for the epoch.\n",
        "        epoch_acc (float): Accuracy for the epoch.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, labels in loader:\n",
        "            \n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            \n",
        "            running_loss += loss.item() * inputs.size(0)  # Accumulate loss\n",
        "            \n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# PLOT LOSS CURVES \n",
        "def plot_loss(curve, dropout, dataset):\n",
        "    # Plot and save loss curve\n",
        "    # dataset = [Training, Validation]\n",
        "    output_dir = \"Plot JPGs\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(curve)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    if dataset == 'Training':\n",
        "        plt.ylabel(\"Training Loss\")\n",
        "    elif dataset == 'Validation':\n",
        "        plt.ylabel(\"Validation Loss\")\n",
        "\n",
        "    plt.title(f\"Categorical Cross Entropy Loss Curve for {dataset} Dataset, Dropout {dropout}\") \n",
        "    filename = f\"{output_dir}/LossCurve_{dataset}_Dataset_{dropout}_Dropout.jpg\"\n",
        "        \n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cd458c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL TRAIN\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=50):\n",
        "    \"\"\"\n",
        "    Train the model and evaluate on test data after each epoch.\n",
        "    \n",
        "    Parameters:\n",
        "        model (nn.Module): The neural network model.\n",
        "        train_loader (DataLoader): DataLoader for training data.\n",
        "        test_loader (DataLoader): DataLoader for test data.\n",
        "        criterion: Loss function.\n",
        "        optimizer: Optimization algorithm.\n",
        "        device: Device to run the training on.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "    \n",
        "    Returns:\n",
        "        Training and validation loss and accuracy.\n",
        "    \"\"\"\n",
        "    training_loss_curve = []\n",
        "    validation_loss_curve = []\n",
        "    training_accuracy = []\n",
        "    validation_accuracy = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "        \n",
        "        training_loss_curve.append(train_loss)\n",
        "        training_accuracy.append(train_acc)\n",
        "        validation_loss_curve.append(val_loss)\n",
        "        validation_accuracy.append(val_acc)\n",
        "        \n",
        "        print(f'Epoch {epoch+1}/{num_epochs} | '\n",
        "              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | '\n",
        "              f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "    \n",
        "    return training_loss_curve, validation_loss_curve, training_accuracy, validation_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ae7e07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAIN EXPERIMENT HELPER\n",
        "def run_experiment(model_arch, train_loader, val_loader, test_loader, max_epochs=50, dropout=None):\n",
        "    # Run full experiment: instantiate model, train, evaluate accuracies, plot, return results to dataframe\n",
        "    if model_arch == 1:\n",
        "        model = Model1(dropout=dropout)\n",
        "    elif model_arch == 2:\n",
        "        model = Model2() # dropout = 0.3 by default\n",
        "    else:\n",
        "        raise ValueError(f\"ERROR: Model selected {model_arch} does not match possible options (1, 2).\")\n",
        "        \n",
        "    # Define Optimizer \n",
        "    # Adam, Categorical Cross Entropy\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Train\n",
        "    training_loss_curve, validation_loss_curve, training_accuracy, validation_accuracy = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, max_epochs\n",
        "    )\n",
        "\n",
        "    # Evaluate final test\n",
        "    final_test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    # Save plots \n",
        "    plot_loss(training_loss_curve, dropout, \"Training\")\n",
        "    plot_loss(validation_loss_curve, dropout, \"Validation\")\n",
        "\n",
        "    # return results\n",
        "    row = {\n",
        "        'Model Name': f\"Model {model_arch}\", \n",
        "        'Dropout': dropout, \n",
        "        'Epochs': max_epochs, \n",
        "        'Training Accuracy (%)': training_accuracy[-1] * 100.0,\n",
        "        'Validation Accuracy (%)': validation_accuracy[-1] * 100.0, \n",
        "        'Test Accuracy (%)': test_acc * 100.0,\n",
        "        'Final Test Loss': final_test_loss\n",
        "    }\n",
        "\n",
        "    return row \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1004a046",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and Test Model 1a, 1b, 1c\n",
        "# For loop to pass different dropouts\n",
        "DROPOUT_SELECTOR = [0.1, 0.3, 0.5]\n",
        "for d in DROPOUT_SELECTOR:\n",
        "    print(f\"RUNNING MODEL 1, DROPOUT {d}\")\n",
        "    model_run = run_experiment(1, train_loader, val_loader, test_loader, max_epochs=MAX_EPOCHS, dropout=d)\n",
        "    results_dataframe = pd.concat([results_dataframe, pd.DataFrame([model_run])], ignore_index=True)\n",
        "    print(f\"COMPLETED MODEL 1, DROPOUT {d}\")\n",
        "\n",
        "# Train and Test Model 2, no dropout arg\n",
        "print(f\"RUNNING MODEL 2\")\n",
        "model_run_2 = run_experiment(2, train_loader, val_loader, test_loader, max_epochs=MAX_EPOCHS)\n",
        "results_dataframe = pd.concat([results_dataframe, pd.DataFrame([model_run_2])], ignore_index=True)\n",
        "print(f\"COMPLETED MODEL 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d639a945",
      "metadata": {},
      "source": [
        "Deliverables: Please report the training and validation accuracy after the training process is carried out for 50 epochs (you can train for 20 epochs if the training is time consuming), in addition to the achieved accuracy levels on the test dataset. Also, plot the loss curves for both training and validation datasets. Discuss the functions of dropout values and the number of convolutional layers in relation to the CNN model performance. Please make sure to submit your working code files along with the final results and the plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131a7be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print results dataframe\n",
        "print(results_dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a637878f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot all loss curves together\n",
        "# Display the 3 Model 1 loss curve images together and Model 2 plot separately\n",
        "\n",
        "# Define subfolder and filenames\n",
        "plot_folder = \"HW_02_CNN/plots\"  # Adjust if your JPGs are in a different folder\n",
        "\n",
        "# Filenames for Model 1 plots\n",
        "model1_filenames = [\n",
        "    \"model1a_dropout_0.1_loss.jpg\",\n",
        "    \"model1b_dropout_0.3_loss.jpg\",\n",
        "    \"model1c_dropout_0.5_loss.jpg\"\n",
        "]\n",
        "# Filename for Model 2 plot (with dropout=0.3)\n",
        "model2_filename = \"model2_dropout_0.3_loss.jpg\"\n",
        "\n",
        "model1_titles = [\n",
        "    \"Model 1a (Dropout=0.1) Loss Curve\",\n",
        "    \"Model 1b (Dropout=0.3) Loss Curve\",\n",
        "    \"Model 1c (Dropout=0.5) Loss Curve\"\n",
        "]\n",
        "\n",
        "# Display Model 1 plots together\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for idx, (fname, title) in enumerate(zip(model1_filenames, model1_titles)):\n",
        "    img_path = os.path.join(plot_folder, fname)\n",
        "    img = mpimg.imread(img_path)\n",
        "    ax = axs[idx]\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(title, fontsize=13)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display Model 2 plot separately\n",
        "img_path2 = os.path.join(plot_folder, model2_filename)\n",
        "img2 = mpimg.imread(img_path2)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(img2)\n",
        "plt.title(\"Model 2 (Dropout=0.3) Loss Curve\", fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c5b225",
      "metadata": {},
      "source": [
        "Discussion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f9826c",
      "metadata": {},
      "source": [
        "Bonus (+1): A skip connection in a neural network is a connection that skips one or more layers and connects to a later layer. Residual Networks (ResNets) have popularized the use of skip connections to address the vanishing gradient problem, and hence enabling the training of deeper networks. Your task for this bonus part is to integrate such a skip connection, any types of skip connections are acceptable. For instance, linking the output of the first layer convolutional directly to the input of the last convolutional layer in your model architecture. Based on your results, analyze and discuss any improvements or effects this change has on the model's performance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
