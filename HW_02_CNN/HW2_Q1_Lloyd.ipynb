{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b02aba61",
      "metadata": {},
      "source": [
        "SYSEN 5888 Spring 2026\n",
        "\n",
        "Jonathan Lloyd\n",
        "\n",
        "Homework 2, Question 1\n",
        "\n",
        "\n",
        "Goal: Building a convolutional neural network (ConvNet) to classify images of fruits and vegetables into their respective classes\n",
        "\n",
        "Tools: Numpy, PyTorch, Torchvision\n",
        "\n",
        "Data: Fruits-360 on Kaggle https://www.kaggle.com/moltean/fruits \n",
        "\n",
        "Task: Load dataset, scale 100x100 images to 75x75, normalization and data augmentation, define training and testing datasets (85%/15% split), batch each dataset into sizes 1000, shuffle seed 42, define sequential ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c3ccab48",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport subprocess\\nresult = subprocess.run([\\'nvidia-smi\\'], capture_output=True, text=True)\\nprint(result.stdout if result.returncode == 0 else \"No GPU detected (CPU runtime)\")\\n'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check Colab Server details if running outside of Colab Online UI \n",
        "'''\n",
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "print(result.stdout if result.returncode == 0 else \"No GPU detected (CPU runtime)\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ff591cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "## IMPORT API KEY - UNCOMMENT APPLICABLE LINES WHEN RUNNING DIFFERENT KERNELS\n",
        "# Import KAGGLE_API_KEY from .env \n",
        "# KAGGLE_API_TOKEN = os.getenv(\"KAGGLE_API_KEY\")\n",
        "# Define directly - delete key before uploading to GitHub\n",
        "KAGGLE_API_TOKEN = \"KGAT_4be0dffb1cb77ca36a6657e84134eaa8\"\n",
        "# When running in Colab - define in web using Colab Secrets \n",
        "# KAGGLE_API_TOKEN = userdata.get(\"KAGGLE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ee068ae6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Collecting kagglehub\n",
            "  Downloading kagglehub-1.0.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.25.0+cpu)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Collecting kagglesdk<1.0,>=0.1.14 (from kagglehub)\n",
            "  Downloading kagglesdk-0.1.15-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (26.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kagglesdk<1.0,>=0.1.14->kagglehub) (5.29.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2026.1.4)\n",
            "Downloading kagglehub-1.0.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kagglesdk-0.1.15-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kagglesdk, kagglehub\n",
            "  Attempting uninstall: kagglehub\n",
            "    Found existing installation: kagglehub 0.3.13\n",
            "    Uninstalling kagglehub-0.3.13:\n",
            "      Successfully uninstalled kagglehub-0.3.13\n",
            "Successfully installed kagglehub-1.0.0 kagglesdk-0.1.15\n"
          ]
        }
      ],
      "source": [
        "# update any packages in Colab server\n",
        "%pip install --upgrade numpy pandas kagglehub torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8f9b5a86",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Colab cache for faster access to the 'fruits' dataset.\n",
            "Loading datasets\n",
            "Dataset downloaded to: /kaggle/input/fruits\n",
            "Training samples: 131030 | Classes: 251\n",
            "Test samples: 43670\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import kagglehub   \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Import dataset from Kaggle\n",
        "# Kagglehub reads KAGGLE_API_TOKEN from environment (set in cell above)\n",
        "os.environ[\"KAGGLE_API_TOKEN\"] = KAGGLE_API_TOKEN\n",
        "# On Colab: use /content to avoid cache disk limits; locally omit output_dir to use cache\n",
        "path = kagglehub.dataset_download(\"moltean/fruits\", output_dir=\"/content/fruits-360\")\n",
        "\n",
        "# Locate Training and Test subfolders (Fruits-360 dataset structure)\n",
        "path = Path(path)\n",
        "# Kaggle zip may nest Training/Test inside a subfolder (e.g. fruits-360/Training)\n",
        "# Search recursively for Training directory\n",
        "train_path = None\n",
        "for p in path.rglob(\"Training\"):\n",
        "    if p.is_dir() and (p.parent / \"Test\").exists():\n",
        "        train_path = p\n",
        "        break\n",
        "if train_path is not None:\n",
        "    test_path = train_path.parent / \"Test\"\n",
        "else:\n",
        "    # Fallback: check direct children\n",
        "    if (path / \"Training\").exists():\n",
        "        train_path, test_path = path / \"Training\", path / \"Test\"\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find Training/Test folders. Dataset root: {path}\\n\"\n",
        "            f\"Contents: {[d.name for d in path.iterdir()] if path.exists() else 'path does not exist'}\"\n",
        "        )\n",
        "\n",
        "# Load image datasets with placeholder transform (will add scaling/normalization in next steps)\n",
        "print(\"Loading datasets\")\n",
        "train_dataset = torchvision.datasets.ImageFolder(str(train_path), transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.ImageFolder(str(test_path), transform=transforms.ToTensor())\n",
        "\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "print(f\"Training samples: {len(train_dataset)} | Classes: {len(train_dataset.classes)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c8b1ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading success\n"
          ]
        }
      ],
      "source": [
        "## Image Preprocessing\n",
        "\n",
        "# 1. Image scaling (100x100 -> 75x75):\n",
        "image_size = 75\n",
        "\n",
        "# 2. Image normalization values for RGB to [-1, 1]\n",
        "normalize_means = [0.5, 0.5, 0.5]\n",
        "normalize_stds = [0.5, 0.5, 0.5]\n",
        "\n",
        "# 3. Data augmentation and normalization for training; only normalization for validation/test\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=normalize_means, std=normalize_stds)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=normalize_means, std=normalize_stds)\n",
        "])\n",
        "\n",
        "# Replace raw datasets with transformed datasets\n",
        "train_dataset = torchvision.datasets.ImageFolder(str(train_path), transform=train_transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(str(test_path), transform=test_transform)\n",
        "\n",
        "# 4. Define training and validation split (85%/15% from train_dataset)\n",
        "total_train = len(train_dataset)\n",
        "val_size = int(0.15 * total_train)\n",
        "train_size = total_train - val_size\n",
        "\n",
        "SHUFFLE_SEED = 42\n",
        "torch.manual_seed(SHUFFLE_SEED)  # set random seed for reproducibility\n",
        "train_subset, val_subset = random_split(train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(SHUFFLE_SEED))\n",
        "\n",
        "# 5. Define dataloaders with batch size 1000 and consistent shuffle with seed\n",
        "batch_size = 1000\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, worker_init_fn=lambda worker_id: np.random.seed(SHUFFLE_SEED))\n",
        "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(\"Loading success\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35609e4",
      "metadata": {},
      "source": [
        "Architecture - MODEL 1: \n",
        "\n",
        "Define a Sequential model, wherein the layers are stacked sequentially and each layer has exactly one input tensor and one output tensor. Please build a ConvNet by adding the layers to the Sequential model using the configuration below. For each of the layers, initialize the kernel weights from a Glorot uniform distribution and set the random seed to 99. Additionally, initialize the bias vector as a zero vector. In this architecture, you may use different dropout values [0.1, 0.3, 0.5] and report the impact of dropout values on model performance.\n",
        "![alt text](<model 1 arch.png>)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1f22d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 1: per architecture table (2 conv layers, BatchNorm, Dropout [0.1, 0.3, 0.5], Dense 256 -> 251).\n",
        "# Assumes input size 75x75; Glorot uniform, zero bias, seed 99.\n",
        "NUM_CLASSES = 251\n",
        "\n",
        "class Model1(nn.Module):\n",
        "    # Sequential ConvNet: Conv2D(64)->ReLU->MaxPool, Conv2D(128)->ReLU->BN->Dropout->MaxPool, Flatten->Dense(256)->ReLU->Dense(251)->Softmax.\n",
        "    def __init__(self, num_classes=251, dropout=0.1, in_channels=3, input_h=75, input_w=75):\n",
        "        super(Model1, self).__init__()\n",
        "        # Layer 1–2: Conv2D 64, (3,3), no padding, ReLU; MaxPool2D (2,2)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        # Layer 3–6: Conv2D 128, (3,3), no padding, ReLU; BatchNorm; Dropout; MaxPool2D (2,2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(128, eps=0.001, momentum=0.01)  # 0.01 in PyTorch = 0.99 in Keras (weight on running stats)\n",
        "        self.drop2 = nn.Dropout2d(p=dropout)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        # Layer 7–9: Flatten; Dense 256 ReLU; Dense 251\n",
        "        self.flatten = nn.Flatten()\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, in_channels, input_h, input_w)\n",
        "            dummy = self.pool2(self.drop2(self.bn2(self.relu2(self.conv2(self.pool1(self.relu1(self.conv1(dummy))))))))\n",
        "            flat_size = self.flatten(dummy).shape[1]\n",
        "        self.fc1 = nn.Linear(flat_size, 256)\n",
        "        self.relu_fc = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.softmax_fc = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = self.drop2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu_fc(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax_fc(x)\n",
        "        return x\n",
        "\n",
        "    def _init_weights_biases(self):\n",
        "        torch.manual_seed(99)\n",
        "        for m in [self.conv1, self.conv2, self.fc1, self.fc2]:\n",
        "            if hasattr(m, \"weight\") and m.weight is not None:\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "            if hasattr(m, \"bias\") and m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef88fe5",
      "metadata": {},
      "source": [
        "Architecture - MODEL 2: \n",
        "\n",
        "The performance of the CNN model is notably impacted by the number of convolutional layers it employs. In the preceding design, two convolutional layers were integrated. Kindly introduce an additional convolutional layer (as depicted in the updated architecture below) and elaborate on the roles of convolutional layers.\n",
        "![alt text](<model 2 arch.png>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0257bffc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2: per architecture table (3 conv layers 64->128->256, BatchNorm, Dropout 0.3, Dense 512->251).\n",
        "class Model2(nn.Module):\n",
        "    # Sequential ConvNet per table: Conv(64)->ReLU->Pool, Conv(128)->ReLU->Pool, Conv(256)->ReLU->BN->Dropout(0.3)->Pool, Flatten->Dense(512)->ReLU->Dense(251)->Softmax.\n",
        "    def __init__(self, num_classes=251, dropout=0.3, in_channels=3, input_h=75, input_w=75):\n",
        "        super(Model2, self).__init__()\n",
        "        # Layers 1–2: Conv2D 64, (3,3), no padding, ReLU; MaxPool2D (2,2)\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        # Layers 3–4: Conv2D 128, (3,3), no padding, ReLU; MaxPool2D (2,2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        # Layers 5–8: Conv2D 256, (3,3), no padding, ReLU; BatchNorm; Dropout 0.3; MaxPool2D (2,2)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=0)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(256, eps=0.001, momentum=0.01)  # 0.01 in PyTorch = 0.99 in Keras (weight on running stats)\n",
        "        self.drop3 = nn.Dropout2d(p=dropout)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        # Layers 9–11: Flatten; Dense 512 ReLU; Dense 251\n",
        "        self.flatten = nn.Flatten()\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, in_channels, input_h, input_w)\n",
        "            dummy = self.pool1(self.relu1(self.conv1(dummy)))\n",
        "            dummy = self.pool2(self.relu2(self.conv2(dummy)))\n",
        "            dummy = self.pool3(self.drop3(self.bn3(self.relu3(self.conv3(dummy)))))\n",
        "            flat_size = self.flatten(dummy).shape[1]\n",
        "        self.fc1 = nn.Linear(flat_size, 512)\n",
        "        self.relu_fc = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.softmax_fc = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        x = self.bn3(x)\n",
        "        x = self.drop3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu_fc(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax_fc(x)\n",
        "        return x\n",
        "\n",
        "    def _init_weights_biases(self):\n",
        "        torch.manual_seed(99)\n",
        "        for m in [self.conv1, self.conv2, self.conv3, self.fc1, self.fc2]:\n",
        "            if hasattr(m, \"weight\") and m.weight is not None:\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "            if hasattr(m, \"bias\") and m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c947bbbc",
      "metadata": {},
      "source": [
        "Training: The model is compiled by specifying the optimizer, the loss function and metrics to be recorded at each step of the training process. The ADAM optimizer should minimize the categorical cross entropy. The ConvNet model can be trained and evaluated with the previously created data generators. The training step size can be calculated by dividing the number of images in the generator with the batch size for training and testing data, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18670b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data storage \n",
        "results_dataframe = pd.DataFrame(columns=[\n",
        "    'Model Name', 'Dropout', 'Epochs', 'Training Accuracy (%)', \n",
        "    'Validation Accuracy (%)', 'Test Accuracy (%)'\n",
        "])\n",
        "\n",
        "# Maximum epochs to run training\n",
        "# Start with 50, go down to 20 if too time consuming\n",
        "MAX_EPOCHS = 50\n",
        "\n",
        "## Helper Functions\n",
        "\n",
        "# MODEL TRAIN\n",
        "def train(model, optimizer, train_loader, val_loader, max_epochs):\n",
        "    # Train model through max epochs\n",
        "    model.train()\n",
        "    training_loss_curve = []\n",
        "    validation_loss_curve = []\n",
        "\n",
        "    # Run training \n",
        "    for i in range(max_epochs):\n",
        "        # Run through train loader, validation loader\n",
        "\n",
        "        # Update loss curves\n",
        "        training_loss_curve.append()\n",
        "        validation_loss_curve.append()\n",
        "        \n",
        "\n",
        "    return training_loss_curve, validation_loss_curve\n",
        "\n",
        "# MODEL EVAL \n",
        "def evaluate(model, train_loader, val_loader, test_loader):\n",
        "    # Compute 3 accuracy percentages \n",
        "    model.eval()\n",
        "    \n",
        "\n",
        "\n",
        "    return training_acc, validation_acc, test_acc \n",
        "\n",
        "# PLOT LOSS CURVES \n",
        "def plot_loss(curve, dropout, dataset):\n",
        "    # Plot and save loss curve\n",
        "    # dataset = [Training, Validation]\n",
        "    output_dir = \"Plot JPGs\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(curve)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    if dataset == 'Training':\n",
        "        plt.ylabel(\"Training Loss\")\n",
        "    elif dataset == 'Validation':\n",
        "        plt.ylable(\"Validation Loss\")\n",
        "\n",
        "    plt.title(f\"Categorical Cross Entropy Loss Curve for {dataset} Dataset, Dropout {dropout}\") \n",
        "    filename = f\"{output_dir}/LossCurve_{dataset}_Dataset_{dropout}_Dropout.jpg\"\n",
        "        \n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ae7e07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAIN EXPERIMENT HELPER\n",
        "def run_experiment(model_arch, train_loader, val_loader, test_loader, max_epochs=50, dropout=None):\n",
        "    # Run full experiment: instantiate model, train, evaluate accuracies, plot, return results to dataframe\n",
        "    if model_arch == 1:\n",
        "        model = Model1(dropout=dropout)\n",
        "    elif model_arch == 2:\n",
        "        model = Model2()\n",
        "    else:\n",
        "        raise ValueError(f\"ERROR: Model selected {model_arch} does not match possible options (1, 2).\")\n",
        "        \n",
        "    # Define Optimizer \n",
        "    # Adam, Categorical Cross Entropy\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Train\n",
        "    training_loss_curve, validation_loss_curve = train(model, optimizer, train_loader, val_loader, max_epochs)\n",
        "\n",
        "    # Evaluate \n",
        "    training_acc, validation_acc, test_acc = evaluate(model, train_loader, val_loader, test_loader)\n",
        "\n",
        "    # Save plots \n",
        "    plot_loss(training_loss_curve, dropout, \"Training\")\n",
        "    plot_loss(validation_loss_curve, dropout, \"Validation\")\n",
        "\n",
        "    # return results\n",
        "    row = {\n",
        "        'Model Name': f\"Model {model_arch}\", \n",
        "        'Dropout': dropout, \n",
        "        'Epochs': max_epochs, \n",
        "        'Training Accuracy (%)': training_acc,\n",
        "        'Validation Accuracy (%)': validation_acc, \n",
        "        'Test Accuracy (%)': test_acc\n",
        "    }\n",
        "\n",
        "    return row \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1004a046",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and Test Model 1a, 1b, 1c\n",
        "# For loop to pass different dropouts\n",
        "DROPOUT_SELECTOR = [0.1, 0.3, 0.5]\n",
        "for d in DROPOUT_SELECTOR:\n",
        "    print(f\"RUNNING MODEL 1, DROPOUT {d}\")\n",
        "    model_run = run_experiment(1, train_loader, val_loader, test_loader, max_epochs=MAX_EPOCHS, dropout=d)\n",
        "    results_dataframe = pd.concat([results_dataframe, pd.Dataframe([model_run])], ignore_index=True)\n",
        "    print(f\"COMPLETED MODEL 1, DROPOUT {d}\")\n",
        "\n",
        "# Train and Test Model 2, no dropout arg\n",
        "print(f\"RUNNING MODEL 2\")\n",
        "model_run_2 = run_experiment(2, train_loader, val_loader, test_loader, max_epochs=MAX_EPOCHS)\n",
        "results_dataframe = pd.concat([results_dataframe, pd.Dataframe([model_run_2])], ignore_index=True)\n",
        "print(f\"COMPLETED MODEL 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d639a945",
      "metadata": {},
      "source": [
        "Deliverables: Please report the training and validation accuracy after the training process is carried out for 50 epochs (you can train for 20 epochs if the training is time consuming), in addition to the achieved accuracy levels on the test dataset. Also, plot the loss curves for both training and validation datasets. Discuss the functions of dropout values and the number of convolutional layers in relation to the CNN model performance. Please make sure to submit your working code files along with the final results and the plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131a7be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print results dataframe\n",
        "print(results_dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a637878f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot all loss curves together"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c5b225",
      "metadata": {},
      "source": [
        "Discussion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f9826c",
      "metadata": {},
      "source": [
        "Bonus (+1): A skip connection in a neural network is a connection that skips one or more layers and connects to a later layer. Residual Networks (ResNets) have popularized the use of skip connections to address the vanishing gradient problem, and hence enabling the training of deeper networks. Your task for this bonus part is to integrate such a skip connection, any types of skip connections are acceptable. For instance, linking the output of the first layer convolutional directly to the input of the last convolutional layer in your model architecture. Based on your results, analyze and discuss any improvements or effects this change has on the model's performance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
