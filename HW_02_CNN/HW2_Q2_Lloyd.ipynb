{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b02aba61",
      "metadata": {},
      "source": [
        "SYSEN 5888 Spring 2026\n",
        "\n",
        "Jonathan Lloyd\n",
        "\n",
        "Homework 2, Question 2\n",
        "\n",
        "\n",
        "Goal: ConvNets while renowned for their prowess in image processing, have also demonstrated strong capabilities in handling sequential data such as text. In this problem, you will be applying these principles of CNNs to a classic problem in natural language processing - sentiment analysis.\n",
        "\n",
        "Tools: Numpy, PyTorch, Keras (TensorFlow)\n",
        "\n",
        "Data: IMDB movie reviews dataset provided by `tensorflow.keras.datasets.imdb`\n",
        "\n",
        "Task: Load the IMDB dataset from Keras using a vocabulary size of 2000 for tokenization & numericalization. Each review in the dataset is already pre-processed and encoded as a sequence of word indexes. A mapping between words and their corresponding indexes is provided using the `imdb.get_word_index()` method. For consistent input to the model, your task is to pad the reviews or truncate them to a uniform length. This can be achieved using the `pad_sequences` method from Keras to convert all reviews to a length of 300 words using the `maxlen` argument in the `pad_sequences` method. The preprocessed NumPy arrays will then be fed into a PyTorch CNN for sentiment classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3133060d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update packages in Colab server (no torchtext)\n",
        "%pip install --force-reinstall \"torch==2.3.0\" --upgrade numpy pandas matplotlib tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7f1bc0e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8f9b5a86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Hyperparameters for data processing\n",
        "VOCAB_SIZE = 2000  # as specified in the assignment\n",
        "MAX_LEN = 300      # fixed sequence length\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_SEED = 42\n",
        "\n",
        "# Load IMDB dataset from Keras (already tokenized and indexed)\n",
        "(X_train_full, y_train_full), (X_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
        "\n",
        "# Get the word -> index mapping (for analysis / interpretability)\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Pad and truncate to fixed length of 300 tokens\n",
        "X_train_full = pad_sequences(X_train_full, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "# Create validation split from training data\n",
        "np.random.seed(SHUFFLE_SEED)\n",
        "indices = np.random.permutation(len(X_train_full))\n",
        "X_train_full = X_train_full[indices]\n",
        "y_train_full = np.array(y_train_full)[indices]\n",
        "\n",
        "n_val = 1000\n",
        "X_val = X_train_full[:n_val]\n",
        "y_val = y_train_full[:n_val]\n",
        "X_train = X_train_full[n_val:]\n",
        "y_train = y_train_full[n_val:]\n",
        "\n",
        "# Summary\n",
        "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
        "print(f\"Sequence length: {MAX_LEN}, Batch size: {BATCH_SIZE}, Vocab size (num_words): {VOCAB_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35609e4",
      "metadata": {},
      "source": [
        "Architecture:\n",
        "\n",
        "The architecture of the convolutional neural network model for this problem is as follows:\n",
        "\n",
        "Embedding Layer:\n",
        "Input Vocabulary Size: 2000 words\n",
        "Embedding Dimension: 16\n",
        "Input Length: 300 words\n",
        "\n",
        "Conv1D Layer:\n",
        "Filters: 128\n",
        "Kernel Size: 3\n",
        "Activation: ReLU\n",
        "Stride: 1\n",
        "Padding: Valid\n",
        "\n",
        "GlobalMaxPooling1D Layer\n",
        "\n",
        "Dense Layer:\n",
        "Units: 1\n",
        "Activation: Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1f22d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch CNN Model Definition\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int = 16,\n",
        "        num_filters: int = 128,\n",
        "        kernel_size: int = 3,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # Embedding layer: maps word indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # 1D convolution over the sequence (time) dimension\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=embed_dim,\n",
        "            out_channels=num_filters,\n",
        "            kernel_size=kernel_size,\n",
        "        )\n",
        "\n",
        "        # Global max pooling over the time dimension\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc = nn.Linear(num_filters, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        x: LongTensor of shape (batch_size, seq_len)\n",
        "        returns: probabilities of shape (batch_size,)\n",
        "        \"\"\"\n",
        "        # (batch, seq_len) -> (batch, seq_len, embed_dim)\n",
        "        embedded = self.embedding(x)\n",
        "        # (batch, seq_len, embed_dim) -> (batch, embed_dim, seq_len)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "\n",
        "        # Convolution + ReLU\n",
        "        conv_out = torch.relu(self.conv(embedded))  # (batch, num_filters, L')\n",
        "\n",
        "        # Global max pooling over time -> (batch, num_filters, 1)\n",
        "        pooled = self.global_max_pool(conv_out).squeeze(-1)  # (batch, num_filters)\n",
        "\n",
        "        # Linear layer to a single logit\n",
        "        logits = self.fc(pooled).squeeze(-1)  # (batch,)\n",
        "\n",
        "        # Sigmoid for binary sentiment probability\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return probs\n",
        "\n",
        "\n",
        "# Instantiate model (will be moved to appropriate device in training cell)\n",
        "EMBED_DIM = 16\n",
        "NUM_FILTERS = 128\n",
        "KERNEL_SIZE = 3\n",
        "\n",
        "# VOCAB_SIZE is defined in the preprocessing cell\n",
        "model = TextCNN(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, num_filters=NUM_FILTERS, kernel_size=KERNEL_SIZE)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c947bbbc",
      "metadata": {},
      "source": [
        "Training: The model should be compiled using the 'binary_crossentropy' as the loss function and 'adam' optimizer. Additionally, 'accuracy' should be assigned as the main metric. A subset of the training data (1000 samples) should be set aside as a validation set, while the rest should be used for training. The model should be trained for a total of 30 (or 10) epochs, with a batch size of 32. After training, the model should be evaluated on the test data to obtain the final accuracy score. This will give a measure of how well the model can generalize to unseen reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb568b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1004a046",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train and Run Model\n",
        "\n",
        "# Instantiate model \n",
        "\n",
        "\n",
        "# Run experiment "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8e98b3",
      "metadata": {},
      "source": [
        "Visualization:\n",
        "Plot the accuracy and loss for both training and validation datasets across epochs to analyze the performance of the model over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e34a24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plots across epochs\n",
        "# Plot accuracy and loss for both training and validation - build a quad "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d639a945",
      "metadata": {},
      "source": [
        "Deliverables: \n",
        "\n",
        "1. Model Accuracy and Loss Curves: A detailed report of the performance of the model, focusing on accuracy and loss curves.\n",
        "2. Analysis of Model Performance: A thorough analysis should be conducted to discuss the results obtained from the model. This analysis should include \n",
        "\n",
        "a. Whether the model overfits or underfits the training data. \n",
        "\n",
        "b. Examination of the loss and accuracy curves to identify potential indicators of the model's behavior (such as plateaus or sharp changes).\n",
        "\n",
        "3. Code and Resources: Please make sure to submit your working code files along with the final results and the plots.\n",
        "\n",
        "4. Bonus (+1) Model Optimization: Consider experimenting with other architectures or hyperparameters to further optimize the model's performance. Discuss the outcomes of your experiments and the effect of different parameters on the accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a637878f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
