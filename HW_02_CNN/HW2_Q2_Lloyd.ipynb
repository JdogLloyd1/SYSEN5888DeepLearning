{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b02aba61",
      "metadata": {},
      "source": [
        "SYSEN 5888 Spring 2026\n",
        "\n",
        "Jonathan Lloyd\n",
        "\n",
        "Homework 2, Question 2\n",
        "\n",
        "\n",
        "Goal: ConvNets while renowned for their prowess in image processing, have also demonstrated strong capabilities in handling sequential data such as text. In this problem, you will be applying these principles of CNNs to a classic problem in natural language processing - sentiment analysis.\n",
        "\n",
        "Tools: Numpy, PyTorch, TorchText\n",
        "\n",
        "Data: IMBD movie reviews dataset torchtext.datasets.IMDB()\n",
        "\n",
        "Task: Load dataset, use TorchText processing to handle a vocabulary size of 2000 for tokenization & numericalization. Each review in the dataset is already pre-processed and encoded as a sequence of word indexes. A mapping between words and their corresponding indexes is provided using the imdb.get_word_index() method. For consistent input to the model, your task is to pad the reviews or truncate them to a uniform length. This can be achieved using the pad_sequences method from Keras to convert all reviews to a length of 300 words using the maxlen argument in the pad_sequences method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f9b5a86",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "IMDB.__init__() missing 3 required positional arguments: 'path', 'text_field', and 'label_field'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m \n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Import dataset \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m imdbRawData = \u001b[43mtorchtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIMDB\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Image scaling \u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Image normalization and data augmentation\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Define batch sizes and shuffle seed\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: IMDB.__init__() missing 3 required positional arguments: 'path', 'text_field', and 'label_field'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext \n",
        "\n",
        "# Import dataset \n",
        "imdbRawData = torchtext.datasets.IMDB()\n",
        "\n",
        "# Tokenize and numericalize\n",
        "\n",
        "# Pad and truncate\n",
        "\n",
        "# Define training and testing datasets\n",
        "\n",
        "# Define batch sizes and shuffle seed to ensure random distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35609e4",
      "metadata": {},
      "source": [
        "Architecture:\n",
        "\n",
        "The architecture of the convolutional neural network model for this problem is as follows:\n",
        "\n",
        "Embedding Layer:\n",
        "Input Vocabulary Size: 2000 words\n",
        "Embedding Dimension: 16\n",
        "Input Length: 300 words\n",
        "\n",
        "Conv1D Layer:\n",
        "Filters: 128\n",
        "Kernel Size: 3\n",
        "Activation: ReLU\n",
        "Stride: 1\n",
        "Padding: Valid\n",
        "\n",
        "GlobalMaxPooling1D Layer\n",
        "\n",
        "Dense Layer:\n",
        "Units: 1\n",
        "Activation: Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1f22d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Definition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c947bbbc",
      "metadata": {},
      "source": [
        "Training: The model should be compiled using the 'binary_crossentropy' as the loss function and 'adam' optimizer. Additionally, 'accuracy' should be assigned as the main metric. A subset of the training data (1000 samples) should be set aside as a validation set, while the rest should be used for training. The model should be trained for a total of 30 (or 10) epochs, with a batch size of 32. After training, the model should be evaluated on the test data to obtain the final accuracy score. This will give a measure of how well the model can generalize to unseen reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1004a046",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and Run Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8e98b3",
      "metadata": {},
      "source": [
        "Visualization:\n",
        "Plot the accuracy and loss for both training and validation datasets across epochs to analyze the performance of the model over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e34a24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plots across epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d639a945",
      "metadata": {},
      "source": [
        "Deliverables: \n",
        "\n",
        "1. Model Accuracy and Loss Curves: A detailed report of the performance of the model, focusing on accuracy and loss curves.\n",
        "2. Analysis of Model Performance: A thorough analysis should be conducted to discuss the results obtained from the model. This analysis should include \n",
        "\n",
        "a. Whether the model overfits or underfits the training data. \n",
        "\n",
        "b. Examination of the loss and accuracy curves to identify potential indicators of the model's behavior (such as plateaus or sharp changes).\n",
        "\n",
        "3. Code and Resources: Please make sure to submit your working code files along with the final results and the plots.\n",
        "\n",
        "4. Bonus (+1) Model Optimization: Consider experimenting with other architectures or hyperparameters to further optimize the model's performance. Discuss the outcomes of your experiments and the effect of different parameters on the accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a637878f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
