{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p62G8M_viUJp"
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "\n",
    "- Playing CartPole with the Actor-Critic Method\n",
    "- Deep Deterministic Policy Gradient (DDPG)for the classic Inverted Pendulum control problem\n",
    "- Deep Q-Learning for Atari Breakout\n",
    "- Proximal Policy Optimization\n",
    "\n",
    "\n",
    "# Playing CartPole with the Actor-Critic Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFgN7h_wiUJq"
   },
   "source": [
    "This tutorial demonstrates how to implement the [Actor-Critic](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf) method to train an agent on the [Open AI Gym](https://gym.openai.com/) CartPole-V0 environment.\n",
    "This is built on [policy gradient methods](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) of reinforcement learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kA10ZKRR0hi"
   },
   "source": [
    "**Actor-Critic methods**\n",
    "\n",
    "Actor-Critic methods are [temporal difference (TD) learning](https://en.wikipedia.org/wiki/Temporal_difference_learning) methods that represent the policy function independent of the value function. \n",
    "\n",
    "A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state.\n",
    "A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\n",
    "\n",
    "In the Actor-Critic method, the policy is referred to as the *actor* that proposes a set of possible actions given a state, and the estimated value function is referred to as the *critic*, which evaluates actions taken by the *actor* based on the given policy.\n",
    "\n",
    "In this tutorial, both the *Actor* and *Critic* will be represented using one neural network with two outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBfiafKSRs2k"
   },
   "source": [
    "**CartPole-v0**\n",
    "\n",
    "In the [CartPole-v0 environment](https://gym.openai.com/envs/CartPole-v0), a pole is attached to a cart moving along a frictionless track. \n",
    "The pole starts upright and the goal of the agent is to prevent it from falling over by applying a force of -1 or +1 to the cart. \n",
    "A reward of +1 is given for every time step the pole remains upright.\n",
    "An episode ends when (1) the pole is more than 15 degrees from vertical or (2) the cart moves more than 2.4 units from the center.\n",
    "\n",
    "![title](https://www.tensorflow.org/tutorials/reinforcement_learning/images/cartpole-v0.gif)\n",
    "\n",
    "<center>\n",
    "  <figure>\n",
    "    <image src=\"https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic_files/output_TLd720SejKmf_0.gif\">\n",
    "    <figcaption>\n",
    "      Trained actor-critic model in Cartpole-v0 environment\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSNVK0AeRoJd"
   },
   "source": [
    "The problem is considered \"solved\" when the average total reward for the episode reaches 195 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glLwIctHiUJq"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary packages and configure global settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "13l6BbxKhCKp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from gym) (3.1.1)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827728 sha256=648482f22aca008853ed471c6d604f5ec8537276b81cb25f87bd2a85637334cf\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
      "Successfully built gym\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: gym_notices, gym\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed gym-0.26.2 gym_notices-0.0.8\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/conda/envs/torch/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/torch/lib/python3.10/site-packages (0.14.1+cu117)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/envs/torch/lib/python3.10/site-packages (0.13.1+cu117)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/torch/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/torch/lib/python3.10/site-packages (from torchvision) (2.32.3)\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/torch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (75.8.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/torch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.45.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/torch/lib/python3.10/site-packages (from requests->torchvision) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/torch/lib/python3.10/site-packages (from requests->torchvision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/torch/lib/python3.10/site-packages (from requests->torchvision) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/torch/lib/python3.10/site-packages (from requests->torchvision) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchmetrics 1.7.0 requires torch>=2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WBeQhPi2S4m5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install additional packages for visualization\n",
    "# sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "pip install pyvirtualdisplay > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tT4N3qYviUJr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOUCe2D0iUJu"
   },
   "source": [
    "## Model\n",
    "\n",
    "The *Actor* and *Critic* will be modeled using one neural network that generates the action probabilities and critic value respectively. This tutorial uses model subclassing to define the model. \n",
    "\n",
    "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value $V$, which models the state-dependent [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions). The goal is to train a model that chooses actions based on a policy $\\pi$ that maximizes expected [return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return).\n",
    "\n",
    "For Cartpole-v0, there are four values representing the state: cart position, cart-velocity, pole angle and pole velocity respectively. The agent can take two actions to push the cart left (0) and right (1) respectively.\n",
    "\n",
    "Refer to [OpenAI Gym's CartPole-v0 wiki page](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aXKbbMC-kmuv"
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.common = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(hidden_size, num_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.common(x)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nWyxJgjLn68c"
   },
   "outputs": [],
   "source": [
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "hidden_size = 128\n",
    "\n",
    "model = ActorCritic(num_inputs, num_actions, hidden_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk92njFziUJw"
   },
   "source": [
    "## Training\n",
    "\n",
    "To train the agent, you will follow these steps:\n",
    "\n",
    "1. Run the agent on the environment to collect training data per episode.\n",
    "2. Compute expected return at each time step.\n",
    "3. Compute the loss for the combined actor-critic model.\n",
    "4. Compute gradients and update network parameters.\n",
    "5. Repeat 1-4 until either success criterion or max episodes has been reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2nde2XDs8Gh"
   },
   "source": [
    "### 1. Collecting training data\n",
    "\n",
    "As in supervised learning, in order to train the actor-critic model, you need\n",
    "to have training data. However, in order to collect such data, the model would\n",
    "need to be \"run\" in the environment.\n",
    "\n",
    "Training data is collected for each episode. Then at each time step, the model's forward pass will be run on the environment's state in order to generate action probabilities and the critic value based on the current policy parameterized by the model's weights.\n",
    "\n",
    "The next action will be sampled from the action probabilities generated by the model, which would then be applied to the environment, causing the next state and reward to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5URrbGlDSAGx"
   },
   "outputs": [],
   "source": [
    "def env_step(action):\n",
    "    state_info = env.step(action)\n",
    "    if isinstance(state_info, tuple):\n",
    "        state, reward, done, _ = state_info\n",
    "    else:\n",
    "        state, reward, done = state_info\n",
    "    return torch.tensor(state, dtype=torch.float32), reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a4qVRV063Cl9"
   },
   "outputs": [],
   "source": [
    "def run_episode(model, max_steps):\n",
    "    state_info = env.reset()\n",
    "    state = torch.tensor(state_info[0] if isinstance(state_info, tuple) else state_info, dtype=torch.float32)\n",
    "    log_probs, values, rewards = [], [], []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        logits, value = model(state)\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "        # Append log probabilities and values\n",
    "        log_probs.append(torch.log(action_probs[action]).unsqueeze(0))\n",
    "        values.append(value)\n",
    "\n",
    "        state_info = env.step(action)\n",
    "        if isinstance(state_info, tuple):\n",
    "            state, reward, done, *_ = state_info\n",
    "        else:\n",
    "            state, reward, done = state_info\n",
    "\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return log_probs, values, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBnIHdz22dIx"
   },
   "source": [
    "### 2. Computing expected returns\n",
    "\n",
    "The sequence of rewards for each timestep $t$, $\\{r_{t}\\}^{T}_{t=1}$ collected during one episode is converted into a sequence of expected returns $\\{G_{t}\\}^{T}_{t=1}$ in which the sum of rewards is taken from the current timestep $t$ to $T$ and each reward is multiplied with an exponentially decaying discount factor $\\gamma$:\n",
    "\n",
    "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
    "\n",
    "Since $\\gamma\\in(0,1)$, rewards further out from the current timestep are given less weight.\n",
    "\n",
    "Intuitively, expected return simply implies that rewards now are better than rewards later. In a mathematical sense, it is to ensure that the sum of the rewards converges.\n",
    "\n",
    "To stabilize training, the resulting sequence of returns is also standardized (i.e. to have zero mean and unit standard deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jpEwFyl315dl"
   },
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    discounted_sum = 0\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        returns.insert(0, discounted_sum)\n",
    "\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    return (returns - returns.mean()) / (returns.std() + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hrPLrgGxlvb"
   },
   "source": [
    "### 3. The actor-critic loss\n",
    "\n",
    "Since a hybrid actor-critic model is used, the chosen loss function is a combination of actor and critic losses for training, as shown below:\n",
    "\n",
    "$$L = L_{actor} + L_{critic}$$\n",
    "\n",
    "#### Actor loss\n",
    "\n",
    "The actor loss is based on [policy gradients with the critic as a state dependent baseline](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) and computed with single-sample (per-episode) estimates.\n",
    "\n",
    "$$L_{actor} = -\\sum^{T}_{t=1} log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
    "\n",
    "where:\n",
    "- $T$: the number of timesteps per episode, which can vary per episode\n",
    "- $s_{t}$: the state at timestep $t$\n",
    "- $a_{t}$: chosen action at timestep $t$ given state $s$\n",
    "- $\\pi_{\\theta}$: is the policy (actor) parameterized by $\\theta$\n",
    "- $V^{\\pi}_{\\theta}$: is the value function (critic) also parameterized by $\\theta$\n",
    "- $G = G_{t}$: the expected return for a given state, action pair at timestep $t$\n",
    "\n",
    "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Advantage\n",
    "\n",
    "The $G - V$ term in our $L_{actor}$ formulation is called the [advantage](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions), which indicates how much better an action is given a particular state over a random action selected according to the policy $\\pi$ for that state.\n",
    "\n",
    "While it's possible to exclude a baseline, this may result in high variance during training. And the nice thing about choosing the critic $V$ as a baseline is that it trained to be as close as possible to $G$, leading to a lower variance.\n",
    "\n",
    "In addition, without the critic, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
    "\n",
    "For instance, suppose that two actions for a given state would yield the same expected return. Without the critic, the algorithm would try to raise the probability of these actions based on the objective $J$. With the critic, it may turn out that there's no advantage ($G - V = 0$) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Critic loss\n",
    "\n",
    "Training $V$ to be as close possible to $G$ can be set up as a regression problem with the following loss function:\n",
    "\n",
    "$$L_{critic} = L_{\\delta}(G, V^{\\pi}_{\\theta})$$\n",
    "\n",
    "where $L_{\\delta}$ is the [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is less sensitive to outliers in data than squared-error loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9EXwbEez6n9m"
   },
   "outputs": [],
   "source": [
    "def compute_loss(log_probs, values, returns):\n",
    "    advantage = returns - torch.cat(values)\n",
    "    actor_loss = -torch.sum(torch.cat(log_probs) * advantage.detach())\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSYkQOmRfV75"
   },
   "source": [
    "### 4. Defining the training step to update parameters\n",
    "\n",
    "All of the steps above are combined into a training step that is run every episode. All steps leading up to the loss function are executed with the `tf.GradientTape` context to enable automatic differentiation.\n",
    "\n",
    "This tutorial uses the Adam optimizer to apply the gradients to the model parameters.\n",
    "\n",
    "The sum of the undiscounted rewards, `episode_reward`, is also computed in this step. This value will be used later on to evaluate if the success criterion is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QoccrkF3IFCg"
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, gamma=0.99, max_steps_per_episode=1000):\n",
    "    log_probs, values, rewards = run_episode(model, max_steps_per_episode)\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "\n",
    "    loss = compute_loss(log_probs, values, returns)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFvZiDoAflGK"
   },
   "source": [
    "### 5. Run the training loop\n",
    "\n",
    "Training is executed by running the training step until either the success criterion or maximum number of episodes is reached.  \n",
    "\n",
    "A running record of episode rewards is kept in a queue. Once 100 trials are reached, the oldest reward is removed at the left (tail) end of the queue and the newest one is added at the head (right). A running sum of the rewards is also maintained for computational efficiency. \n",
    "\n",
    "Depending on your runtime, training can finish in less than a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kbmBxnzLiUJx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved at episode 100: average reward: 350.85!\n"
     ]
    }
   ],
   "source": [
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "episode_rewards = deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "for i in range(max_episodes):\n",
    "    episode_reward = train_step(model, optimizer)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    running_reward = np.mean(episode_rewards)\n",
    "\n",
    "    if running_reward > reward_threshold and i >= min_episodes_criterion:\n",
    "        print(f'Solved at episode {i}: average reward: {running_reward:.2f}!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru8BEwS1EmAv"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "After training, it would be good to visualize how the model performs in the environment. You can run the cells below to generate a GIF animation of one episode run of the model. Note that additional packages need to be installed for OpenAI Gym to render the environment's images correctly in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyvirtualdisplay in /opt/conda/envs/torch/lib/python3.10/site-packages (3.0)\n",
      "Collecting xvfbwrapper\n",
      "  Downloading xvfbwrapper-0.2.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "Downloading xvfbwrapper-0.2.10-py3-none-any.whl (6.1 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: xvfbwrapper\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed xvfbwrapper-0.2.10\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gym[classic_control] in /opt/conda/envs/torch/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from gym[classic_control]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from gym[classic_control]) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /opt/conda/envs/torch/lib/python3.10/site-packages (from gym[classic_control]) (0.0.8)\n",
      "Collecting pygame==2.1.0 (from gym[classic_control])\n",
      "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pygame\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pygame-2.1.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tqdm in /opt/conda/envs/torch/lib/python3.10/site-packages (4.67.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyvirtualdisplay xvfbwrapper\n",
    "!pip install gym[classic_control]\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qbIMMkfmRHyC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "def render_episode(env, model, max_steps):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        frame = env.render()\n",
    "        frames.append(Image.fromarray(frame))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(state.unsqueeze(0))[0]\n",
    "        action = torch.argmax(torch.softmax(logits, dim=-1)).item()\n",
    "\n",
    "        state, _, done, *_ = env.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return frames\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "frames = render_episode(env, model, max_steps=1000)\n",
    "frames[0].save('cartpole.gif', save_all=True, append_images=frames[1:], loop=0, duration=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic_files/output_TLd720SejKmf_0.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)for the classic Inverted Pendulum control problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deep Deterministic Policy Gradient (DDPG) is a model-free off-policy algorithm for learning continous actions.\n",
    "\n",
    "It combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network). It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces.\n",
    "\n",
    "This tutorial closely follow this paper - Continuous control with deep reinforcement learning https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "\n",
    "## Problem\n",
    "\n",
    "We are trying to solve the classic Inverted Pendulum control problem. In this setting, we can take only two actions: swing left or swing right.\n",
    "\n",
    "What make this problem challenging for Q-Learning Algorithms is that actions are continuous instead of being discrete. That is, instead of using two discrete actions like -1 or +1, we have to select from infinite actions ranging from -2 to +2.\n",
    "\n",
    "## Quick theory\n",
    "Just like the Actor-Critic method, we have two networks:\n",
    "\n",
    "Actor - It proposes an action given a state.\n",
    "Critic - It predicts if the action is good (positive value) or bad (negative value) given a state and an action.\n",
    "DDPG uses two more techniques not present in the original DQN:\n",
    "\n",
    "First, it uses two Target networks.\n",
    "\n",
    "Why? Because it add stability to training. In short, we are learning from estimated targets and Target networks are updated slowly, hence keeping our estimated targets stable.\n",
    "\n",
    "Conceptually, this is like saying, \"I have an idea of how to play this well, I'm going to try it out for a bit until I find something better\", as opposed to saying \"I'm going to re-learn how to play this entire game after every move\". See this StackOverflow answer.\n",
    "\n",
    "Second, it uses Experience Replay.\n",
    "\n",
    "We store list of tuples (state, action, reward, next_state), and instead of learning only from recent experience, we learn from sampling all of our experience accumulated so far.\n",
    "\n",
    "Now, let's see how is it implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use OpenAIGym to create the environment. We will use the upper_bound parameter to scale our actions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: 3\n",
      "Action space: 1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"State space:\", num_states)\n",
    "print(\"Action space:\", num_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement better exploration by the Actor network, we use noisy perturbations, specifically an Ornstein-Uhlenbeck process for generating noise, as described in the paper. It samples noise from a correlated normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * torch.sqrt(torch.tensor(self.dt)) * torch.randn_like(self.mean)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x_initial if self.x_initial is not None else torch.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Buffer class implements Experience Replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://i.imgur.com/mS6iGyJ.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critic loss** - Mean Squared Error of y - Q(s, a) where y is the expected return as seen by the Target network, and Q(s, a) is action value predicted by the Critic network. y is a moving target that the critic model tries to achieve; we make this target stable by updating the Target model slowly.\n",
    "\n",
    "**Actor loss** - This is computed using the mean of the value given by the Critic network for the actions taken by the Actor network. We seek to maximize this quantity.\n",
    "\n",
    "Hence we update the Actor network so that it produces actions that get the maximum predicted value as seen by the Critic, for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def sample(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = torch.tensor(self.state_buffer[batch_indices], dtype=torch.float32)\n",
    "        action_batch = torch.tensor(self.action_buffer[batch_indices], dtype=torch.float32)\n",
    "        reward_batch = torch.tensor(self.reward_buffer[batch_indices], dtype=torch.float32)\n",
    "        next_state_batch = torch.tensor(self.next_state_buffer[batch_indices], dtype=torch.float32)\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the Actor and Critic networks. These are basic Dense models with ReLU activation.\n",
    "\n",
    "Note: We need the initialization for last layer of the Actor to be between -0.003 and 0.003 as this prevents us from getting 1 or -1 output values in the initial stages, which would squash our gradients to zero, as we use the tanh activation.\n",
    "\n",
    "def get_actor():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.out = nn.Linear(256, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.out(x)) * self.action_bound\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.out = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(num_states, num_actions, upper_bound)\n",
    "critic = Critic(num_states, num_actions)\n",
    "\n",
    "actor_target = Actor(num_states, num_actions, upper_bound)\n",
    "critic_target = Critic(num_states, num_actions)\n",
    "\n",
    "actor_target.load_state_dict(actor.state_dict())\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement our main training loop, and iterate over episodes. We sample actions using policy() and train with learn() at each time step, along with updating the Target networks at a rate tau.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 1/100 [00:02<04:34,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Avg Reward: -6462.32644833403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 2/100 [00:05<04:38,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Avg Reward: -7004.118412147303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 3/100 [00:08<04:38,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2, Avg Reward: -6533.6602679720645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 4/100 [00:11<04:36,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3, Avg Reward: -6014.594160592539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 5/100 [00:14<04:34,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4, Avg Reward: -4864.77349032798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 6/100 [00:17<04:31,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5, Avg Reward: -4134.487266891069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 7/100 [00:20<04:28,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6, Avg Reward: -3580.3606354599474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 8/100 [00:22<04:25,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7, Avg Reward: -3163.840674195263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 9/100 [00:25<04:22,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8, Avg Reward: -2826.425721647952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 10/100 [00:28<04:18,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9, Avg Reward: -2555.9467279160594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|█         | 11/100 [00:31<04:15,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Avg Reward: -2334.2238062504525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 12/100 [00:34<04:12,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11, Avg Reward: -2150.6511577195583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 13/100 [00:37<04:09,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12, Avg Reward: -1985.8952476386887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 14/100 [00:40<04:07,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13, Avg Reward: -1852.9986365272073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 15/100 [00:43<04:04,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14, Avg Reward: -1738.4160472024384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 16/100 [00:45<04:01,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15, Avg Reward: -1637.143891998895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|█▋        | 17/100 [00:48<03:58,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16, Avg Reward: -1571.2771260231766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 18/100 [00:51<03:55,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17, Avg Reward: -1512.9434600418338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 19/100 [00:54<03:53,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18, Avg Reward: -1486.1666596664697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 20/100 [00:57<03:50,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19, Avg Reward: -1470.515638451796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  21%|██        | 21/100 [01:00<03:47,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20, Avg Reward: -1462.6818372121343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|██▏       | 22/100 [01:03<03:44,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21, Avg Reward: -1402.635200458159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  23%|██▎       | 23/100 [01:06<03:41,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22, Avg Reward: -1412.960447047984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|██▍       | 24/100 [01:08<03:38,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23, Avg Reward: -1376.1658111648758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▌       | 25/100 [01:11<03:35,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24, Avg Reward: -1336.566821264356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  26%|██▌       | 26/100 [01:14<03:32,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25, Avg Reward: -1295.522222751685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  27%|██▋       | 27/100 [01:17<03:29,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26, Avg Reward: -1256.2909400034637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  28%|██▊       | 28/100 [01:20<03:27,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27, Avg Reward: -1220.354733321065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  29%|██▉       | 29/100 [01:23<03:24,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28, Avg Reward: -1189.6668907484977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  30%|███       | 30/100 [01:26<03:21,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29, Avg Reward: -1158.3809422009303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  31%|███       | 31/100 [01:29<03:18,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30, Avg Reward: -1132.1061266502875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  32%|███▏      | 32/100 [01:31<03:15,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31, Avg Reward: -1100.8800390730264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 33/100 [01:34<03:12,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32, Avg Reward: -1071.6818569939044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  34%|███▍      | 34/100 [01:37<03:09,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33, Avg Reward: -1047.4389182127682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 35/100 [01:40<03:06,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34, Avg Reward: -1024.420284523407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  36%|███▌      | 36/100 [01:43<03:03,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35, Avg Reward: -1002.3695292269074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  37%|███▋      | 37/100 [01:46<03:02,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36, Avg Reward: -984.501472215017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  38%|███▊      | 38/100 [01:49<03:00,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37, Avg Reward: -961.6430523412397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  39%|███▉      | 39/100 [01:52<02:57,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 38, Avg Reward: -937.0851100338579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████      | 40/100 [01:55<02:54,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39, Avg Reward: -916.8103743980613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  41%|████      | 41/100 [01:58<02:52,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40, Avg Reward: -758.3465389305578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  42%|████▏     | 42/100 [02:01<02:49,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 41, Avg Reward: -578.9913840724305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  43%|████▎     | 43/100 [02:03<02:46,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42, Avg Reward: -445.9608812788316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  44%|████▍     | 44/100 [02:06<02:43,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 43, Avg Reward: -353.078609404524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 45/100 [02:09<02:40,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 44, Avg Reward: -352.63011520913744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  46%|████▌     | 46/100 [02:12<02:37,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 45, Avg Reward: -347.0128099540527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  47%|████▋     | 47/100 [02:15<02:34,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 46, Avg Reward: -343.90625610785844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  48%|████▊     | 48/100 [02:18<02:31,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 47, Avg Reward: -346.54086241210143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  49%|████▉     | 49/100 [02:21<02:28,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 48, Avg Reward: -349.6826200321224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  50%|█████     | 50/100 [02:24<02:25,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49, Avg Reward: -350.1557340984581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  51%|█████     | 51/100 [02:27<02:22,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Avg Reward: -353.5056417563313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  52%|█████▏    | 52/100 [02:30<02:19,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 51, Avg Reward: -353.70822018719724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  53%|█████▎    | 53/100 [02:33<02:17,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 52, Avg Reward: -356.9240187837839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  54%|█████▍    | 54/100 [02:35<02:13,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 53, Avg Reward: -359.7950979984814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▌    | 55/100 [02:38<02:11,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 54, Avg Reward: -359.9120699364153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  56%|█████▌    | 56/100 [02:41<02:08,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 55, Avg Reward: -363.79114207702395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  57%|█████▋    | 57/100 [02:44<02:05,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 56, Avg Reward: -354.9374051276065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  58%|█████▊    | 58/100 [02:47<02:02,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 57, Avg Reward: -348.88247051072995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  59%|█████▉    | 59/100 [02:50<01:59,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 58, Avg Reward: -330.4508973268738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|██████    | 60/100 [02:53<01:57,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 59, Avg Reward: -302.1891134518053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  61%|██████    | 61/100 [02:56<01:54,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60, Avg Reward: -295.34697346334445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  62%|██████▏   | 62/100 [02:59<01:51,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 61, Avg Reward: -296.54621754069626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  63%|██████▎   | 63/100 [03:02<01:49,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 62, Avg Reward: -273.13772485751485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  64%|██████▍   | 64/100 [03:05<01:47,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 63, Avg Reward: -271.4091907007495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  65%|██████▌   | 65/100 [03:08<01:44,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 64, Avg Reward: -269.8589885500133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  66%|██████▌   | 66/100 [03:11<01:41,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 65, Avg Reward: -292.36825293967206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 67/100 [03:14<01:39,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 66, Avg Reward: -318.81665674373755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  68%|██████▊   | 68/100 [03:17<01:36,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 67, Avg Reward: -316.17543874077387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  69%|██████▉   | 69/100 [03:20<01:34,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 68, Avg Reward: -308.6183475375575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 70/100 [03:23<01:31,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 69, Avg Reward: -306.3981451577188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  71%|███████   | 71/100 [03:26<01:29,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70, Avg Reward: -301.68782808463504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  72%|███████▏  | 72/100 [03:30<01:26,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 71, Avg Reward: -305.73982676375743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  73%|███████▎  | 73/100 [03:33<01:24,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 72, Avg Reward: -306.3677473062523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  74%|███████▍  | 74/100 [03:36<01:21,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 73, Avg Reward: -301.27361832915244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  75%|███████▌  | 75/100 [03:39<01:19,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 74, Avg Reward: -302.42343410923314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  76%|███████▌  | 76/100 [03:42<01:16,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 75, Avg Reward: -303.9277641527827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  77%|███████▋  | 77/100 [03:46<01:14,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 76, Avg Reward: -305.2281711139417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  78%|███████▊  | 78/100 [03:49<01:11,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 77, Avg Reward: -303.7820850268974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  79%|███████▉  | 79/100 [03:52<01:08,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 78, Avg Reward: -308.1176201406935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████  | 80/100 [03:56<01:05,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 79, Avg Reward: -305.8706916500237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  81%|████████  | 81/100 [03:59<01:02,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80, Avg Reward: -306.9420139593609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  82%|████████▏ | 82/100 [04:02<00:59,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 81, Avg Reward: -327.06658952928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  83%|████████▎ | 83/100 [04:06<00:56,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 82, Avg Reward: -324.1935565830957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  84%|████████▍ | 84/100 [04:09<00:53,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 83, Avg Reward: -318.5074496829615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  85%|████████▌ | 85/100 [04:12<00:50,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 84, Avg Reward: -315.3108580899605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  86%|████████▌ | 86/100 [04:16<00:47,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 85, Avg Reward: -318.2438501505682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  87%|████████▋ | 87/100 [04:19<00:43,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 86, Avg Reward: -318.30010573236234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  88%|████████▊ | 88/100 [04:23<00:40,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 87, Avg Reward: -313.28741069883336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  89%|████████▉ | 89/100 [04:26<00:37,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 88, Avg Reward: -310.57492107428845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  90%|█████████ | 90/100 [04:29<00:34,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 89, Avg Reward: -310.3214259119915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  91%|█████████ | 91/100 [04:33<00:30,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90, Avg Reward: -312.2611874599471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  92%|█████████▏| 92/100 [04:36<00:27,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 91, Avg Reward: -312.33620979752146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  93%|█████████▎| 93/100 [04:40<00:23,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 92, Avg Reward: -312.2310935304443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  94%|█████████▍| 94/100 [04:43<00:20,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 93, Avg Reward: -309.307850118033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▌| 95/100 [04:46<00:16,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 94, Avg Reward: -311.64734214780253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  96%|█████████▌| 96/100 [04:50<00:13,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 95, Avg Reward: -305.0061016719691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  97%|█████████▋| 97/100 [04:53<00:10,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 96, Avg Reward: -304.33078031143157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  98%|█████████▊| 98/100 [04:56<00:06,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 97, Avg Reward: -304.7366856232059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  99%|█████████▉| 99/100 [05:00<00:03,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 98, Avg Reward: -301.43223753376236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 100/100 [05:03<00:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99, Avg Reward: -306.8189953799518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAGzCAYAAADg2in0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUsFJREFUeJzt3XtcVGX+B/DPXJgZrgNyGxBUVBLvFyzC1HJjxbILu/0qLymWrelqilreRdtqMUxXzcp1K63NstwtKy8kS2mpSIp4TVATxQsDIpdBkGEu5/cHcHKClMEZhoHP+/Wal8w5zznzPad0PjznOc+RCIIggIiIiIgaTeroAoiIiIicDQMUERERkZUYoIiIiIisxABFREREZCUGKCIiIiIrMUARERERWYkBioiIiMhKDFBEREREVmKAIiIiIrISAxQRERGRleSOLqClevvtt7F8+XJotVr07dsXb731Fu65555GbWs2m3HlyhV4enpCIpHYuVIiIiKyBUEQUF5ejuDgYEilt+5jkvBZePV99tlnGD9+PNatW4eoqCisWrUKW7ZsQU5ODgICAm67/aVLlxAaGtoMlRIREZGtXbx4ESEhIbdswwDVgKioKNx9991Yu3YtgJoepdDQULz44ouYN2/ebbcvKyuDt7c3Ll68CC8vL3uXS0RERDag0+kQGhqK0tJSqNXqW7blJbzfqK6uRmZmJubPny8uk0qliImJQXp6eoPb6PV66PV68X15eTkAwMvLiwGKiIjIyTRm+A0Hkf9GUVERTCYTAgMDLZYHBgZCq9U2uE1SUhLUarX44uU7IiKi1o0Bygbmz5+PsrIy8XXx4kVHl0RERER2xEt4v+Hn5weZTIaCggKL5QUFBdBoNA1uo1QqoVQqm6M8IiIiagHYA/UbCoUCkZGRSEtLE5eZzWakpaUhOjragZURERFRS8EeqAbMmjUL8fHxGDhwIO655x6sWrUKFRUVePbZZx1dGhEREbUADFANePrpp3H16lUkJiZCq9WiX79+SElJqTewnIiIiNomzgNlBzqdDmq1GmVlZZzGgIiIyElY8/3NMVBEREREVmKAIiIiIrISAxQRERGRlRigiIiIiKzEAEVERERkJU5jQEREv8tsFmAwm2E0CTCaBZjMAowmM4xmAZ4qOTxVLo4ukcghGKCIiAjasiqknyvC/rPXcCD3GgrK9DCazTDfYqIbqQToEeyFqDBfRIW1wz1h7eDtpmi+ookciPNA2QHngSIiR6usNuLHM0U4erEUJZXVKK6oRkmlAWWVBhjMZsilEkglEshlEpRXGXHhWmWj9y2VAHKpFNUms8VyiQTo016NYREBGNYtAL3bqyGVSm5Z47Xr1Si7YUC1yQyD0QyDSUC1yYTKahMq9EaUVxlRoTdBbzRB5SKDm0IGN4Uc7koZXGRSSCWARFJzLIIgoLLahHK9EeVVBlyvMqLaaIZMKhFfUokEcqkEcpkU8tplclntesmv7cwCYDKbYTLX/CkANZ+rkMFNWfMnAFSbzKiurdtoMkNSu3+ZVAKpVAKNlwpdAzwgu8V5sDezWUC1yQy9wYxqkxl1X/t1X/4SCaCQSSGXSeEik8BFKoXkNuUKQs2xVxlM0Btr9m0SBLjIJFDIpVDIpHCRSVGhN6Kk0oCSymqUVhpwXW+EWRAgCAIEAWJAl0gASe2fggDojWbcMJhQZTChylDz/2tdj6enSg4vVxeE+bqjg6+bTc+VNd/fDFB2wABFRI5wtVyP77MLsetnLX48UwS90Xz7jWpJJUDv9mrc28UXg7r4oYu/OxQyaW3AqPlilUtrQkddKCrQVeHAuWvIyC1Gxrlr+OVqhcU+fd0V6BvqDaNZQLXx1y/ashsGXKvQo8rQ+PqcmYdSjr6havQP9UHvEDVkEknNuag9J5XVJuhuGFB2wwBdlQHlVcba4FATHqoMJpgEAUq5DEq5tOblIoPsNyHHaBZQoTfiut6I61VGlOtr9mMwtc6v+ReGdsb8h7vbdJ8MUA7GAEXkOHqjCeVVRuhuGKCrqumJuFFd80VVZTChyljzG7hSLq39Tbn2S8lFCpWLDCq5DCqXmnUSSMTfxKVSCRQyKVwVMqjkNb+tO4LJLKC8qubL9lLJDRy7VIbjl0tx7FIZLpXcsGgb4uOKoXf5Q+Olgo+bC7zdFPBxU8BFJoFJqB3PZBbgIpWid4gaatc7G89UqKvC7pyr+D6nED+eKcJ1vfG22yjkUni7ukDpUtNjUddz4aqQwUMph7tSDg+lDEq5DHqjCRX6mt6pGwYjDEYBAgSYBdT2atSEFQ+lHJ4qOTxUcihkUpjMAkyCAHPt8ZoFAQbTr8dvNJlhql1eN85LelNPklwmgSAAldUmVFbX9IhVVBshAeBSW69CXhMuzYIAkwDxs/KuVaCi2nRH59XW6np7an6WwHSr67SNIJUAKhcZpBIJDKa6Xq5f16ldXeDjpoDazQUeSrnY+ymR/Pr3q6Z9zX9DiQRQymU1fx9r/14aTeaav9e1f6fLq4wYfU8oxkV3uqPaf4sBysEYoIjs770fz2FN2hncMJggCDWXIwRBuOWYHVuSSyVQyqWQ1H4D1H0hucilUMlr/tFX1n0ByGVQutT2HMhlkN98OUcCSCCBykUKN4UMrgo53BQ1XxiF5XoU6vQoLK/C1et68RLIrf7V7hnsheE9NPhjj0B0D/IU62tu1UYzDl0oxoVrlVDUBoy60Kp2dYGvuxLtPBRwV8gcVmNzMJkFnC4ox+G8EmTllSJHWw5p7f87df8/uCpkULvK4aVygZdrzSUqN0VdmK/5f0cqkaDaaBZ/EdAbzTD/5n8EmUQCD5Ucnsqa8OiurNmPUi4Tz79cKql3voWbwnS1qeaGgd8SBKHednXH8NtfJur2ZzDV/KJyq8u4LQ0DlIMxQBHZ17o9v2DZzuxbtvFU1Xwheark4m+yytrepbovo7pxIXWXUsReKoMJ1UZzbSiD2MtRbcUlMXtzdZEhwEuJXu3V6NNejd7t1ejZ/s57kYjaMmu+v3kXHhE5lfd+PCeGp5kxd+Gpu0PES20S1HT9e6jkdhm0KwjCTSGrJnjV/Qpa1wNmMAm/jl+pbVvXc6A31mxnNtcN4q350ywAN6pNuGEw4UZ1zSUqmRQI8FTB31OJAE8l/D2V8HZTQO3qArWrCxRyTuNH5EgMUETkNDbsy8Vr208BAGY8GI4ZMeHN+vkSiaS2N0vWrJ9LRC0Pf4UhIqfw7/TzeOWbnwEA04Z1RUIzhyciopsxQBFRi7fvbBEWf3USAPDC/Z0xe/hdrXrgMRG1fAxQRNSild0w4KUtRwEATw8MxbwREQxPRORwDFBE1KK98s1J5JdVoaOvGxIf7cHwREQtAgMUEbVYKSe0+OLwZUglwIon+8JdyfteiKhlYIAiohap6LoeC788DgB44f4uGNipnYMrIiL6FQMUEbU4giBg/hfHca2iGhEaT95xR0QtDgMUEbU4Xxy+jNSfC+Aik2DlU/2glHPeJSJqWRigiKhFqTKYsCylZqbxhJi70COYj0MiopaHAYqIWpSPD1zA1XI9Qnxc8ZchnR1dDhFRgxigiKjFqKw24t3dvwAApv8hnM97I6IWi/86EVGL8VH6BVyrqEZHXzf8aUB7R5dDRPS7GKCIqEW4rjfin3t+7X1ykfGfJyJqufgvFBG1CBv35aKk0oDOfu54vF+wo8shIrolBigicjhdlQHrfzgHAJgREw45e5+IqIXjv1JE5HDv/5gLXZUR4QEeeKQPe5+IqOVjgCIihyqtrMYHe3MB1Mz7JJPyYcFE1PIxQBGRQ61OO4NyvRERGk881Evj6HKIiBqFAYqIHOZsYTk+Sr8AAFg0sgek7H0iIifBAEVEDiEIAv627RRMZgEx3QMxONzP0SURETUaAxQROcT3OYX44fRVuMgkWDSyu6PLISKyCgMUETW7aqMZr207BQB47r4wdPJzd3BFRETWYYAiomb3Ufp5nCuqgJ+HAtP+0NXR5RARWY0Bioia1bXreqxOOwMAeDm2GzxVLg6uiIjIenJHF0BErV95lQFHLpYiK68UqT8XoLzKiF7tvfB/kaGOLo2IqEmcpgfq9ddfx6BBg+Dm5gZvb+8G2+Tl5WHkyJFwc3NDQEAAXn75ZRiNRos2u3fvxoABA6BUKtG1a1ds3Lix3n7efvttdOrUCSqVClFRUfjpp5/scERErVeF3ojvsgvwyjcnMWLVD+jzyi6Me/8nrEw9jeOXy6CQSbH00Z6cNJOInJbT9EBVV1fjySefRHR0NN5///16600mE0aOHAmNRoP9+/cjPz8f48ePh4uLC/7+978DAHJzczFy5EhMnjwZmzZtQlpaGp5//nkEBQUhNjYWAPDZZ59h1qxZWLduHaKiorBq1SrExsYiJycHAQEBzXrMRM5CbzTh6MUyHDh3DXvPFiErrwQGk2DRJsTHFQM6+GBAB28Mvcsfnf09HFQtEdGdkwiCINy+WcuxceNGJCQkoLS01GL5zp078cgjj+DKlSsIDAwEAKxbtw5z587F1atXoVAoMHfuXGzfvh0nTpwQtxs1ahRKS0uRkpICAIiKisLdd9+NtWvXAgDMZjNCQ0Px4osvYt68eY2qUafTQa1Wo6ysDF5eXjY4aqKWpei6Hicul+HoxTJk5F5D5oUS6I1mizYhPq4YEu6H+7r64Z6wdgjwVDmoWiKixrHm+9tpeqBuJz09Hb179xbDEwDExsZiypQpOHnyJPr374/09HTExMRYbBcbG4uEhAQANb1cmZmZmD9/vrheKpUiJiYG6enpv/vZer0eer1efK/T6Wx0VESOV2Uw4cjFUhzMLcbRS6U4cVkHra6qXjs/DwWiOvsiurMvhoT7oaMvpyYgotar1QQorVZrEZ4AiO+1Wu0t2+h0Oty4cQMlJSUwmUwNtsnOzv7dz05KSsIrr7xii8MgcrjKaiMOnS9BRu41/JRbjKMXy1BtsuxdkkiAMD939ApW4+6wdoju3A5d/D0gkXBMExG1DQ4NUPPmzcMbb7xxyzanTp1CREREM1XUNPPnz8esWbPE9zqdDqGhvLuInEOVwYTMCyVI/+Ua0s9dw9GLpTCaLa/sB3gqcU9YOwzo4IPeIWp0D/KCh7LV/P5FRGQ1h/4LOHv2bEyYMOGWbTp37tyofWk0mnp3yxUUFIjr6v6sW3ZzGy8vL7i6ukImk0EmkzXYpm4fDVEqlVAqlY2qk8jRzGYBJ6/osPdsEfadLcLB88X1xi+193ZFVOd2uDfMF/eEtUNHXzf2LhER3cShAcrf3x/+/v422Vd0dDRef/11FBYWinfLpaamwsvLCz169BDb7Nixw2K71NRUREdHAwAUCgUiIyORlpaGuLg4ADWDyNPS0jBt2jSb1EnkCAaTGRnnipFyMh+7ThagsFxvsT7QS4lBXfwQ3dkX0V18EeLjysBERHQLTtMHn5eXh+LiYuTl5cFkMuHIkSMAgK5du8LDwwPDhw9Hjx49MG7cOCQnJ0Or1WLRokWYOnWq2Ds0efJkrF27FnPmzMFzzz2H7777Dp9//jm2b98ufs6sWbMQHx+PgQMH4p577sGqVatQUVGBZ5991hGHTdRkBpMZe88WYdvRfPzvVAHKbhjEdR5KOe7t7IvBXX0xONyP45eIiKzkNNMYTJgwAR9++GG95d9//z0eeOABAMCFCxcwZcoU7N69G+7u7oiPj8eyZcsgl/+aE3fv3o2ZM2fi559/RkhICBYvXlzvMuLatWuxfPlyaLVa9OvXD2vWrEFUVFSja+U0BuQoJrOAjHPX8M2xK9h5QovSyl9Dk6+7AsN7BmJ4Tw0GdfGFUi5zYKVERC2PNd/fThOgnAkDFDWna9f12HP6Kr7PuYofz1y1CE1+Hgo83DsID/cOwt2d2nHmbyKiW2iT80ARtSWXS29g+7Er2HFci6OXSnHzr0Hebi54qJcGj/QJRlRYO8hlTvPEJiIip8EAReQkCsursONYPrYdy8ehCyUW63oEeWFYhD+GdQtAv1BvhiYiIjtjgCJqwa6W65FyUovtx64gI7dY7GmSSIC7O7XDo32CMLynBoFefEwKEVFzYoAiamGqDCbsOJ6P/2RewoFz13DznJb9Qr3xaN9gjOwdBI2aoYmIyFEYoIhaiJ+v6LD5YB6+zLqM8iqjuLxvqDce6R2Eh3prEOLj5sAKiYioDgMUkQNVG83YcTwfG/efx5GLpeLyEB9XPD0wFHH92yO0HUMTEVFLwwBF5ABXy/XYlHEBmzLycLV2VnAXmQTDe2gw6p5Q3NfFD1JOOUBE1GIxQBE1o1P5Orz3Yy6+PnoZBlPN4KYATyXG3dsRo6M6wM+Dz1QkInIGDFBEdiYIAn44U4T3fjyHH88UicsHdPDGhPvC8FAvDVw47QARkVNhgCKykyqDCV9mXcbGfeeRU1AOAJBKgId6B+H5wWHo38HHwRUSEVFTMUAR2Vh+2Q18lH4Bn/6UJz5WxV0hw9N3d8Cz93XioHAiolaAAYrIBsxmAft+KcInGXnY9XMBTLWTN4X4uGLCoE54cmAo1K4uDq6SiIhshQGKqBEEQUDqzwU4cK4Y/p5KBHuroPFSoZ27Av87VYjNB/Nw4Vql2P7ezu3w7H1hiOkeyAf4EhG1QgxQRLdRWF6FxVtP4NuTBbds56mU408D2mNMVAdEaG79FG8iInJuDFBEv0MQBHyZdRmvfPMzym4YIJdK8MSAEFSbzLhSegP5ZVUoLK9CN40Xxt7TAY/0DYKbgn+liIjaAv5rT22e2SzgcukNXCypxLXr1bh2XY9rFdXIyivF3rM10w70DPbC8v/rix7B7FkiIiIGKGpjBEHA6YLr2He2CNlaHXIKruNMQTkqq00NtlfIpJgRE45JQztzriYiIhIxQFGrV2UwIf2Xa0jLLsD32VdxufRGvTYuMglC27nBz0MJPw8FfN2V8PNQYmSfIHQN8HBA1URE1JIxQFGrJQgCth/Px9Kvf0bRdb24XCmX4t7Ovugb6o1ugZ7opvFAR1939jAREVGjMUBRq3Sl9AYSvzqB/50qBABovFR4sHsA/hARgEFd/OCqkDm4QiIicmYMUNSqmM0CPs64gDd2ZqOi2gQXmQR/faAr/jqsC5RyhiYiIrINBihqNbRlVZj52RGkn7sGoOZhvcue6IO7Aj0dXBkREbU2DFDUKuw6qcWc/x5DaaUBbgoZ5o6IwLh7O0LKWcCJiMgOGKDIqVUZTHht+8/4+EAeAKBXey+sGdUfnf155xwREdkPAxQ5raLrejzzXgayteUAgElDO+Ol4d2gkPNuOiIisi8GKHJKFXojntt4ENnacvh5KLHyqb4Yepe/o8siIqI2ggGKnE610Ywpmw7j2KUy+Li54LMX7kUXXrIjIqJmxGsd5FTMZgFz/3sMP5y+ClcXGT6YcDfDExERNTsGKHIqy1Ky8WXWZcikErzzzAD07+Dj6JKIiKgNYoAip7FhXy7W/3AOAPDGE30wrFuAgysiIqK2igGKnML+s0V4bfspAMDcERH4v8gQB1dERERtGQMUtXiXSiox7dMsmMwC/jygPSbf39nRJRERURvHAEUtWpXBhMkfZ6K4ohq92nvh73/qDYmEs4sTEZFjMUBRiyUIAhZ8eRwnLuvQzl2Bdc9EQuXCBwITEZHjMUBRi/Xh/vP44nDNHXdrx/RHiI+bo0siIiICwABFLVRWXok4aHz+QxEY1MXPwRURERH9igGKWpwb1SbM+vwojGYBj/QJwsTBYY4uiYiIyAIDFLU4y3aeQm5RBTReKrwex0HjRETU8jBAUYuy90wRPky/AABY/mQfqN1cHFwRERFRfU4RoM6fP4+JEyciLCwMrq6u6NKlC5YsWYLq6mqLdseOHcOQIUOgUqkQGhqK5OTkevvasmULIiIioFKp0Lt3b+zYscNivSAISExMRFBQEFxdXRETE4MzZ87Y9fioRtkNA17+z1EAwLh7O2JIuL+DKyIiImqYUwSo7OxsmM1m/POf/8TJkyfxj3/8A+vWrcOCBQvENjqdDsOHD0fHjh2RmZmJ5cuXY+nSpVi/fr3YZv/+/Rg9ejQmTpyIrKwsxMXFIS4uDidOnBDbJCcnY82aNVi3bh0yMjLg7u6O2NhYVFVVNesxt0WvfH0S+WVV6OTrhvkPRzi6HCIiot8lEQRBcHQRTbF8+XK8++67OHeu5tlo7777LhYuXAitVguFQgEAmDdvHrZu3Yrs7GwAwNNPP42Kigps27ZN3M+9996Lfv36Yd26dRAEAcHBwZg9ezZeeuklAEBZWRkCAwOxceNGjBo1qlG16XQ6qNVqlJWVwcvLy5aH3WqlnMjH5I8PQyoBtkwehMiOfEgwERE1L2u+v52iB6ohZWVlaNeunfg+PT0dQ4cOFcMTAMTGxiInJwclJSVim5iYGIv9xMbGIj09HQCQm5sLrVZr0UatViMqKkps0xC9Xg+dTmfxosar0BuR+NVJAMCUB7owPBERUYvnlAHq7NmzeOutt/DCCy+Iy7RaLQIDAy3a1b3XarW3bHPz+pu3a6hNQ5KSkqBWq8VXaGhoE4+sbfrXj+dQWK5Hh3ZumP5guKPLISIiui2HBqh58+ZBIpHc8lV3+a3O5cuXMWLECDz55JP4y1/+4qDKLc2fPx9lZWXi6+LFi44uyWkU6qrwzz01l2HnjOgGpZyPaiEiopZP7sgPnz17NiZMmHDLNp07dxZ/vnLlCoYNG4ZBgwZZDA4HAI1Gg4KCAotlde81Gs0t29y8vm5ZUFCQRZt+/fr9bo1KpRJKpfKWx0ENW5l6GjcMJvQL9cbI3kG334CIiKgFcGiA8vf3h79/425Vv3z5MoYNG4bIyEhs2LABUqll51l0dDQWLlwIg8EAF5eauYNSU1PRrVs3+Pj4iG3S0tKQkJAgbpeamoro6GgAQFhYGDQaDdLS0sTApNPpkJGRgSlTptzh0dJv5WjL8fmhmt66RSO7c8JMIiJyGk4xBury5ct44IEH0KFDB7z55pu4evUqtFqtxbikMWPGQKFQYOLEiTh58iQ+++wzrF69GrNmzRLbzJgxAykpKVixYgWys7OxdOlSHDp0CNOmTQMASCQSJCQk4LXXXsPXX3+N48ePY/z48QgODkZcXFxzH3arl7TzFMwC8FAvDQZ2anf7DYiIiFoIh/ZANVZqairOnj2Ls2fPIiQkxGJd3SwMarUau3btwtSpUxEZGQk/Pz8kJiZi0qRJYttBgwbhk08+waJFi7BgwQKEh4dj69at6NWrl9hmzpw5qKiowKRJk1BaWorBgwcjJSUFKpWqeQ62jfjxzFXszrkKuVSCuSM45xMRETkXp50HqiXjPFC3ZjILGLnmR2Rry/HsfZ2w5NGeji6JiIiobcwDRc7r66OXka0th6dKjul/4LQFRETkfBigqFkZTWa8lXYWADD5/i7wcVfcZgsiIqKWhwGKmtU3x67gXFEFvN1cED+ok6PLISIiahIGKGo2JrOAt76r6X36y5DO8FA6xT0MRERE9TBAUbPZduwKzl2tgNrVBeOjOzq6HCIioiZjgKJmYTILWJN2BgDwlyFh8FS5OLgiIiKipmOAomax/Xg+fqntfeLYJyIicnYMUGR3N/c+TRzM3iciInJ+DFBkdzuO5+Ns4XV4qeSYcF8nR5dDRER0xxigyK4EQcDa2jvvJg7uDC/2PhERUSvAAEV2lZFbjJyCcrgpZOx9IiKiVoMBiuzq4wMXAACP92sPtSt7n4iIqHVggCK7KSyvQsoJLQDgmXs7OLgaIiIi22GAIrv5/OBFGM0CBnTwRs9gtaPLISIishkGKLILk1nAJxl5AIBn7uWs40RE1LowQJFdfJddiCtlVfBxc8HDvYMcXQ4REZFNMUCRXdQNHn9qYChULjIHV0NERGRbDFBkcxeuVWDP6asAgDFRHDxOREStDwMU2Vzd2Kf77/JHR193B1dDRERkewxQZFNVBhM+P3QRAAePExFR68UARTa180Q+SioNCFar8IeIAEeXQ0REZBcMUGRTXx25AgB4cmAoZFKJg6shIiKyDwYospnSymrsPVMEAHi0b7CDqyEiIrIfBiiymW9PamE0C4jQeKJrgIejyyEiIrIbBiiymW3H8gEAj/ThxJlERNS6MUCRTVy7rsf+X64BAEb24eU7IiJq3RigyCa+PVkAk1lAz2AvhPlx7iciImrdGKDIJrYdq7n77hH2PhERURvAAEV37Gq5HgfO1V6+44ODiYioDWCAojuWclILswD0DVGjg6+bo8shIiKyOwYoumPbjtZcvhvJu++IiKiNYICiO1Koq8JP54sBAA/z8h0REbURDFB0R3Ycz4cgAP07eCPEh5fviIiobWCAojuy/Xjd5Jm8+46IiNoOBihqsqLrehw8XwIAeLi3xsHVEBERNR8GKGqyfWdrHhzcPcgLQWpXB1dDRETUfOSNadS/f39IJJJG7fDw4cN3VBA5jx/P1ASoIeF+Dq6EiIioeTUqQMXFxYk/V1VV4Z133kGPHj0QHR0NADhw4ABOnjyJv/71r3YpkloeQRCwtzZADe7KAEVERG1LowLUkiVLxJ+ff/55TJ8+Ha+++mq9NhcvXrRtddRi/XL1OrS6KijkUtwT1s7R5RARETUrq8dAbdmyBePHj6+3/JlnnsF///tfmxTVkMceewwdOnSASqVCUFAQxo0bhytXrli0OXbsGIYMGQKVSoXQ0FAkJyc3WH9ERARUKhV69+6NHTt2WKwXBAGJiYkICgqCq6srYmJicObMGbsdl7Oqu3x3T6d2ULnIHFwNERFR87I6QLm6umLfvn31lu/btw8qlcomRTVk2LBh+Pzzz5GTk4P//ve/+OWXX/B///d/4nqdTofhw4ejY8eOyMzMxPLly7F06VKsX79ebLN//36MHj0aEydORFZWFuLi4hAXF4cTJ06IbZKTk7FmzRqsW7cOGRkZcHd3R2xsLKqqqux2bM6oLkAN5vgnIiJqiwQrJSUlCSqVSnjxxReFf//738K///1vYdq0aYKbm5uQlJRk7e6a7KuvvhIkEolQXV0tCIIgvPPOO4KPj4+g1+vFNnPnzhW6desmvn/qqaeEkSNHWuwnKipKeOGFFwRBEASz2SxoNBph+fLl4vrS0lJBqVQKn376aaNrKysrEwAIZWVlTTq2lk5vMAndF+8UOs7dJhy/VOrocoiIiGzCmu9vq3ug5s2bhw8//BCZmZmYPn06pk+fjsOHD2PDhg2YN2+e7RNeA4qLi7Fp0yYMGjQILi4uAID09HQMHToUCoVCbBcbG4ucnByUlJSIbWJiYiz2FRsbi/T0dABAbm4utFqtRRu1Wo2oqCixTUP0ej10Op3FqzXLyitBZbUJvu4K9AjycnQ5REREzc6qAGU0GvG3v/0NgwYNwr59+1BcXIzi4mLs27cPTz31lL1qFM2dOxfu7u7w9fVFXl4evvrqK3GdVqtFYGCgRfu691qt9pZtbl5/83YNtWlIUlIS1Gq1+AoNDW3iETqHvbXzP93X1Q9SaeOmtyAiImpNrApQcrkcycnJMBqNNvnwefPmQSKR3PKVnZ0ttn/55ZeRlZWFXbt2QSaTYfz48RAEwSa13In58+ejrKxMfLX2uxE5/omIiNq6Rk1jcLMHH3wQe/bsQadOne74w2fPno0JEybcsk3nzp3Fn/38/ODn54e77roL3bt3R2hoKA4cOIDo6GhoNBoUFBRYbFv3XqPRiH821Obm9XXLgoKCLNr069fvd2tUKpVQKpW3PthWoqzSgGOXSgFwAk0iImq7rA5QDz30EObNm4fjx48jMjIS7u7uFusfe+yxRu/L398f/v7+1pYAADCbzQBqxh8BQHR0NBYuXAiDwSCOi0pNTUW3bt3g4+MjtklLS0NCQoK4n9TUVHFC0LCwMGg0GqSlpYmBSafTISMjA1OmTGlSna3N/l+KYBaArgEefHwLERG1WVYHqLrZxleuXFlvnUQigclkuvOqfiMjIwMHDx7E4MGD4ePjg19++QWLFy9Gly5dxPAzZswYvPLKK5g4cSLmzp2LEydOYPXq1fjHP/4h7mfGjBm4//77sWLFCowcORKbN2/GoUOHxKkOJBIJEhIS8NprryE8PBxhYWFYvHgxgoODLWZjb8t+PMvZx4mIiKwOUHU9P83Jzc0NX3zxBZYsWYKKigoEBQVhxIgRWLRokXjpTK1WY9euXZg6dSoiIyPh5+eHxMRETJo0SdzPoEGD8Mknn2DRokVYsGABwsPDsXXrVvTq1UtsM2fOHFRUVGDSpEkoLS3F4MGDkZKSYtc5rpzJXj7/joiICBKhJYzCbmV0Oh3UajXKysrg5dV6bvO/cK0C9y/fDblUgiNLhsNDaXX+JiIiarGs+f5u0jdgRUUF9uzZg7y8PFRXV1usmz59elN2SU6g7u67AR19GJ6IiKhNs/pbMCsrCw8//DAqKytRUVGBdu3aoaioCG5ubggICGCAasUycosBAPd14eU7IiJq26yeiXzmzJl49NFHUVJSAldXVxw4cAAXLlxAZGQk3nzzTXvUSC1EVl7NjO4DO/k4uBIiIiLHsjpAHTlyBLNnz4ZUKoVMJoNer0doaCiSk5OxYMECe9RILUBheRUuldyARAL0CVE7uhwiIiKHsjpAubi4QCqt2SwgIAB5eXkAau6Ca+0zcLdlR/JKAQB3BXjCU+Xi2GKIiIgczOoxUP3798fBgwcRHh6O+++/H4mJiSgqKsK///1vi+kAqHXJulgKAOjfwduhdRAREbUEVvdA/f3vfxcfc/L666/Dx8cHU6ZMwdWrV8UJKan1qRv/xABFRETUhB6ogQMHij8HBAQgJSXFpgVRy2M0mXH0YhkAYEAHDiAnIiKyugfqgw8+QG5urj1qoRYqp6AcNwwmeCrl6OLv4ehyiIiIHM7qAJWUlISuXbuiQ4cOGDduHN577z2cPXvWHrVRC5FVO4C8XwdvSKUSxxZDRETUAlgdoM6cOYO8vDwkJSXBzc0Nb775Jrp164aQkBA888wz9qiRHKwuQPUP9XZoHURERC3FHT0Lr7KyEj/++CM+/fRTbNq0CYIgwGg02rI+p9TanoX3hxW7ce5qBTZMuBvDIgIcXQ4REZFd2PVZeLt27cLu3buxe/duZGVloXv37rj//vvxn//8B0OHDm1y0dQylVZW49zVCgBAP/ZAERERAWhCgBoxYgT8/f0xe/Zs7NixA97e3nYoi1qKI7XzP4X5ucPHXeHYYoiIiFoIq8dArVy5Evfddx+Sk5PRs2dPjBkzBuvXr8fp06ftUR85GMc/ERER1Wd1gEpISMAXX3yBoqIipKSkYNCgQUhJSUGvXr0QEhJijxrJgTgDORERUX1WX8IDAEEQkJWVhd27d+P777/H3r17YTab4e/vb+v6yIHMZgFHxBnIOYEmERFRHasD1KOPPop9+/ZBp9Ohb9++eOCBB/CXv/wFQ4cO5XioVuZc0XXoqoxQuUgRofF0dDlEREQthtUBKiIiAi+88AKGDBkCtVptj5qohThcO/6pT4g35DKrr/YSERG1WlYHqOXLl4s/V1VVQaVS2bQgajnEAeQc/0RERGTB6m4Fs9mMV199Fe3bt4eHhwfOnTsHAFi8eDHef/99mxdIjpNVN/4plOOfiIiIbmZ1gHrttdewceNGJCcnQ6H4dV6gXr164b333rNpceQ4FXojTheUA2APFBER0W9ZHaA++ugjrF+/HmPHjoVMJhOX9+3bF9nZ2TYtjhwnW1sOswAEeikR6MXLtERERDezOkBdvnwZXbt2rbfcbDbDYDDYpChyvLOFNb1PdwXy7jsiIqLfsjpA9ejRAz/++GO95f/5z3/Qv39/mxRFjnem4DoAoGuAh4MrISIianmsvgsvMTER8fHxuHz5MsxmM7744gvk5OTgo48+wrZt2+xRIznAmcKaAMUeKCIiovqs7oF6/PHH8c033+B///sf3N3dkZiYiFOnTuGbb77BH//4R3vUSA5wpnYAeTh7oIiIiOpp0qNchgwZgtTU1HrLDx06hIEDB95xUeRY5VUGXCmrAsBLeERERA2xugfq+vXruHHjhsWyI0eO4NFHH0VUVJTNCiPH+eVqBQDA31MJbzfFbVoTERG1PY0OUBcvXkR0dDTUajXUajVmzZqFyspKjB8/HlFRUXB3d8f+/fvtWSs1E16+IyIiurVGX8J7+eWXUVVVhdWrV+OLL77A6tWr8eOPPyIqKgq//PILQkJC7FknNaOzHEBORER0S40OUD/88AO++OIL3HvvvXjqqaeg0WgwduxYJCQk2LE8coS6O/A4/omIiKhhjb6EV1BQgLCwMABAQEAA3Nzc8NBDD9mtMHKc07yER0REdEtWDSKXSqUWP9/8LDxqHSqrjbhUUnOTQDgv4RERETWo0ZfwBEHAXXfdBYlEAqDmbrz+/ftbhCoAKC4utm2F1Kx+Kay5A8/XXYF27gzIREREDWl0gNqwYYM966AW4kztM/A4/omIiOj3NTpAxcfH27MOaiH4CBciIqLbs3oiTWrd6h4iHB7IHigiIqLfwwBFFngJj4iI6PacLkDp9Xr069cPEokER44csVh37NgxDBkyBCqVCqGhoUhOTq63/ZYtWxAREQGVSoXevXtjx44dFusFQUBiYiKCgoLg6uqKmJgYnDlzxp6H1GJUGUzIK64EAIQH8BIeERHR73G6ADVnzhwEBwfXW67T6TB8+HB07NgRmZmZWL58OZYuXYr169eLbfbv34/Ro0dj4sSJyMrKQlxcHOLi4nDixAmxTXJyMtasWYN169YhIyMD7u7uiI2NRVVVVbMcnyP9cvU6BAHwdnOBnwfvwCMiIvo9ThWgdu7ciV27duHNN9+st27Tpk2orq7GBx98gJ49e2LUqFGYPn06Vq5cKbZZvXo1RowYgZdffhndu3fHq6++igEDBmDt2rUAanqfVq1ahUWLFuHxxx9Hnz598NFHH+HKlSvYunVrcx2mw9Q9wiU8wEOcroKIiIjqa/RdeHVmzZrV4HKJRAKVSoWuXbvi8ccfR7t27e64uJsVFBTgL3/5C7Zu3Qo3N7d669PT0zF06FCLyT1jY2PxxhtvoKSkBD4+PkhPT69Xf2xsrBiOcnNzodVqERMTI65Xq9WIiopCeno6Ro0a1WBter0eer1efK/T6e7kUB3m1wHkvHxHRER0K1YHqKysLBw+fBgmkwndunUDAJw+fRoymQwRERF45513MHv2bOzduxc9evSwSZGCIGDChAmYPHkyBg4ciPPnz9dro9VqxUfN1AkMDBTX+fj4QKvVistubqPVasV2N2/XUJuGJCUl4ZVXXrH6uFoaPsKFiIiocay+hPf4448jJiYGV65cQWZmJjIzM3Hp0iX88Y9/xOjRo3H58mUMHToUM2fOvO2+5s2bB4lEcstXdnY23nrrLZSXl2P+/PlNOkh7mz9/PsrKysTXxYsXHV1Sk/x6CY89UERERLdidQ/U8uXLkZqaCi8vL3GZWq3G0qVLMXz4cMyYMQOJiYkYPnz4bfc1e/ZsTJgw4ZZtOnfujO+++w7p6elQKpUW6wYOHIixY8fiww8/hEajQUFBgcX6uvcajUb8s6E2N6+vWxYUFGTRpl+/fr9bo1KprFebs9EbTTh/reYxLpwDioiI6NasDlBlZWUoLCysd3nu6tWr4tgfb29vVFdX33Zf/v7+8Pf3v227NWvW4LXXXhPfX7lyBbGxsfjss88QFRUFAIiOjsbChQthMBjg4uICAEhNTUW3bt3g4+MjtklLS0NCQoK4r9TUVERHRwMAwsLCoNFokJaWJgYmnU6HjIwMTJky5bZ1OrPcogqYBcBTJUeAp3OHQSIiInuzOkA9/vjjeO6557BixQrcfffdAICDBw/ipZdeQlxcHADgp59+wl133WWzIjt06GDx3sOjpoekS5cuCAkJAQCMGTMGr7zyCiZOnIi5c+fixIkTWL16Nf7xj3+I282YMQP3338/VqxYgZEjR2Lz5s04dOiQONWBRCJBQkICXnvtNYSHhyMsLAyLFy9GcHCweGytlTiAnHfgERER3ZbVAeqf//wnZs6ciVGjRsFoNNbsRC5HfHy8GFYiIiLw3nvv2bbS21Cr1di1axemTp2KyMhI+Pn5ITExEZMmTRLbDBo0CJ988gkWLVqEBQsWIDw8HFu3bkWvXr3ENnPmzEFFRQUmTZqE0tJSDB48GCkpKVCpVM16PM2Nz8AjIiJqPIkgCEJTNrx+/TrOnTsHoGacUl2vENVc9lOr1SgrK7MYK9aS/XVTJnYc12LRyO54fkhnR5dDRETU7Kz5/rb6LryPP/4YlZWV8PDwQJ8+fdCnTx+Gp1ag7hEunXzdHVwJERFRy2d1gJo5cyYCAgIwZswY7NixAyaTyR51UTO7VHIDABDSztXBlRAREbV8Vgeo/Px8bN68GRKJBE899RSCgoIwdepU7N+/3x71UTMorzKgtNIAAGjvzQBFRER0O1YHKLlcjkceeQSbNm1CYWEh/vGPf+D8+fMYNmwYunTpYo8ayc4ul9b0Pnm7ucBT5eLgaoiIiFo+q+/Cu5mbmxtiY2NRUlKCCxcu4NSpU7aqi5rRpeLay3c+7H0iIiJqDKt7oACgsrISmzZtwsMPP4z27dtj1apV+NOf/oSTJ0/auj5qBpdKagaQh3jXf0gzERER1Wd1D9SoUaOwbds2uLm54amnnsLixYvFmbzJOYkDyNkDRURE1ChWByiZTIbPP/8csbGxkMlkFutOnDhhMSklOQcGKCIiIutYHaA2bdpk8b68vByffvop3nvvPWRmZnJaAyd0qbT2Ep4PL+ERERE1RpPGQAHADz/8gPj4eAQFBeHNN9/EH/7wBxw4cMCWtVEz4RxQRERE1rGqB0qr1WLjxo14//33odPp8NRTT0Gv12Pr1q3o0aOHvWokO+IcUERERNZrdA/Uo48+im7duuHYsWNYtWoVrly5grfeesuetVEz4BxQRERE1mt0D9TOnTsxffp0TJkyBeHh4fasiZoR54AiIiKyXqN7oPbu3Yvy8nJERkYiKioKa9euRVFRkT1ro2bAOaCIiIis1+gAde+99+Jf//oX8vPz8cILL2Dz5s0IDg6G2WxGamoqysvL7Vkn2QmnMCAiIrKe1Xfhubu747nnnsPevXtx/PhxzJ49G8uWLUNAQAAee+wxe9RIdsQARUREZL0mT2MAAN26dUNycjIuXbqETz/91FY1UTPiHFBERETWu6MAVUcmkyEuLg5ff/21LXZHzYhzQBEREVnPJgGKnBPngCIiImoaBqg2jHNAERERNQ0DVBvGOaCIiIiahgGqDeMcUERERE3DANWGcQoDIiKipmGAasMYoIiIiJqGAaoN4xxQRERETcMA1YZxDigiIqKmYYBqozgHFBERUdMxQLVRnAOKiIio6Rig2ijOAUVERNR0DFBtFOeAIiIiajoGqDaKUxgQERE1HQNUG8UARURE1HQMUG0U54AiIiJqOgaoNopzQBERETUdA1QbxDmgiIiI7gwDVBvEOaCIiIjuDANUG6QtqwIABKnZ+0RERNQUDFBtUHFFNQDA113h4EqIiIick9MEqE6dOkEikVi8li1bZtHm2LFjGDJkCFQqFUJDQ5GcnFxvP1u2bEFERARUKhV69+6NHTt2WKwXBAGJiYkICgqCq6srYmJicObMGbseW3OrC1A+DFBERERN4jQBCgD+9re/IT8/X3y9+OKL4jqdTofhw4ejY8eOyMzMxPLly7F06VKsX79ebLN//36MHj0aEydORFZWFuLi4hAXF4cTJ06IbZKTk7FmzRqsW7cOGRkZcHd3R2xsLKqqqpr1WO2ppJI9UERERHfCqQKUp6cnNBqN+HJ3dxfXbdq0CdXV1fjggw/Qs2dPjBo1CtOnT8fKlSvFNqtXr8aIESPw8ssvo3v37nj11VcxYMAArF27FkBN79OqVauwaNEiPP744+jTpw8++ugjXLlyBVu3bm3uw7UbsQfKjQGKiIioKZwqQC1btgy+vr7o378/li9fDqPRKK5LT0/H0KFDoVD8GgpiY2ORk5ODkpISsU1MTIzFPmNjY5Geng4AyM3NhVartWijVqsRFRUltmkN6gJUO3fegUdERNQUckcX0FjTp0/HgAED0K5dO+zfvx/z589Hfn6+2MOk1WoRFhZmsU1gYKC4zsfHB1qtVlx2cxutViu2u3m7hto0RK/XQ6/Xi+91Ol0Tj7J5lFTUzAHVzl3p4EqIiIick0N7oObNm1dvYPhvX9nZ2QCAWbNm4YEHHkCfPn0wefJkrFixAm+99ZZFcHGUpKQkqNVq8RUaGurokm7pWkXNOfNhDxQREVGTOLQHavbs2ZgwYcIt23Tu3LnB5VFRUTAajTh//jy6desGjUaDgoICizZ17zUajfhnQ21uXl+3LCgoyKJNv379frfG+fPnY9asWeJ7nU7XokNUSWVdDxTHQBERETWFQwOUv78//P39m7TtkSNHIJVKERAQAACIjo7GwoULYTAY4OJS07OSmpqKbt26wcfHR2yTlpaGhIQEcT+pqamIjo4GAISFhUGj0SAtLU0MTDqdDhkZGZgyZcrv1qJUKqFUOsflMJNZQGntXXjtOIiciIioSZxiEHl6ejpWrVqFo0eP4ty5c9i0aRNmzpyJZ555RgxHY8aMgUKhwMSJE3Hy5El89tlnWL16tUXP0IwZM5CSkoIVK1YgOzsbS5cuxaFDhzBt2jQAgEQiQUJCAl577TV8/fXXOH78OMaPH4/g4GDExcU54tBtTnfDALNQ8zPngSIiImoapxhErlQqsXnzZixduhR6vR5hYWGYOXOmRThSq9XYtWsXpk6disjISPj5+SExMRGTJk0S2wwaNAiffPIJFi1ahAULFiA8PBxbt25Fr169xDZz5sxBRUUFJk2ahNLSUgwePBgpKSlQqVTNesz2cq32DjxPlRwuMqfIz0RERC2ORBAEwdFFtDY6nQ5qtRplZWXw8vJydDkWDp4vxpPr0tHR1w17Xh7m6HKIiIhaDGu+v9kF0cb8OgcUL98RERE1FQNUGyMGKA4gJyIiajIGqDaGDxImIiK6cwxQbUxJBR8kTEREdKcYoNoY9kARERHdOQaoNqaYk2gSERHdMQaoNqaEd+ERERHdMQaoNuYaL+ERERHdMQaoNoY9UERERHeOAaoNqTKYUFFtAsAARUREdCcYoNqQktoB5DKpBF4qp3gMIhERUYvEANWGiFMYuCkgkUgcXA0REZHzYoBqQ0oqDAA4iSYREdGdYoBqQ65V6AEAPu4uDq6EiIjIuTFAtSG8A4+IiMg2GKDakOLKmkt4PpyFnIiI6I4wQLUhfJAwERGRbTBAtSF8kDAREZFtMEC1IcUcA0VERGQTDFBtSN1EmgxQREREd4YBqg25dtNEmkRERNR0DFBthCAInMaAiIjIRhig2ohyvRFGswCAAYqIiOhOMUC1EcXXa3qf3BQyqFxkDq6GiIjIuTFAtRHFlRz/REREZCsMUG2EOImmBwMUERHRnWKAaiN4Bx4REZHtMEC1EbwDj4iIyHYYoNqIYk6iSUREZDMMUG1E3V14DFBERER3jgGqjSjhXXhEREQ2wwDVRvBBwkRERLbDANVGMEARERHZDgNUG/FrgHJxcCVERETOjwGqDTCYzNBVGQFwDBQREZEtMEC1AaWVBgCARAJ4M0ARERHdMQaoNqDu8p23qwtkUomDqyEiInJ+DFBtQF2A8uEAciIiIptggGoD6uaA8mWAIiIisgmnClDbt29HVFQUXF1d4ePjg7i4OIv1eXl5GDlyJNzc3BAQEICXX34ZRqPRos3u3bsxYMAAKJVKdO3aFRs3bqz3OW+//TY6deoElUqFqKgo/PTTT3Y8Kvvjg4SJiIhsy2kC1H//+1+MGzcOzz77LI4ePYp9+/ZhzJgx4nqTyYSRI0eiuroa+/fvx4cffoiNGzciMTFRbJObm4uRI0di2LBhOHLkCBISEvD888/j22+/Fdt89tlnmDVrFpYsWYLDhw+jb9++iI2NRWFhYbMery3xQcJERES2JREEQXB0EbdjNBrRqVMnvPLKK5g4cWKDbXbu3IlHHnkEV65cQWBgIABg3bp1mDt3Lq5evQqFQoG5c+di+/btOHHihLjdqFGjUFpaipSUFABAVFQU7r77bqxduxYAYDabERoaihdffBHz5s1rVL06nQ5qtRplZWXw8vK6k0O3iaVfn8TG/efx1we6YM6ICEeXQ0RE1CJZ8/3tFD1Qhw8fxuXLlyGVStG/f38EBQXhoYcesghC6enp6N27txieACA2NhY6nQ4nT54U28TExFjsOzY2Funp6QCA6upqZGZmWrSRSqWIiYkR2zgjzkJORERkW04RoM6dOwcAWLp0KRYtWoRt27bBx8cHDzzwAIqLiwEAWq3WIjwBEN9rtdpbttHpdLhx4waKiopgMpkabFO3j4bo9XrodDqLV0vCBwkTERHZlkMD1Lx58yCRSG75ys7OhtlsBgAsXLgQTzzxBCIjI7FhwwZIJBJs2bLFkYcAAEhKSoJarRZfoaGhji7JQqFODwDw9WCAIiIisgW5Iz989uzZmDBhwi3bdO7cGfn5+QCAHj16iMuVSiU6d+6MvLw8AIBGo6l3t1xBQYG4ru7PumU3t/Hy8oKrqytkMhlkMlmDber20ZD58+dj1qxZ4nudTtdiQpTBZMa5ousAgC7+Hg6uhoiIqHVwaIDy9/eHv7//bdtFRkZCqVQiJycHgwcPBgAYDAacP38eHTt2BABER0fj9ddfR2FhIQICAgAAqamp8PLyEoNXdHQ0duzYYbHv1NRUREdHAwAUCgUiIyORlpYmTpFgNpuRlpaGadOm/W59SqUSSqXSuoNvJrlFFTCYBHgo5QjxcXV0OURERK2CU4yB8vLywuTJk7FkyRLs2rULOTk5mDJlCgDgySefBAAMHz4cPXr0wLhx43D06FF8++23WLRoEaZOnSqGm8mTJ+PcuXOYM2cOsrOz8c477+Dzzz/HzJkzxc+aNWsW/vWvf+HDDz/EqVOnMGXKFFRUVODZZ59t/gO3gWxtOQDgrkAPSCR8jAsREZEtOLQHyhrLly+HXC7HuHHjcOPGDURFReG7776Dj48PAEAmk2Hbtm2YMmUKoqOj4e7ujvj4ePztb38T9xEWFobt27dj5syZWL16NUJCQvDee+8hNjZWbPP000/j6tWrSExMhFarRb9+/ZCSklJvYLmzyNHWDGjvpnH8dApERESthVPMA+VsWtI8UM9/eBD/O1WIVx7rifhBnRxaCxERUUvW6uaBoqaru4TXTePp4EqIiIhaDwaoVuy63ohLJTcAABEMUERERDbDANWK5dT2PgV6KeHNSTSJiIhshgGqFcsRL99xADkREZEtMUC1YnV34PHyHRERkW0xQLVi4gDyQAYoIiIiW2KAaqUEQUBOAe/AIyIisgcGqFaqsFyP0koDZFIJugbwGXhERES2xADVStVdvuvk6waVi8zB1RAREbUuDFCt1K8DyHkHHhERka0xQLVSnIGciIjIfhigWqkcBigiIiK7YYBqhYwmM84UXgfAOaCIiIjsgQGqFTp/rQLVRjPcFDKE+rg5uhwiIqJWhwGqFaob/xQe6AmpVOLgaoiIiFofBignYjCZcbbwOi4WV96yXd34pwjOQE5ERGQXDFBO5I2d2YhZuQcf7Mu9ZTvegUdERGRfDFBOJDywZkbxMwXXb9lO7IFigCIiIrILBignEl57Se507TPuGlKhNyKv9hIfe6CIiIjsgwHKiYTXPtOusFyPskpDg23qwpWfhxK+Hspmq42IiKgtYYByIp4qF7T3dgUAnC5suBcqm5fviIiI7I4BysnUjYOqG+f0WyevlAEAegbzGXhERET2wgDlZO6qHQd15nfGQZ24XPMQ4Z7t1c1WExERUVvDAOVk6sZBnW7gTjyjyYxT+bUBij1QREREdsMA5WTEHqgGxkCdK6qA3miGu0KGMF/35i6NiIiozWCAcjJ1Y6CKrlejuKLaYt2JyzXjn3oEe/ERLkRERHbEAOVk3BRyhLarvRPvN+OgxPFPwRz/REREZE8MUE7oroCGB5LzDjwiIqLmwQDlhOpmJM+5KUCZzQJ+vlLTA9WLd+ARERHZFQOUE7orsP6deHnFlSjXG6GQS9G19k49IiIisg8GKCd081xQgiAAAE7W9j5FaDzhIuN/ViIiInviN60T6hrgAYkEKKk0oOh6zZ14J8TxT7x8R0REZG8MUE5I5SJDx3ZuAH4dSF43hQEHkBMREdkfA5STqhtIfrr2Mh4HkBMRETUfBignJQ4kL7wOra4K1yqqIZNKEKHxdHBlRERErR8DlJOqG0h+WluOk7UTaHb194DKRebIsoiIiNoEBignFR7w6yW843Xjn9pz/BMREVFzYIByUp393SGVALoqI3bnFAIAevEOPCIiombBAOWkVC4ydPJzBwAcvcQ78IiIiJqTUwSo3bt3QyKRNPg6ePCg2O7YsWMYMmQIVCoVQkNDkZycXG9fW7ZsQUREBFQqFXr37o0dO3ZYrBcEAYmJiQgKCoKrqytiYmJw5swZux9jU9Q9E69ODwYoIiKiZuEUAWrQoEHIz8+3eD3//PMICwvDwIEDAQA6nQ7Dhw9Hx44dkZmZieXLl2Pp0qVYv369uJ/9+/dj9OjRmDhxIrKyshAXF4e4uDicOHFCbJOcnIw1a9Zg3bp1yMjIgLu7O2JjY1FVVdXsx307dXfiAUCYnzs8VS4OrIaIiKjtkAh1zwJxIgaDAe3bt8eLL76IxYsXAwDeffddLFy4EFqtFgqFAgAwb948bN26FdnZ2QCAp59+GhUVFdi2bZu4r3vvvRf9+vXDunXrIAgCgoODMXv2bLz00ksAgLKyMgQGBmLjxo0YNWpUo+rT6XRQq9UoKyuDl5f9eoW+OXoFL36aBQAY2ScIb48ZYLfPIiIiau2s+f52ih6o3/r6669x7do1PPvss+Ky9PR0DB06VAxPABAbG4ucnByUlJSIbWJiYiz2FRsbi/T0dABAbm4utFqtRRu1Wo2oqCixTUtSN5UBwAHkREREzUnu6AKa4v3330dsbCxCQkLEZVqtFmFhYRbtAgMDxXU+Pj7QarXispvbaLVasd3N2zXUpiF6vR56vV58r9PpmnBU1gvzc4dcKoHRLHAAORERUTNyaA/UvHnzfndweN2r7vJbnUuXLuHbb7/FxIkTHVR1fUlJSVCr1eIrNDS0WT5XIZdifHQn3Nu5He4Ja9csn0lEREQO7oGaPXs2JkyYcMs2nTt3tni/YcMG+Pr64rHHHrNYrtFoUFBQYLGs7r1Go7llm5vX1y0LCgqyaNOvX7/frXH+/PmYNWuW+F6n0zVbiEp8tEezfA4RERH9yqEByt/fH/7+/o1uLwgCNmzYgPHjx8PFxfKOs+joaCxcuBAGg0Fcl5qaim7dusHHx0dsk5aWhoSEBHG71NRUREdHAwDCwsKg0WiQlpYmBiadToeMjAxMmTLld+tSKpVQKpWNPg4iIiJybk41iPy7775Dbm4unn/++XrrxowZA4VCgYkTJ+LkyZP47LPPsHr1aoueoRkzZiAlJQUrVqxAdnY2li5dikOHDmHatGkAAIlEgoSEBLz22mv4+uuvcfz4cYwfPx7BwcGIi4trrsMkIiKiFs6pBpG///77GDRoECIiIuqtU6vV2LVrF6ZOnYrIyEj4+fkhMTERkyZNEtsMGjQIn3zyCRYtWoQFCxYgPDwcW7duRa9evcQ2c+bMQUVFBSZNmoTS0lIMHjwYKSkpUKlUzXKMRERE1PI55TxQLV1zzQNFREREttPq54EiIiIiciQGKCIiIiIrMUARERERWYkBioiIiMhKDFBEREREVmKAIiIiIrISAxQRERGRlRigiIiIiKzEAEVERERkJQYoIiIiIis51bPwnEXd03F0Op2DKyEiIqLGqvvebsxT7hig7KC8vBwAEBoa6uBKiIiIyFrl5eVQq9W3bMOHCduB2WzGlStX4OnpCYlEYtN963Q6hIaG4uLFi3xQsZ3xXDcfnuvmw3PdfHium4+tzrUgCCgvL0dwcDCk0luPcmIPlB1IpVKEhITY9TO8vLz4F7KZ8Fw3H57r5sNz3Xx4rpuPLc717Xqe6nAQOREREZGVGKCIiIiIrMQA5WSUSiWWLFkCpVLp6FJaPZ7r5sNz3Xx4rpsPz3XzccS55iByIiIiIiuxB4qIiIjISgxQRERERFZigCIiIiKyEgOUE3n77bfRqVMnqFQqREVF4aeffnJ0SU4vKSkJd999Nzw9PREQEIC4uDjk5ORYtKmqqsLUqVPh6+sLDw8PPPHEEygoKHBQxa3HsmXLIJFIkJCQIC7jubady5cv45lnnoGvry9cXV3Ru3dvHDp0SFwvCAISExMRFBQEV1dXxMTE4MyZMw6s2DmZTCYsXrwYYWFhcHV1RZcuXfDqq69aPAqE57ppfvjhBzz66KMIDg6GRCLB1q1bLdY35rwWFxdj7Nix8PLygre3NyZOnIjr16/bpD4GKCfx2WefYdasWViyZAkOHz6Mvn37IjY2FoWFhY4uzant2bMHU6dOxYEDB5CamgqDwYDhw4ejoqJCbDNz5kx888032LJlC/bs2YMrV67gz3/+swOrdn4HDx7EP//5T/Tp08diOc+1bZSUlOC+++6Di4sLdu7ciZ9//hkrVqyAj4+P2CY5ORlr1qzBunXrkJGRAXd3d8TGxqKqqsqBlTufN954A++++y7Wrl2LU6dO4Y033kBycjLeeustsQ3PddNUVFSgb9++ePvttxtc35jzOnbsWJw8eRKpqanYtm0bfvjhB0yaNMk2BQrkFO655x5h6tSp4nuTySQEBwcLSUlJDqyq9SksLBQACHv27BEEQRBKS0sFFxcXYcuWLWKbU6dOCQCE9PR0R5Xp1MrLy4Xw8HAhNTVVuP/++4UZM2YIgsBzbUtz584VBg8e/LvrzWazoNFohOXLl4vLSktLBaVSKXz66afNUWKrMXLkSOG5556zWPbnP/9ZGDt2rCAIPNe2AkD48ssvxfeNOa8///yzAEA4ePCg2Gbnzp2CRCIRLl++fMc1sQfKCVRXVyMzMxMxMTHiMqlUipiYGKSnpzuwstanrKwMANCuXTsAQGZmJgwGg8W5j4iIQIcOHXjum2jq1KkYOXKkxTkFeK5t6euvv8bAgQPx5JNPIiAgAP3798e//vUvcX1ubi60Wq3FuVar1YiKiuK5ttKgQYOQlpaG06dPAwCOHj2KvXv34qGHHgLAc20vjTmv6enp8Pb2xsCBA8U2MTExkEqlyMjIuOMa+Cw8J1BUVASTyYTAwECL5YGBgcjOznZQVa2P2WxGQkIC7rvvPvTq1QsAoNVqoVAo4O3tbdE2MDAQWq3WAVU6t82bN+Pw4cM4ePBgvXU817Zz7tw5vPvuu5g1axYWLFiAgwcPYvr06VAoFIiPjxfPZ0P/pvBcW2fevHnQ6XSIiIiATCaDyWTC66+/jrFjxwIAz7WdNOa8arVaBAQEWKyXy+Vo166dTc49AxRRralTp+LEiRPYu3evo0tplS5evIgZM2YgNTUVKpXK0eW0amazGQMHDsTf//53AED//v1x4sQJrFu3DvHx8Q6urnX5/PPPsWnTJnzyySfo2bMnjhw5goSEBAQHB/Nct3K8hOcE/Pz8IJPJ6t2NVFBQAI1G46CqWpdp06Zh27Zt+P777xESEiIu12g0qK6uRmlpqUV7nnvrZWZmorCwEAMGDIBcLodcLseePXuwZs0ayOVyBAYG8lzbSFBQEHr06GGxrHv37sjLywMA8Xzy35Q79/LLL2PevHkYNWoUevfujXHjxmHmzJlISkoCwHNtL405rxqNpt6NVkajEcXFxTY59wxQTkChUCAyMhJpaWniMrPZjLS0NERHRzuwMucnCAKmTZuGL7/8Et999x3CwsIs1kdGRsLFxcXi3Ofk5CAvL4/n3koPPvggjh8/jiNHjoivgQMHYuzYseLPPNe2cd9999WbjuP06dPo2LEjACAsLAwajcbiXOt0OmRkZPBcW6myshJSqeVXqUwmg9lsBsBzbS+NOa/R0dEoLS1FZmam2Oa7776D2WxGVFTUnRdxx8PQqVls3rxZUCqVwsaNG4Wff/5ZmDRpkuDt7S1otVpHl+bUpkyZIqjVamH37t1Cfn6++KqsrBTbTJ48WejQoYPw3XffCYcOHRKio6OF6OhoB1bdetx8F54g8Fzbyk8//STI5XLh9ddfF86cOSNs2rRJcHNzEz7++GOxzbJlywRvb2/hq6++Eo4dOyY8/vjjQlhYmHDjxg0HVu584uPjhfbt2wvbtm0TcnNzhS+++ELw8/MT5syZI7bhuW6a8vJyISsrS8jKyhIACCtXrhSysrKECxcuCILQuPM6YsQIoX///kJGRoawd+9eITw8XBg9erRN6mOAciJvvfWW0KFDB0GhUAj33HOPcODAAUeX5PQANPjasGGD2ObGjRvCX//6V8HHx0dwc3MT/vSnPwn5+fmOK7oV+W2A4rm2nW+++Ubo1auXoFQqhYiICGH9+vUW681ms7B48WIhMDBQUCqVwoMPPijk5OQ4qFrnpdPphBkzZggdOnQQVCqV0LlzZ2HhwoWCXq8X2/BcN83333/f4L/P8fHxgiA07rxeu3ZNGD16tODh4SF4eXkJzz77rFBeXm6T+iSCcNN0qURERER0WxwDRURERGQlBigiIiIiKzFAEREREVmJAYqIiIjISgxQRERERFZigCIiIiKyEgMUERERkZUYoIiIiIisxABFRFTr/PnzkEgkOHLkiN0+Y8KECYiLi7Pb/omoeTBAEVGrMWHCBEgkknqvESNGNGr70NBQ5Ofno1evXnaulIicndzRBRAR2dKIESOwYcMGi2VKpbJR28pkMmg0GnuURUStDHugiKhVUSqV0Gg0Fi8fHx8AgEQiwbvvvouHHnoIrq6u6Ny5M/7zn/+I2/72El5JSQnGjh0Lf39/uLq6Ijw83CKcHT9+HH/4wx/g6uoKX19fTJo0CdevXxfXm0wmzJo1C97e3vD19cWcOXPw28ePms1mJCUlISwsDK6urujbt69FTUTUMjFAEVGbsnjxYjzxxBM4evQoxo4di1GjRuHUqVO/2/bnn3/Gzp07cerUKbz77rvw8/MDAFRUVCA2NhY+Pj44ePAgtmzZgv/973+YNm2auP2KFSuwceNGfPDBB9i7dy+Ki4vx5ZdfWnxGUlISPvroI6xbtw4nT57EzJkz8cwzz2DPnj32OwlEdOcEIqJWIj4+XpDJZIK7u7vF6/XXXxcEQRAACJMnT7bYJioqSpgyZYogCIKQm5srABCysrIEQRCERx99VHj22Wcb/Kz169cLPj4+wvXr18Vl27dvF6RSqaDVagVBEISgoCAhOTlZXG8wGISQkBDh8ccfFwRBEKqqqgQ3Nzdh//79FvueOHGiMHr06KafCCKyO46BIqJWZdiwYXj33XctlrVr1078OTo62mJddHT07951N2XKFDzxxBM4fPgwhg8fjri4OAwaNAgAcOrUKfTt2xfu7u5i+/vuuw9msxk5OTlQqVTIz89HVFSUuF4ul2PgwIHiZbyzZ8+isrISf/zjHy0+t7q6Gv3797f+4Imo2TBAEVGr4u7ujq5du9pkXw899BAuXLiAHTt2IDU1FQ8++CCmTp2KN9980yb7rxsvtX37drRv395iXWMHvhORY3AMFBG1KQcOHKj3vnv37r/b3t/fH/Hx8fj444+xatUqrF+/HgDQvXt3HD16FBUVFWLbffv2QSqVolu3blCr1QgKCkJGRoa43mg0IjMzU3zfo0cPKJVK5OXloWvXrhav0NBQWx0yEdkBe6CIqFXR6/XQarUWy+RyuTj4e8uWLRg4cCAGDx6MTZs24aeffsL777/f4L4SExMRGRmJnj17Qq/XY9u2bWLYGjt2LJYsWYL4+HgsXboUV69exYsvvohx48YhMDAQADBjxgwsW7YM4eHhiIiIwMqVK1FaWiru39PTEy+99BJmzpwJs9mMwYMHo6ysDPv27YOXlxfi4+PtcIaIyBYYoIioVUlJSUFQUJDFsm7duiE7OxsA8Morr2Dz5s3461//iqCgIHz66afo0aNHg/tSKBSYP38+zp8/D1dXVwwZMgSbN28GALi5ueHbb7/FjBkzcPfdd8PNzQ1PPPEEVq5cKW4/e/Zs5OfnIz4+HlKpFM899xz+9Kc/oaysTGzz6quvwt/fH0lJSTh37hy8vb0xYMAALFiwwNanhohsSCIIv5mUhIiolZJIJPjyyy/5KBUiumMcA0VERERkJQYoIiIiIitxDBQRtRkcsUBEtsIeKCIiIiIrMUARERERWYkBioiIiMhKDFBEREREVmKAIiIiIrISAxQRERGRlRigiIiIiKzEAEVERERkJQYoIiIiIiv9P3VS65nKYJymAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import the tqdm library for progress display\n",
    "\n",
    "ep_reward_list = []\n",
    "avg_reward_list = []\n",
    "\n",
    "buffer = Buffer(50000, 64)\n",
    "ou_noise = OUActionNoise(mean=torch.zeros(1), std_deviation=0.2 * torch.ones(1))\n",
    "\n",
    "def policy(state, noise):\n",
    "    # Use the actor network to predict actions with added noise for exploration\n",
    "    with torch.no_grad():\n",
    "        action = actor(state) + noise()\n",
    "    return action.clamp(lower_bound, upper_bound)\n",
    "\n",
    "# Set a maximum step limit per episode to prevent infinite loops\n",
    "MAX_STEPS_PER_EPISODE = 1000\n",
    "\n",
    "# Wrap the main loop with tqdm for progress tracking\n",
    "for ep in tqdm(range(100), desc=\"Training Progress\"):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # Unpack if the environment returns a tuple\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    episodic_reward = 0\n",
    "\n",
    "    for step in range(MAX_STEPS_PER_EPISODE):  # Add a step limit\n",
    "        # Select action using the policy and apply it to the environment\n",
    "        action = policy(state.unsqueeze(0), ou_noise).squeeze(0).numpy()\n",
    "        next_state, reward, done, *_ = env.step(action)\n",
    "\n",
    "        # Record the transition in the replay buffer\n",
    "        buffer.record((state.numpy(), action, reward, next_state))\n",
    "        state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "        # Perform training when there are enough samples in the buffer\n",
    "        if buffer.buffer_counter > buffer.batch_size:\n",
    "            state_batch, action_batch, reward_batch, next_state_batch = buffer.sample()\n",
    "\n",
    "            # Update Critic network\n",
    "            target_actions = actor_target(next_state_batch)\n",
    "            y = reward_batch + 0.99 * critic_target(next_state_batch, target_actions)\n",
    "            critic_loss = F.mse_loss(critic(state_batch, action_batch), y.detach())\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Update Actor network\n",
    "            actor_loss = -critic(state_batch, actor(state_batch)).mean()\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Update target networks using soft updates\n",
    "            with torch.no_grad():\n",
    "                for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
    "                    target_param.data.copy_(0.005 * param.data + 0.995 * target_param.data)\n",
    "\n",
    "                for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
    "                    target_param.data.copy_(0.005 * param.data + 0.995 * target_param.data)\n",
    "\n",
    "        episodic_reward += reward\n",
    "        if done:  # Exit the loop if the episode ends\n",
    "            break\n",
    "\n",
    "    # Append the episodic reward and calculate the moving average\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "    # Print the average reward for the current episode\n",
    "    tqdm.write(f\"Episode {ep}, Avg Reward: {avg_reward}\")\n",
    "\n",
    "# Plot the average reward trend\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If training proceeds correctly, the average episodic reward will increase with time.\n",
    "\n",
    "Feel free to try different learning rates, tau values, and architectures for the Actor and Critic networks.\n",
    "\n",
    "The Inverted Pendulum problem has low complexity, but DDPG work great on many other problems.\n",
    "\n",
    "Another great environment to try this on is LunarLandingContinuous-v2, but it will take more episodes to obtain good results.\n",
    "\n",
    "before training:\n",
    "![title](https://i.imgur.com/ox6b9rC.giff)\n",
    "\n",
    "after training:\n",
    "![title](https://i.imgur.com/eEH8Cz6.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Materials: Reinforcement Learning Applications in Material Science\n",
    "\n",
    "In material science, many decisions involve complex trade-offs such as optimizing component ratios, reaction conditions, or synthesis parameters. Reinforcement Learning (RL) offers a promising approach to automatically explore and optimize these parameter spaces. In this section, we introduce a custom Gym environment for materials design and demonstrate how to integrate it with an Actor-Critic method.\n",
    "\n",
    "## 1. Custom Materials Environment\n",
    "\n",
    "We begin by defining a custom OpenAI Gym environment. In this example, the state is represented by three continuous values (for example, representing different component fractions) and the action is a small adjustment to these values. The goal is to adjust the state so that it approaches a specified target vector (which represents an optimal material composition).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [0.31706378 0.24520058 0.66345886]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class MaterialsEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Materials Design\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MaterialsEnv, self).__init__()\n",
    "        # Define state space (e.g., three component fractions ranging from 0 to 1)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3,), dtype=np.float32)\n",
    "        # Define action space (small adjustments for each component, between -0.1 and 0.1)\n",
    "        self.action_space = spaces.Box(low=-0.1, high=0.1, shape=(3,), dtype=np.float32)\n",
    "        # Initialize state randomly\n",
    "        self.state = np.random.rand(3)\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 50  # Max steps per episode\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.rand(3)\n",
    "        self.current_step = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update the state by applying the action and clip the values to remain within [0, 1]\n",
    "        self.state = np.clip(self.state + action, 0, 1)\n",
    "        # Define a reward based on closeness to a target material composition\n",
    "        target = np.array([0.7, 0.2, 0.1])\n",
    "        reward = -np.linalg.norm(self.state - target)  # The smaller the distance, the higher the reward\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"Step: {self.current_step}, State: {self.state}\")\n",
    "\n",
    "# Test the custom environment\n",
    "if __name__ == '__main__':\n",
    "    env = MaterialsEnv()\n",
    "    initial_state = env.reset()\n",
    "    print(\"Initial State:\", initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Integrate with the Actor-Critic Method\n",
    "Now, modify the Actor-Critic agent from the CartPole example to work with the materials environment. Adjust the model input and output dimensions to match the environment's state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average Reward: -37.658\n",
      "Episode 1, Average Reward: -39.064\n",
      "Episode 2, Average Reward: -41.529\n",
      "Episode 3, Average Reward: -41.832\n",
      "Episode 4, Average Reward: -43.322\n",
      "Episode 5, Average Reward: -41.968\n",
      "Episode 6, Average Reward: -43.160\n",
      "Episode 7, Average Reward: -42.664\n",
      "Episode 8, Average Reward: -41.506\n",
      "Episode 9, Average Reward: -42.467\n",
      "Episode 10, Average Reward: -42.663\n",
      "Episode 11, Average Reward: -43.375\n",
      "Episode 12, Average Reward: -43.932\n",
      "Episode 13, Average Reward: -44.199\n",
      "Episode 14, Average Reward: -44.642\n",
      "Episode 15, Average Reward: -45.171\n",
      "Episode 16, Average Reward: -45.481\n",
      "Episode 17, Average Reward: -45.904\n",
      "Episode 18, Average Reward: -46.237\n",
      "Episode 19, Average Reward: -46.469\n",
      "Episode 20, Average Reward: -46.726\n",
      "Episode 21, Average Reward: -46.863\n",
      "Episode 22, Average Reward: -47.060\n",
      "Episode 23, Average Reward: -47.128\n",
      "Episode 24, Average Reward: -47.149\n",
      "Episode 25, Average Reward: -47.141\n",
      "Episode 26, Average Reward: -47.319\n",
      "Episode 27, Average Reward: -47.525\n",
      "Episode 28, Average Reward: -47.581\n",
      "Episode 29, Average Reward: -47.680\n",
      "Episode 30, Average Reward: -47.816\n",
      "Episode 31, Average Reward: -47.961\n",
      "Episode 32, Average Reward: -48.075\n",
      "Episode 33, Average Reward: -48.177\n",
      "Episode 34, Average Reward: -48.179\n",
      "Episode 35, Average Reward: -48.191\n",
      "Episode 36, Average Reward: -48.296\n",
      "Episode 37, Average Reward: -48.350\n",
      "Episode 38, Average Reward: -48.451\n",
      "Episode 39, Average Reward: -48.533\n",
      "Episode 40, Average Reward: -48.590\n",
      "Episode 41, Average Reward: -48.549\n",
      "Episode 42, Average Reward: -48.605\n",
      "Episode 43, Average Reward: -48.622\n",
      "Episode 44, Average Reward: -48.685\n",
      "Episode 45, Average Reward: -48.710\n",
      "Episode 46, Average Reward: -48.779\n",
      "Episode 47, Average Reward: -48.878\n",
      "Episode 48, Average Reward: -48.904\n",
      "Episode 49, Average Reward: -48.967\n",
      "Episode 50, Average Reward: -49.251\n",
      "Episode 51, Average Reward: -49.452\n",
      "Episode 52, Average Reward: -49.550\n",
      "Episode 53, Average Reward: -49.748\n",
      "Episode 54, Average Reward: -49.822\n",
      "Episode 55, Average Reward: -50.159\n",
      "Episode 56, Average Reward: -50.132\n",
      "Episode 57, Average Reward: -50.302\n",
      "Episode 58, Average Reward: -50.720\n",
      "Episode 59, Average Reward: -50.721\n",
      "Episode 60, Average Reward: -50.871\n",
      "Episode 61, Average Reward: -50.896\n",
      "Episode 62, Average Reward: -50.861\n",
      "Episode 63, Average Reward: -50.893\n",
      "Episode 64, Average Reward: -50.930\n",
      "Episode 65, Average Reward: -50.884\n",
      "Episode 66, Average Reward: -50.877\n",
      "Episode 67, Average Reward: -50.855\n",
      "Episode 68, Average Reward: -50.849\n",
      "Episode 69, Average Reward: -50.859\n",
      "Episode 70, Average Reward: -50.864\n",
      "Episode 71, Average Reward: -50.906\n",
      "Episode 72, Average Reward: -50.946\n",
      "Episode 73, Average Reward: -51.004\n",
      "Episode 74, Average Reward: -51.094\n",
      "Episode 75, Average Reward: -51.220\n",
      "Episode 76, Average Reward: -51.218\n",
      "Episode 77, Average Reward: -51.146\n",
      "Episode 78, Average Reward: -51.155\n",
      "Episode 79, Average Reward: -51.161\n",
      "Episode 80, Average Reward: -51.119\n",
      "Episode 81, Average Reward: -51.134\n",
      "Episode 82, Average Reward: -51.135\n",
      "Episode 83, Average Reward: -51.158\n",
      "Episode 84, Average Reward: -51.184\n",
      "Episode 85, Average Reward: -51.228\n",
      "Episode 86, Average Reward: -51.225\n",
      "Episode 87, Average Reward: -51.220\n",
      "Episode 88, Average Reward: -51.192\n",
      "Episode 89, Average Reward: -51.209\n",
      "Episode 90, Average Reward: -51.262\n",
      "Episode 91, Average Reward: -51.354\n",
      "Episode 92, Average Reward: -51.389\n",
      "Episode 93, Average Reward: -51.417\n",
      "Episode 94, Average Reward: -51.417\n",
      "Episode 95, Average Reward: -51.382\n",
      "Episode 96, Average Reward: -51.402\n",
      "Episode 97, Average Reward: -51.321\n",
      "Episode 98, Average Reward: -51.360\n",
      "Episode 99, Average Reward: -51.289\n",
      "Episode 100, Average Reward: -51.294\n",
      "Episode 101, Average Reward: -51.358\n",
      "Episode 102, Average Reward: -51.353\n",
      "Episode 103, Average Reward: -51.343\n",
      "Episode 104, Average Reward: -51.286\n",
      "Episode 105, Average Reward: -51.259\n",
      "Episode 106, Average Reward: -51.350\n",
      "Episode 107, Average Reward: -51.401\n",
      "Episode 108, Average Reward: -51.352\n",
      "Episode 109, Average Reward: -51.410\n",
      "Episode 110, Average Reward: -51.424\n",
      "Episode 111, Average Reward: -51.438\n",
      "Episode 112, Average Reward: -51.429\n",
      "Episode 113, Average Reward: -51.426\n",
      "Episode 114, Average Reward: -51.390\n",
      "Episode 115, Average Reward: -51.432\n",
      "Episode 116, Average Reward: -51.461\n",
      "Episode 117, Average Reward: -51.454\n",
      "Episode 118, Average Reward: -51.425\n",
      "Episode 119, Average Reward: -51.441\n",
      "Episode 120, Average Reward: -51.447\n",
      "Episode 121, Average Reward: -51.474\n",
      "Episode 122, Average Reward: -51.456\n",
      "Episode 123, Average Reward: -51.473\n",
      "Episode 124, Average Reward: -51.505\n",
      "Episode 125, Average Reward: -51.451\n",
      "Episode 126, Average Reward: -51.470\n",
      "Episode 127, Average Reward: -51.521\n",
      "Episode 128, Average Reward: -51.549\n",
      "Episode 129, Average Reward: -51.579\n",
      "Episode 130, Average Reward: -51.554\n",
      "Episode 131, Average Reward: -51.484\n",
      "Episode 132, Average Reward: -51.487\n",
      "Episode 133, Average Reward: -51.466\n",
      "Episode 134, Average Reward: -51.521\n",
      "Episode 135, Average Reward: -51.570\n",
      "Episode 136, Average Reward: -51.511\n",
      "Episode 137, Average Reward: -51.505\n",
      "Episode 138, Average Reward: -51.494\n",
      "Episode 139, Average Reward: -51.444\n",
      "Episode 140, Average Reward: -51.435\n",
      "Episode 141, Average Reward: -51.429\n",
      "Episode 142, Average Reward: -51.387\n",
      "Episode 143, Average Reward: -51.423\n",
      "Episode 144, Average Reward: -51.442\n",
      "Episode 145, Average Reward: -51.520\n",
      "Episode 146, Average Reward: -51.481\n",
      "Episode 147, Average Reward: -51.509\n",
      "Episode 148, Average Reward: -51.513\n",
      "Episode 149, Average Reward: -51.592\n",
      "Episode 150, Average Reward: -51.519\n",
      "Episode 151, Average Reward: -51.487\n",
      "Episode 152, Average Reward: -51.488\n",
      "Episode 153, Average Reward: -51.461\n",
      "Episode 154, Average Reward: -51.495\n",
      "Episode 155, Average Reward: -51.521\n",
      "Episode 156, Average Reward: -51.519\n",
      "Episode 157, Average Reward: -51.482\n",
      "Episode 158, Average Reward: -51.528\n",
      "Episode 159, Average Reward: -51.505\n",
      "Episode 160, Average Reward: -51.471\n",
      "Episode 161, Average Reward: -51.476\n",
      "Episode 162, Average Reward: -51.540\n",
      "Episode 163, Average Reward: -51.563\n",
      "Episode 164, Average Reward: -51.558\n",
      "Episode 165, Average Reward: -51.497\n",
      "Episode 166, Average Reward: -51.503\n",
      "Episode 167, Average Reward: -51.509\n",
      "Episode 168, Average Reward: -51.552\n",
      "Episode 169, Average Reward: -51.483\n",
      "Episode 170, Average Reward: -51.368\n",
      "Episode 171, Average Reward: -51.360\n",
      "Episode 172, Average Reward: -51.316\n",
      "Episode 173, Average Reward: -51.253\n",
      "Episode 174, Average Reward: -51.095\n",
      "Episode 175, Average Reward: -51.103\n",
      "Episode 176, Average Reward: -51.062\n",
      "Episode 177, Average Reward: -51.019\n",
      "Episode 178, Average Reward: -51.050\n",
      "Episode 179, Average Reward: -51.038\n",
      "Episode 180, Average Reward: -50.925\n",
      "Episode 181, Average Reward: -50.744\n",
      "Episode 182, Average Reward: -50.564\n",
      "Episode 183, Average Reward: -50.332\n",
      "Episode 184, Average Reward: -50.133\n",
      "Episode 185, Average Reward: -49.929\n",
      "Episode 186, Average Reward: -49.774\n",
      "Episode 187, Average Reward: -49.567\n",
      "Episode 188, Average Reward: -49.407\n",
      "Episode 189, Average Reward: -49.238\n",
      "Episode 190, Average Reward: -49.040\n",
      "Episode 191, Average Reward: -48.872\n",
      "Episode 192, Average Reward: -48.709\n",
      "Episode 193, Average Reward: -48.507\n",
      "Episode 194, Average Reward: -48.316\n",
      "Episode 195, Average Reward: -48.125\n",
      "Episode 196, Average Reward: -47.953\n",
      "Episode 197, Average Reward: -47.746\n",
      "Episode 198, Average Reward: -47.546\n",
      "Episode 199, Average Reward: -47.308\n",
      "Episode 200, Average Reward: -47.191\n",
      "Episode 201, Average Reward: -47.001\n",
      "Episode 202, Average Reward: -46.789\n",
      "Episode 203, Average Reward: -46.549\n",
      "Episode 204, Average Reward: -46.304\n",
      "Episode 205, Average Reward: -46.123\n",
      "Episode 206, Average Reward: -45.922\n",
      "Episode 207, Average Reward: -45.819\n",
      "Episode 208, Average Reward: -45.584\n",
      "Episode 209, Average Reward: -45.306\n",
      "Episode 210, Average Reward: -45.091\n",
      "Episode 211, Average Reward: -44.901\n",
      "Episode 212, Average Reward: -44.673\n",
      "Episode 213, Average Reward: -44.529\n",
      "Episode 214, Average Reward: -44.362\n",
      "Episode 215, Average Reward: -44.214\n",
      "Episode 216, Average Reward: -44.024\n",
      "Episode 217, Average Reward: -44.153\n",
      "Episode 218, Average Reward: -43.959\n",
      "Episode 219, Average Reward: -43.839\n",
      "Episode 220, Average Reward: -43.712\n",
      "Episode 221, Average Reward: -43.484\n",
      "Episode 222, Average Reward: -43.334\n",
      "Episode 223, Average Reward: -43.199\n",
      "Episode 224, Average Reward: -43.138\n",
      "Episode 225, Average Reward: -42.985\n",
      "Episode 226, Average Reward: -42.815\n",
      "Episode 227, Average Reward: -42.683\n",
      "Episode 228, Average Reward: -42.502\n",
      "Episode 229, Average Reward: -42.340\n",
      "Episode 230, Average Reward: -42.328\n",
      "Episode 231, Average Reward: -42.379\n",
      "Episode 232, Average Reward: -42.338\n",
      "Episode 233, Average Reward: -42.374\n",
      "Episode 234, Average Reward: -42.374\n",
      "Episode 235, Average Reward: -42.370\n",
      "Episode 236, Average Reward: -42.393\n",
      "Episode 237, Average Reward: -42.447\n",
      "Episode 238, Average Reward: -42.408\n",
      "Episode 239, Average Reward: -42.436\n",
      "Episode 240, Average Reward: -42.390\n",
      "Episode 241, Average Reward: -42.358\n",
      "Episode 242, Average Reward: -42.381\n",
      "Episode 243, Average Reward: -42.391\n",
      "Episode 244, Average Reward: -42.395\n",
      "Episode 245, Average Reward: -42.401\n",
      "Episode 246, Average Reward: -42.417\n",
      "Episode 247, Average Reward: -42.431\n",
      "Episode 248, Average Reward: -42.437\n",
      "Episode 249, Average Reward: -42.474\n",
      "Episode 250, Average Reward: -42.441\n",
      "Episode 251, Average Reward: -42.434\n",
      "Episode 252, Average Reward: -42.478\n",
      "Episode 253, Average Reward: -42.509\n",
      "Episode 254, Average Reward: -42.579\n",
      "Episode 255, Average Reward: -42.579\n",
      "Episode 256, Average Reward: -42.514\n",
      "Episode 257, Average Reward: -42.525\n",
      "Episode 258, Average Reward: -42.550\n",
      "Episode 259, Average Reward: -42.621\n",
      "Episode 260, Average Reward: -42.670\n",
      "Episode 261, Average Reward: -42.651\n",
      "Episode 262, Average Reward: -42.690\n",
      "Episode 263, Average Reward: -42.689\n",
      "Episode 264, Average Reward: -42.629\n",
      "Episode 265, Average Reward: -42.571\n",
      "Episode 266, Average Reward: -42.535\n",
      "Episode 267, Average Reward: -42.173\n",
      "Episode 268, Average Reward: -42.149\n",
      "Episode 269, Average Reward: -42.125\n",
      "Episode 270, Average Reward: -42.119\n",
      "Episode 271, Average Reward: -42.087\n",
      "Episode 272, Average Reward: -42.077\n",
      "Episode 273, Average Reward: -42.033\n",
      "Episode 274, Average Reward: -42.026\n",
      "Episode 275, Average Reward: -41.968\n",
      "Episode 276, Average Reward: -41.999\n",
      "Episode 277, Average Reward: -41.924\n",
      "Episode 278, Average Reward: -41.802\n",
      "Episode 279, Average Reward: -41.746\n",
      "Episode 280, Average Reward: -41.708\n",
      "Episode 281, Average Reward: -41.647\n",
      "Episode 282, Average Reward: -41.680\n",
      "Episode 283, Average Reward: -41.624\n",
      "Episode 284, Average Reward: -41.610\n",
      "Episode 285, Average Reward: -41.586\n",
      "Episode 286, Average Reward: -41.568\n",
      "Episode 287, Average Reward: -41.503\n",
      "Episode 288, Average Reward: -41.547\n",
      "Episode 289, Average Reward: -41.446\n",
      "Episode 290, Average Reward: -41.458\n",
      "Episode 291, Average Reward: -41.490\n",
      "Episode 292, Average Reward: -41.419\n",
      "Episode 293, Average Reward: -41.262\n",
      "Episode 294, Average Reward: -41.154\n",
      "Episode 295, Average Reward: -41.058\n",
      "Episode 296, Average Reward: -40.995\n",
      "Episode 297, Average Reward: -41.020\n",
      "Episode 298, Average Reward: -41.019\n",
      "Episode 299, Average Reward: -41.026\n",
      "Episode 300, Average Reward: -41.054\n",
      "Episode 301, Average Reward: -41.081\n",
      "Episode 302, Average Reward: -41.083\n",
      "Episode 303, Average Reward: -41.070\n",
      "Episode 304, Average Reward: -41.080\n",
      "Episode 305, Average Reward: -41.049\n",
      "Episode 306, Average Reward: -41.085\n",
      "Episode 307, Average Reward: -41.064\n",
      "Episode 308, Average Reward: -41.052\n",
      "Episode 309, Average Reward: -41.062\n",
      "Episode 310, Average Reward: -41.049\n",
      "Episode 311, Average Reward: -40.981\n",
      "Episode 312, Average Reward: -41.005\n",
      "Episode 313, Average Reward: -41.005\n",
      "Episode 314, Average Reward: -41.055\n",
      "Episode 315, Average Reward: -41.046\n",
      "Episode 316, Average Reward: -41.062\n",
      "Episode 317, Average Reward: -41.094\n",
      "Episode 318, Average Reward: -41.116\n",
      "Episode 319, Average Reward: -41.099\n",
      "Episode 320, Average Reward: -41.101\n",
      "Episode 321, Average Reward: -41.151\n",
      "Episode 322, Average Reward: -41.110\n",
      "Episode 323, Average Reward: -41.161\n",
      "Episode 324, Average Reward: -41.101\n",
      "Episode 325, Average Reward: -41.150\n",
      "Episode 326, Average Reward: -41.134\n",
      "Episode 327, Average Reward: -41.179\n",
      "Episode 328, Average Reward: -41.242\n",
      "Episode 329, Average Reward: -41.213\n",
      "Episode 330, Average Reward: -41.257\n",
      "Episode 331, Average Reward: -41.259\n",
      "Episode 332, Average Reward: -41.235\n",
      "Episode 333, Average Reward: -41.314\n",
      "Episode 334, Average Reward: -41.336\n",
      "Episode 335, Average Reward: -41.282\n",
      "Episode 336, Average Reward: -41.312\n",
      "Episode 337, Average Reward: -41.342\n",
      "Episode 338, Average Reward: -41.279\n",
      "Episode 339, Average Reward: -41.380\n",
      "Episode 340, Average Reward: -41.366\n",
      "Episode 341, Average Reward: -41.322\n",
      "Episode 342, Average Reward: -41.339\n",
      "Episode 343, Average Reward: -41.491\n",
      "Episode 344, Average Reward: -41.572\n",
      "Episode 345, Average Reward: -41.648\n",
      "Episode 346, Average Reward: -41.665\n",
      "Episode 347, Average Reward: -41.608\n",
      "Episode 348, Average Reward: -41.612\n",
      "Episode 349, Average Reward: -41.581\n",
      "Episode 350, Average Reward: -41.592\n",
      "Episode 351, Average Reward: -41.544\n",
      "Episode 352, Average Reward: -41.533\n",
      "Episode 353, Average Reward: -41.571\n",
      "Episode 354, Average Reward: -41.496\n",
      "Episode 355, Average Reward: -41.475\n",
      "Episode 356, Average Reward: -41.511\n",
      "Episode 357, Average Reward: -41.480\n",
      "Episode 358, Average Reward: -41.487\n",
      "Episode 359, Average Reward: -41.442\n",
      "Episode 360, Average Reward: -41.447\n",
      "Episode 361, Average Reward: -41.508\n",
      "Episode 362, Average Reward: -41.479\n",
      "Episode 363, Average Reward: -41.431\n",
      "Episode 364, Average Reward: -41.440\n",
      "Episode 365, Average Reward: -41.488\n",
      "Episode 366, Average Reward: -41.451\n",
      "Episode 367, Average Reward: -41.452\n",
      "Episode 368, Average Reward: -41.453\n",
      "Episode 369, Average Reward: -41.424\n",
      "Episode 370, Average Reward: -41.472\n",
      "Episode 371, Average Reward: -41.487\n",
      "Episode 372, Average Reward: -41.534\n",
      "Episode 373, Average Reward: -41.538\n",
      "Episode 374, Average Reward: -41.602\n",
      "Episode 375, Average Reward: -41.609\n",
      "Episode 376, Average Reward: -41.601\n",
      "Episode 377, Average Reward: -41.608\n",
      "Episode 378, Average Reward: -41.643\n",
      "Episode 379, Average Reward: -41.694\n",
      "Episode 380, Average Reward: -41.645\n",
      "Episode 381, Average Reward: -41.697\n",
      "Episode 382, Average Reward: -41.699\n",
      "Episode 383, Average Reward: -41.700\n",
      "Episode 384, Average Reward: -41.701\n",
      "Episode 385, Average Reward: -41.760\n",
      "Episode 386, Average Reward: -41.713\n",
      "Episode 387, Average Reward: -41.768\n",
      "Episode 388, Average Reward: -41.806\n",
      "Episode 389, Average Reward: -41.806\n",
      "Episode 390, Average Reward: -41.850\n",
      "Episode 391, Average Reward: -41.874\n",
      "Episode 392, Average Reward: -41.899\n",
      "Episode 393, Average Reward: -41.896\n",
      "Episode 394, Average Reward: -41.914\n",
      "Episode 395, Average Reward: -41.911\n",
      "Episode 396, Average Reward: -41.951\n",
      "Episode 397, Average Reward: -42.018\n",
      "Episode 398, Average Reward: -41.955\n",
      "Episode 399, Average Reward: -42.003\n",
      "Episode 400, Average Reward: -42.003\n",
      "Episode 401, Average Reward: -42.012\n",
      "Episode 402, Average Reward: -41.969\n",
      "Episode 403, Average Reward: -41.933\n",
      "Episode 404, Average Reward: -41.931\n",
      "Episode 405, Average Reward: -41.970\n",
      "Episode 406, Average Reward: -41.929\n",
      "Episode 407, Average Reward: -41.903\n",
      "Episode 408, Average Reward: -41.911\n",
      "Episode 409, Average Reward: -41.936\n",
      "Episode 410, Average Reward: -41.944\n",
      "Episode 411, Average Reward: -41.901\n",
      "Episode 412, Average Reward: -41.919\n",
      "Episode 413, Average Reward: -41.975\n",
      "Episode 414, Average Reward: -41.988\n",
      "Episode 415, Average Reward: -42.030\n",
      "Episode 416, Average Reward: -42.100\n",
      "Episode 417, Average Reward: -42.098\n",
      "Episode 418, Average Reward: -42.101\n",
      "Episode 419, Average Reward: -42.168\n",
      "Episode 420, Average Reward: -42.173\n",
      "Episode 421, Average Reward: -42.148\n",
      "Episode 422, Average Reward: -42.106\n",
      "Episode 423, Average Reward: -42.036\n",
      "Episode 424, Average Reward: -42.029\n",
      "Episode 425, Average Reward: -42.017\n",
      "Episode 426, Average Reward: -42.031\n",
      "Episode 427, Average Reward: -42.025\n",
      "Episode 428, Average Reward: -42.006\n",
      "Episode 429, Average Reward: -42.020\n",
      "Episode 430, Average Reward: -42.077\n",
      "Episode 431, Average Reward: -41.998\n",
      "Episode 432, Average Reward: -41.986\n",
      "Episode 433, Average Reward: -41.940\n",
      "Episode 434, Average Reward: -41.959\n",
      "Episode 435, Average Reward: -41.926\n",
      "Episode 436, Average Reward: -41.978\n",
      "Episode 437, Average Reward: -41.957\n",
      "Episode 438, Average Reward: -41.984\n",
      "Episode 439, Average Reward: -41.993\n",
      "Episode 440, Average Reward: -41.973\n",
      "Episode 441, Average Reward: -41.940\n",
      "Episode 442, Average Reward: -41.949\n",
      "Episode 443, Average Reward: -41.954\n",
      "Episode 444, Average Reward: -41.947\n",
      "Episode 445, Average Reward: -41.962\n",
      "Episode 446, Average Reward: -41.935\n",
      "Episode 447, Average Reward: -41.927\n",
      "Episode 448, Average Reward: -41.993\n",
      "Episode 449, Average Reward: -41.920\n",
      "Episode 450, Average Reward: -41.912\n",
      "Episode 451, Average Reward: -41.926\n",
      "Episode 452, Average Reward: -41.922\n",
      "Episode 453, Average Reward: -41.982\n",
      "Episode 454, Average Reward: -42.016\n",
      "Episode 455, Average Reward: -42.003\n",
      "Episode 456, Average Reward: -42.031\n",
      "Episode 457, Average Reward: -42.094\n",
      "Episode 458, Average Reward: -42.066\n",
      "Episode 459, Average Reward: -42.078\n",
      "Episode 460, Average Reward: -42.092\n",
      "Episode 461, Average Reward: -42.078\n",
      "Episode 462, Average Reward: -42.078\n",
      "Episode 463, Average Reward: -42.058\n",
      "Episode 464, Average Reward: -42.030\n",
      "Episode 465, Average Reward: -41.947\n",
      "Episode 466, Average Reward: -41.909\n",
      "Episode 467, Average Reward: -41.894\n",
      "Episode 468, Average Reward: -41.831\n",
      "Episode 469, Average Reward: -41.816\n",
      "Episode 470, Average Reward: -41.786\n",
      "Episode 471, Average Reward: -41.749\n",
      "Episode 472, Average Reward: -41.808\n",
      "Episode 473, Average Reward: -41.860\n",
      "Episode 474, Average Reward: -41.873\n",
      "Episode 475, Average Reward: -41.833\n",
      "Episode 476, Average Reward: -41.827\n",
      "Episode 477, Average Reward: -41.848\n",
      "Episode 478, Average Reward: -41.814\n",
      "Episode 479, Average Reward: -41.808\n",
      "Episode 480, Average Reward: -41.807\n",
      "Episode 481, Average Reward: -41.877\n",
      "Episode 482, Average Reward: -41.877\n",
      "Episode 483, Average Reward: -41.919\n",
      "Episode 484, Average Reward: -41.889\n",
      "Episode 485, Average Reward: -41.928\n",
      "Episode 486, Average Reward: -41.940\n",
      "Episode 487, Average Reward: -41.918\n",
      "Episode 488, Average Reward: -41.922\n",
      "Episode 489, Average Reward: -41.897\n",
      "Episode 490, Average Reward: -41.864\n",
      "Episode 491, Average Reward: -41.892\n",
      "Episode 492, Average Reward: -41.904\n",
      "Episode 493, Average Reward: -41.844\n",
      "Episode 494, Average Reward: -41.813\n",
      "Episode 495, Average Reward: -41.816\n",
      "Episode 496, Average Reward: -41.823\n",
      "Episode 497, Average Reward: -41.808\n",
      "Episode 498, Average Reward: -41.784\n",
      "Episode 499, Average Reward: -41.846\n",
      "Episode 500, Average Reward: -41.804\n",
      "Episode 501, Average Reward: -41.803\n",
      "Episode 502, Average Reward: -41.856\n",
      "Episode 503, Average Reward: -41.862\n",
      "Episode 504, Average Reward: -41.856\n",
      "Episode 505, Average Reward: -41.831\n",
      "Episode 506, Average Reward: -41.828\n",
      "Episode 507, Average Reward: -41.815\n",
      "Episode 508, Average Reward: -41.847\n",
      "Episode 509, Average Reward: -41.829\n",
      "Episode 510, Average Reward: -41.833\n",
      "Episode 511, Average Reward: -41.862\n",
      "Episode 512, Average Reward: -41.876\n",
      "Episode 513, Average Reward: -41.866\n",
      "Episode 514, Average Reward: -41.901\n",
      "Episode 515, Average Reward: -41.971\n",
      "Episode 516, Average Reward: -41.978\n",
      "Episode 517, Average Reward: -41.998\n",
      "Episode 518, Average Reward: -42.053\n",
      "Episode 519, Average Reward: -42.029\n",
      "Episode 520, Average Reward: -42.052\n",
      "Episode 521, Average Reward: -42.058\n",
      "Episode 522, Average Reward: -41.998\n",
      "Episode 523, Average Reward: -41.948\n",
      "Episode 524, Average Reward: -41.893\n",
      "Episode 525, Average Reward: -41.879\n",
      "Episode 526, Average Reward: -41.832\n",
      "Episode 527, Average Reward: -41.842\n",
      "Episode 528, Average Reward: -41.865\n",
      "Episode 529, Average Reward: -41.874\n",
      "Episode 530, Average Reward: -41.889\n",
      "Episode 531, Average Reward: -41.851\n",
      "Episode 532, Average Reward: -41.891\n",
      "Episode 533, Average Reward: -41.840\n",
      "Episode 534, Average Reward: -41.854\n",
      "Episode 535, Average Reward: -41.868\n",
      "Episode 536, Average Reward: -41.856\n",
      "Episode 537, Average Reward: -41.892\n",
      "Episode 538, Average Reward: -41.891\n",
      "Episode 539, Average Reward: -41.836\n",
      "Episode 540, Average Reward: -41.880\n",
      "Episode 541, Average Reward: -41.902\n",
      "Episode 542, Average Reward: -41.895\n",
      "Episode 543, Average Reward: -41.958\n",
      "Episode 544, Average Reward: -41.996\n",
      "Episode 545, Average Reward: -41.995\n",
      "Episode 546, Average Reward: -41.990\n",
      "Episode 547, Average Reward: -42.013\n",
      "Episode 548, Average Reward: -41.980\n",
      "Episode 549, Average Reward: -41.959\n",
      "Episode 550, Average Reward: -42.007\n",
      "Episode 551, Average Reward: -42.021\n",
      "Episode 552, Average Reward: -42.029\n",
      "Episode 553, Average Reward: -42.000\n",
      "Episode 554, Average Reward: -42.039\n",
      "Episode 555, Average Reward: -42.090\n",
      "Episode 556, Average Reward: -42.107\n",
      "Episode 557, Average Reward: -42.060\n",
      "Episode 558, Average Reward: -41.994\n",
      "Episode 559, Average Reward: -42.015\n",
      "Episode 560, Average Reward: -41.988\n",
      "Episode 561, Average Reward: -42.022\n",
      "Episode 562, Average Reward: -42.006\n",
      "Episode 563, Average Reward: -41.979\n",
      "Episode 564, Average Reward: -41.969\n",
      "Episode 565, Average Reward: -41.935\n",
      "Episode 566, Average Reward: -41.923\n",
      "Episode 567, Average Reward: -41.889\n",
      "Episode 568, Average Reward: -41.855\n",
      "Episode 569, Average Reward: -41.835\n",
      "Episode 570, Average Reward: -41.835\n",
      "Episode 571, Average Reward: -41.873\n",
      "Episode 572, Average Reward: -41.879\n",
      "Episode 573, Average Reward: -41.925\n",
      "Episode 574, Average Reward: -41.952\n",
      "Episode 575, Average Reward: -42.016\n",
      "Episode 576, Average Reward: -42.070\n",
      "Episode 577, Average Reward: -42.023\n",
      "Episode 578, Average Reward: -42.044\n",
      "Episode 579, Average Reward: -42.019\n",
      "Episode 580, Average Reward: -41.974\n",
      "Episode 581, Average Reward: -42.003\n",
      "Episode 582, Average Reward: -41.981\n",
      "Episode 583, Average Reward: -42.030\n",
      "Episode 584, Average Reward: -41.969\n",
      "Episode 585, Average Reward: -41.923\n",
      "Episode 586, Average Reward: -41.909\n",
      "Episode 587, Average Reward: -41.901\n",
      "Episode 588, Average Reward: -41.913\n",
      "Episode 589, Average Reward: -41.923\n",
      "Episode 590, Average Reward: -41.862\n",
      "Episode 591, Average Reward: -41.794\n",
      "Episode 592, Average Reward: -41.775\n",
      "Episode 593, Average Reward: -41.769\n",
      "Episode 594, Average Reward: -41.776\n",
      "Episode 595, Average Reward: -41.780\n",
      "Episode 596, Average Reward: -41.818\n",
      "Episode 597, Average Reward: -41.747\n",
      "Episode 598, Average Reward: -41.790\n",
      "Episode 599, Average Reward: -41.757\n",
      "Episode 600, Average Reward: -41.743\n",
      "Episode 601, Average Reward: -41.736\n",
      "Episode 602, Average Reward: -41.736\n",
      "Episode 603, Average Reward: -41.756\n",
      "Episode 604, Average Reward: -41.724\n",
      "Episode 605, Average Reward: -41.671\n",
      "Episode 606, Average Reward: -41.641\n",
      "Episode 607, Average Reward: -41.642\n",
      "Episode 608, Average Reward: -41.679\n",
      "Episode 609, Average Reward: -41.678\n",
      "Episode 610, Average Reward: -41.701\n",
      "Episode 611, Average Reward: -41.657\n",
      "Episode 612, Average Reward: -41.599\n",
      "Episode 613, Average Reward: -41.648\n",
      "Episode 614, Average Reward: -41.597\n",
      "Episode 615, Average Reward: -41.608\n",
      "Episode 616, Average Reward: -41.584\n",
      "Episode 617, Average Reward: -41.631\n",
      "Episode 618, Average Reward: -41.652\n",
      "Episode 619, Average Reward: -41.696\n",
      "Episode 620, Average Reward: -41.703\n",
      "Episode 621, Average Reward: -41.677\n",
      "Episode 622, Average Reward: -41.725\n",
      "Episode 623, Average Reward: -41.751\n",
      "Episode 624, Average Reward: -41.716\n",
      "Episode 625, Average Reward: -41.696\n",
      "Episode 626, Average Reward: -41.660\n",
      "Episode 627, Average Reward: -41.695\n",
      "Episode 628, Average Reward: -41.676\n",
      "Episode 629, Average Reward: -41.632\n",
      "Episode 630, Average Reward: -41.658\n",
      "Episode 631, Average Reward: -41.602\n",
      "Episode 632, Average Reward: -41.640\n",
      "Episode 633, Average Reward: -41.610\n",
      "Episode 634, Average Reward: -41.631\n",
      "Episode 635, Average Reward: -41.618\n",
      "Episode 636, Average Reward: -41.629\n",
      "Episode 637, Average Reward: -41.654\n",
      "Episode 638, Average Reward: -41.624\n",
      "Episode 639, Average Reward: -41.653\n",
      "Episode 640, Average Reward: -41.716\n",
      "Episode 641, Average Reward: -41.775\n",
      "Episode 642, Average Reward: -41.777\n",
      "Episode 643, Average Reward: -41.709\n",
      "Episode 644, Average Reward: -41.707\n",
      "Episode 645, Average Reward: -41.715\n",
      "Episode 646, Average Reward: -41.706\n",
      "Episode 647, Average Reward: -41.777\n",
      "Episode 648, Average Reward: -41.791\n",
      "Episode 649, Average Reward: -41.841\n",
      "Episode 650, Average Reward: -41.828\n",
      "Episode 651, Average Reward: -41.830\n",
      "Episode 652, Average Reward: -41.783\n",
      "Episode 653, Average Reward: -41.790\n",
      "Episode 654, Average Reward: -41.800\n",
      "Episode 655, Average Reward: -41.850\n",
      "Episode 656, Average Reward: -41.797\n",
      "Episode 657, Average Reward: -41.855\n",
      "Episode 658, Average Reward: -41.831\n",
      "Episode 659, Average Reward: -41.781\n",
      "Episode 660, Average Reward: -41.774\n",
      "Episode 661, Average Reward: -41.758\n",
      "Episode 662, Average Reward: -41.822\n",
      "Episode 663, Average Reward: -41.814\n",
      "Episode 664, Average Reward: -41.871\n",
      "Episode 665, Average Reward: -41.867\n",
      "Episode 666, Average Reward: -41.899\n",
      "Episode 667, Average Reward: -41.880\n",
      "Episode 668, Average Reward: -41.897\n",
      "Episode 669, Average Reward: -41.903\n",
      "Episode 670, Average Reward: -41.901\n",
      "Episode 671, Average Reward: -41.896\n",
      "Episode 672, Average Reward: -41.877\n",
      "Episode 673, Average Reward: -41.859\n",
      "Episode 674, Average Reward: -41.920\n",
      "Episode 675, Average Reward: -41.900\n",
      "Episode 676, Average Reward: -41.936\n",
      "Episode 677, Average Reward: -41.911\n",
      "Episode 678, Average Reward: -41.902\n",
      "Episode 679, Average Reward: -41.911\n",
      "Episode 680, Average Reward: -41.913\n",
      "Episode 681, Average Reward: -41.925\n",
      "Episode 682, Average Reward: -41.906\n",
      "Episode 683, Average Reward: -41.896\n",
      "Episode 684, Average Reward: -41.937\n",
      "Episode 685, Average Reward: -41.968\n",
      "Episode 686, Average Reward: -41.965\n",
      "Episode 687, Average Reward: -41.877\n",
      "Episode 688, Average Reward: -41.830\n",
      "Episode 689, Average Reward: -41.871\n",
      "Episode 690, Average Reward: -41.874\n",
      "Episode 691, Average Reward: -41.888\n",
      "Episode 692, Average Reward: -41.872\n",
      "Episode 693, Average Reward: -41.952\n",
      "Episode 694, Average Reward: -41.938\n",
      "Episode 695, Average Reward: -41.886\n",
      "Episode 696, Average Reward: -41.881\n",
      "Episode 697, Average Reward: -41.862\n",
      "Episode 698, Average Reward: -41.861\n",
      "Episode 699, Average Reward: -41.827\n",
      "Episode 700, Average Reward: -41.858\n",
      "Episode 701, Average Reward: -41.828\n",
      "Episode 702, Average Reward: -41.820\n",
      "Episode 703, Average Reward: -41.819\n",
      "Episode 704, Average Reward: -41.838\n",
      "Episode 705, Average Reward: -41.813\n",
      "Episode 706, Average Reward: -41.880\n",
      "Episode 707, Average Reward: -41.879\n",
      "Episode 708, Average Reward: -41.929\n",
      "Episode 709, Average Reward: -41.985\n",
      "Episode 710, Average Reward: -41.955\n",
      "Episode 711, Average Reward: -41.979\n",
      "Episode 712, Average Reward: -41.977\n",
      "Episode 713, Average Reward: -41.956\n",
      "Episode 714, Average Reward: -41.961\n",
      "Episode 715, Average Reward: -41.955\n",
      "Episode 716, Average Reward: -41.969\n",
      "Episode 717, Average Reward: -41.978\n",
      "Episode 718, Average Reward: -41.906\n",
      "Episode 719, Average Reward: -41.919\n",
      "Episode 720, Average Reward: -41.933\n",
      "Episode 721, Average Reward: -41.987\n",
      "Episode 722, Average Reward: -41.978\n",
      "Episode 723, Average Reward: -41.912\n",
      "Episode 724, Average Reward: -41.866\n",
      "Episode 725, Average Reward: -41.908\n",
      "Episode 726, Average Reward: -41.902\n",
      "Episode 727, Average Reward: -41.900\n",
      "Episode 728, Average Reward: -41.899\n",
      "Episode 729, Average Reward: -41.903\n",
      "Episode 730, Average Reward: -41.909\n",
      "Episode 731, Average Reward: -41.969\n",
      "Episode 732, Average Reward: -41.950\n",
      "Episode 733, Average Reward: -41.937\n",
      "Episode 734, Average Reward: -41.949\n",
      "Episode 735, Average Reward: -41.978\n",
      "Episode 736, Average Reward: -41.909\n",
      "Episode 737, Average Reward: -41.983\n",
      "Episode 738, Average Reward: -42.044\n",
      "Episode 739, Average Reward: -41.997\n",
      "Episode 740, Average Reward: -41.942\n",
      "Episode 741, Average Reward: -41.945\n",
      "Episode 742, Average Reward: -41.980\n",
      "Episode 743, Average Reward: -41.934\n",
      "Episode 744, Average Reward: -41.962\n",
      "Episode 745, Average Reward: -41.994\n",
      "Episode 746, Average Reward: -41.994\n",
      "Episode 747, Average Reward: -42.009\n",
      "Episode 748, Average Reward: -42.005\n",
      "Episode 749, Average Reward: -41.981\n",
      "Episode 750, Average Reward: -41.978\n",
      "Episode 751, Average Reward: -42.014\n",
      "Episode 752, Average Reward: -42.081\n",
      "Episode 753, Average Reward: -42.082\n",
      "Episode 754, Average Reward: -42.083\n",
      "Episode 755, Average Reward: -42.062\n",
      "Episode 756, Average Reward: -42.022\n",
      "Episode 757, Average Reward: -41.999\n",
      "Episode 758, Average Reward: -41.964\n",
      "Episode 759, Average Reward: -41.908\n",
      "Episode 760, Average Reward: -41.952\n",
      "Episode 761, Average Reward: -41.977\n",
      "Episode 762, Average Reward: -41.981\n",
      "Episode 763, Average Reward: -42.013\n",
      "Episode 764, Average Reward: -41.974\n",
      "Episode 765, Average Reward: -42.022\n",
      "Episode 766, Average Reward: -42.022\n",
      "Episode 767, Average Reward: -41.986\n",
      "Episode 768, Average Reward: -42.020\n",
      "Episode 769, Average Reward: -42.021\n",
      "Episode 770, Average Reward: -41.937\n",
      "Episode 771, Average Reward: -41.927\n",
      "Episode 772, Average Reward: -41.951\n",
      "Episode 773, Average Reward: -41.990\n",
      "Episode 774, Average Reward: -42.034\n",
      "Episode 775, Average Reward: -42.007\n",
      "Episode 776, Average Reward: -42.022\n",
      "Episode 777, Average Reward: -41.986\n",
      "Episode 778, Average Reward: -42.023\n",
      "Episode 779, Average Reward: -42.079\n",
      "Episode 780, Average Reward: -42.066\n",
      "Episode 781, Average Reward: -42.055\n",
      "Episode 782, Average Reward: -42.078\n",
      "Episode 783, Average Reward: -42.058\n",
      "Episode 784, Average Reward: -42.021\n",
      "Episode 785, Average Reward: -42.010\n",
      "Episode 786, Average Reward: -42.062\n",
      "Episode 787, Average Reward: -42.017\n",
      "Episode 788, Average Reward: -42.026\n",
      "Episode 789, Average Reward: -42.071\n",
      "Episode 790, Average Reward: -42.104\n",
      "Episode 791, Average Reward: -42.070\n",
      "Episode 792, Average Reward: -42.075\n",
      "Episode 793, Average Reward: -42.116\n",
      "Episode 794, Average Reward: -42.084\n",
      "Episode 795, Average Reward: -42.092\n",
      "Episode 796, Average Reward: -42.092\n",
      "Episode 797, Average Reward: -42.073\n",
      "Episode 798, Average Reward: -42.025\n",
      "Episode 799, Average Reward: -42.033\n",
      "Episode 800, Average Reward: -42.001\n",
      "Episode 801, Average Reward: -41.995\n",
      "Episode 802, Average Reward: -41.949\n",
      "Episode 803, Average Reward: -41.965\n",
      "Episode 804, Average Reward: -41.941\n",
      "Episode 805, Average Reward: -41.966\n",
      "Episode 806, Average Reward: -42.010\n",
      "Episode 807, Average Reward: -42.040\n",
      "Episode 808, Average Reward: -42.078\n",
      "Episode 809, Average Reward: -42.111\n",
      "Episode 810, Average Reward: -42.103\n",
      "Episode 811, Average Reward: -42.069\n",
      "Episode 812, Average Reward: -42.066\n",
      "Episode 813, Average Reward: -42.008\n",
      "Episode 814, Average Reward: -42.008\n",
      "Episode 815, Average Reward: -41.994\n",
      "Episode 816, Average Reward: -41.937\n",
      "Episode 817, Average Reward: -41.926\n",
      "Episode 818, Average Reward: -41.951\n",
      "Episode 819, Average Reward: -41.936\n",
      "Episode 820, Average Reward: -41.998\n",
      "Episode 821, Average Reward: -42.001\n",
      "Episode 822, Average Reward: -41.949\n",
      "Episode 823, Average Reward: -42.000\n",
      "Episode 824, Average Reward: -41.949\n",
      "Episode 825, Average Reward: -41.930\n",
      "Episode 826, Average Reward: -41.902\n",
      "Episode 827, Average Reward: -41.914\n",
      "Episode 828, Average Reward: -41.925\n",
      "Episode 829, Average Reward: -41.926\n",
      "Episode 830, Average Reward: -41.933\n",
      "Episode 831, Average Reward: -41.891\n",
      "Episode 832, Average Reward: -41.910\n",
      "Episode 833, Average Reward: -41.973\n",
      "Episode 834, Average Reward: -41.982\n",
      "Episode 835, Average Reward: -41.960\n",
      "Episode 836, Average Reward: -41.908\n",
      "Episode 837, Average Reward: -41.940\n",
      "Episode 838, Average Reward: -41.914\n",
      "Episode 839, Average Reward: -41.876\n",
      "Episode 840, Average Reward: -41.876\n",
      "Episode 841, Average Reward: -41.900\n",
      "Episode 842, Average Reward: -41.856\n",
      "Episode 843, Average Reward: -41.787\n",
      "Episode 844, Average Reward: -41.817\n",
      "Episode 845, Average Reward: -41.799\n",
      "Episode 846, Average Reward: -41.798\n",
      "Episode 847, Average Reward: -41.815\n",
      "Episode 848, Average Reward: -41.824\n",
      "Episode 849, Average Reward: -41.874\n",
      "Episode 850, Average Reward: -41.869\n",
      "Episode 851, Average Reward: -41.867\n",
      "Episode 852, Average Reward: -41.893\n",
      "Episode 853, Average Reward: -41.864\n",
      "Episode 854, Average Reward: -41.822\n",
      "Episode 855, Average Reward: -41.855\n",
      "Episode 856, Average Reward: -41.782\n",
      "Episode 857, Average Reward: -41.784\n",
      "Episode 858, Average Reward: -41.722\n",
      "Episode 859, Average Reward: -41.702\n",
      "Episode 860, Average Reward: -41.694\n",
      "Episode 861, Average Reward: -41.727\n",
      "Episode 862, Average Reward: -41.714\n",
      "Episode 863, Average Reward: -41.774\n",
      "Episode 864, Average Reward: -41.747\n",
      "Episode 865, Average Reward: -41.747\n",
      "Episode 866, Average Reward: -41.798\n",
      "Episode 867, Average Reward: -41.858\n",
      "Episode 868, Average Reward: -41.833\n",
      "Episode 869, Average Reward: -41.821\n",
      "Episode 870, Average Reward: -41.770\n",
      "Episode 871, Average Reward: -41.751\n",
      "Episode 872, Average Reward: -41.805\n",
      "Episode 873, Average Reward: -41.802\n",
      "Episode 874, Average Reward: -41.815\n",
      "Episode 875, Average Reward: -41.872\n",
      "Episode 876, Average Reward: -41.822\n",
      "Episode 877, Average Reward: -41.853\n",
      "Episode 878, Average Reward: -41.858\n",
      "Episode 879, Average Reward: -41.856\n",
      "Episode 880, Average Reward: -41.835\n",
      "Episode 881, Average Reward: -41.884\n",
      "Episode 882, Average Reward: -41.806\n",
      "Episode 883, Average Reward: -41.783\n",
      "Episode 884, Average Reward: -41.792\n",
      "Episode 885, Average Reward: -41.823\n",
      "Episode 886, Average Reward: -41.838\n",
      "Episode 887, Average Reward: -41.820\n",
      "Episode 888, Average Reward: -41.844\n",
      "Episode 889, Average Reward: -41.835\n",
      "Episode 890, Average Reward: -41.863\n",
      "Episode 891, Average Reward: -41.863\n",
      "Episode 892, Average Reward: -41.907\n",
      "Episode 893, Average Reward: -41.941\n",
      "Episode 894, Average Reward: -41.927\n",
      "Episode 895, Average Reward: -41.954\n",
      "Episode 896, Average Reward: -41.950\n",
      "Episode 897, Average Reward: -41.955\n",
      "Episode 898, Average Reward: -41.953\n",
      "Episode 899, Average Reward: -41.907\n",
      "Episode 900, Average Reward: -41.931\n",
      "Episode 901, Average Reward: -41.946\n",
      "Episode 902, Average Reward: -41.893\n",
      "Episode 903, Average Reward: -41.907\n",
      "Episode 904, Average Reward: -41.903\n",
      "Episode 905, Average Reward: -41.879\n",
      "Episode 906, Average Reward: -41.923\n",
      "Episode 907, Average Reward: -41.901\n",
      "Episode 908, Average Reward: -41.960\n",
      "Episode 909, Average Reward: -42.005\n",
      "Episode 910, Average Reward: -41.953\n",
      "Episode 911, Average Reward: -41.962\n",
      "Episode 912, Average Reward: -41.902\n",
      "Episode 913, Average Reward: -41.901\n",
      "Episode 914, Average Reward: -41.925\n",
      "Episode 915, Average Reward: -41.916\n",
      "Episode 916, Average Reward: -41.890\n",
      "Episode 917, Average Reward: -41.870\n",
      "Episode 918, Average Reward: -41.879\n",
      "Episode 919, Average Reward: -41.890\n",
      "Episode 920, Average Reward: -41.898\n",
      "Episode 921, Average Reward: -41.895\n",
      "Episode 922, Average Reward: -41.887\n",
      "Episode 923, Average Reward: -41.894\n",
      "Episode 924, Average Reward: -41.948\n",
      "Episode 925, Average Reward: -41.887\n",
      "Episode 926, Average Reward: -41.916\n",
      "Episode 927, Average Reward: -41.946\n",
      "Episode 928, Average Reward: -41.900\n",
      "Episode 929, Average Reward: -41.883\n",
      "Episode 930, Average Reward: -41.893\n",
      "Episode 931, Average Reward: -41.901\n",
      "Episode 932, Average Reward: -41.923\n",
      "Episode 933, Average Reward: -41.928\n",
      "Episode 934, Average Reward: -41.938\n",
      "Episode 935, Average Reward: -41.952\n",
      "Episode 936, Average Reward: -41.982\n",
      "Episode 937, Average Reward: -41.957\n",
      "Episode 938, Average Reward: -41.907\n",
      "Episode 939, Average Reward: -41.930\n",
      "Episode 940, Average Reward: -41.886\n",
      "Episode 941, Average Reward: -41.886\n",
      "Episode 942, Average Reward: -41.839\n",
      "Episode 943, Average Reward: -41.875\n",
      "Episode 944, Average Reward: -41.829\n",
      "Episode 945, Average Reward: -41.758\n",
      "Episode 946, Average Reward: -41.731\n",
      "Episode 947, Average Reward: -41.734\n",
      "Episode 948, Average Reward: -41.771\n",
      "Episode 949, Average Reward: -41.775\n",
      "Episode 950, Average Reward: -41.782\n",
      "Episode 951, Average Reward: -41.752\n",
      "Episode 952, Average Reward: -41.815\n",
      "Episode 953, Average Reward: -41.816\n",
      "Episode 954, Average Reward: -41.881\n",
      "Episode 955, Average Reward: -41.866\n",
      "Episode 956, Average Reward: -41.887\n",
      "Episode 957, Average Reward: -41.898\n",
      "Episode 958, Average Reward: -41.891\n",
      "Episode 959, Average Reward: -41.891\n",
      "Episode 960, Average Reward: -41.934\n",
      "Episode 961, Average Reward: -41.929\n",
      "Episode 962, Average Reward: -41.986\n",
      "Episode 963, Average Reward: -41.981\n",
      "Episode 964, Average Reward: -42.011\n",
      "Episode 965, Average Reward: -41.997\n",
      "Episode 966, Average Reward: -42.047\n",
      "Episode 967, Average Reward: -42.063\n",
      "Episode 968, Average Reward: -42.066\n",
      "Episode 969, Average Reward: -42.074\n",
      "Episode 970, Average Reward: -42.128\n",
      "Episode 971, Average Reward: -42.104\n",
      "Episode 972, Average Reward: -42.108\n",
      "Episode 973, Average Reward: -42.085\n",
      "Episode 974, Average Reward: -42.035\n",
      "Episode 975, Average Reward: -42.077\n",
      "Episode 976, Average Reward: -42.099\n",
      "Episode 977, Average Reward: -42.059\n",
      "Episode 978, Average Reward: -42.102\n",
      "Episode 979, Average Reward: -42.096\n",
      "Episode 980, Average Reward: -42.094\n",
      "Episode 981, Average Reward: -42.106\n",
      "Episode 982, Average Reward: -42.120\n",
      "Episode 983, Average Reward: -42.151\n",
      "Episode 984, Average Reward: -42.148\n",
      "Episode 985, Average Reward: -42.123\n",
      "Episode 986, Average Reward: -42.146\n",
      "Episode 987, Average Reward: -42.188\n",
      "Episode 988, Average Reward: -42.245\n",
      "Episode 989, Average Reward: -42.260\n",
      "Episode 990, Average Reward: -42.265\n",
      "Episode 991, Average Reward: -42.257\n",
      "Episode 992, Average Reward: -42.264\n",
      "Episode 993, Average Reward: -42.221\n",
      "Episode 994, Average Reward: -42.272\n",
      "Episode 995, Average Reward: -42.323\n",
      "Episode 996, Average Reward: -42.312\n",
      "Episode 997, Average Reward: -42.248\n",
      "Episode 998, Average Reward: -42.198\n",
      "Episode 999, Average Reward: -42.207\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the Actor-Critic model that outputs continuous actions.\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.common = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Actor output for continuous actions; consider using Tanh to ensure the output is in [-1,1]\n",
    "        self.actor = nn.Linear(hidden_size, num_actions)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.common(x)\n",
    "        # Tanh activation for the actor ensures outputs in a fixed range;\n",
    "        # you might later rescale to match the action space of the environment\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "# Create the materials environment\n",
    "env = MaterialsEnv()\n",
    "num_inputs = env.observation_space.shape[0]  # 3\n",
    "num_actions = env.action_space.shape[0]        # 3\n",
    "hidden_size = 128\n",
    "\n",
    "model = ActorCritic(num_inputs, num_actions, hidden_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Use a small epsilon value for numerical stability.\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "def run_materials_episode(model, max_steps):\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "    log_probs, values, rewards = [], [], []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        logits, value = model(state)\n",
    "        # Use tanh to scale outputs to [-1,1], then rescale to the environment's action space [-0.1, 0.1]\n",
    "        action_raw = torch.tanh(logits)\n",
    "        # Rescale the action by 0.1 for our environment.\n",
    "        action = action_raw * 0.1\n",
    "\n",
    "        # Compute a simple log probability (this might need adjustments for continuous actions)\n",
    "        log_prob = torch.log(torch.abs(action) + 1e-8).sum()\n",
    "        log_probs.append(log_prob.unsqueeze(0))\n",
    "        values.append(value)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "        rewards.append(reward)\n",
    "        state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return log_probs, values, rewards\n",
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    discounted_sum = 0\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        returns.insert(0, discounted_sum)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    return (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "def compute_loss(log_probs, values, returns):\n",
    "    advantage = returns - torch.cat(values)\n",
    "    actor_loss = -torch.sum(torch.cat(log_probs) * advantage.detach())\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "    return actor_loss + critic_loss\n",
    "\n",
    "def train_step(model, optimizer, gamma=0.99, max_steps=50):\n",
    "    log_probs, values, rewards = run_materials_episode(model, max_steps)\n",
    "    returns = compute_returns(rewards, gamma)\n",
    "\n",
    "    loss = compute_loss(log_probs, values, returns)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return sum(rewards)\n",
    "\n",
    "# Training loop for the materials environment\n",
    "num_episodes = 1000\n",
    "reward_threshold = -0.2  # set an appropriate reward threshold based on your reward function\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    episode_reward = train_step(model, optimizer)\n",
    "    reward_list.append(episode_reward)\n",
    "    avg_reward = np.mean(reward_list[-50:])\n",
    "    print(f\"Episode {episode}, Average Reward: {avg_reward:.3f}\")\n",
    "    if avg_reward > reward_threshold:\n",
    "        print(f\"Solved materials design task at episode {episode}!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discussion and Further Extensions\n",
    "\n",
    "In real-world materials design applications, you may need to:\n",
    "\n",
    "Design a more realistic reward function: Instead of a simple distance metric, incorporate material properties predictions or experimental data.\n",
    "\n",
    "Use continuous action distributions: For better exploration, replace the simple tanh output with a parameterized Gaussian distribution.\n",
    "\n",
    "Combine with high-throughput experiments or simulations: Integrate simulation outputs to dynamically update the environment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
