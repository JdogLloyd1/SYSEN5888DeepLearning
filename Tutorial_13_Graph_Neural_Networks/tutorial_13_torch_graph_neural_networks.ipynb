{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "Graph Neural Networks (GNNs) are a class of neural networks designed to work directly with graph-structured data. GNNs have gained significant attention for their ability to model relationships and interactions between entities in fields such as social networks, molecular biology, recommendation systems, and more.\n",
    "\n",
    "## What is a Graph?\n",
    "\n",
    "A graph \\( G \\) consists of:\n",
    "- **Nodes (Vertices)**: Represent entities in the data.  \n",
    "- **Edges**: Represent relationships or connections between nodes.  \n",
    "\n",
    "Graphs can be:\n",
    "- **Undirected or Directed**: Depending on whether edges have a direction.  \n",
    "- **Weighted or Unweighted**: Edges may carry weights representing the strength of relationships.  \n",
    "- **Homogeneous or Heterogeneous**: Graphs may have one type of node/edge or multiple types.\n",
    "\n",
    "## Why Use GNNs?\n",
    "\n",
    "Unlike traditional neural networks, GNNs can capture the intricate dependencies and structure of graph data. They enable:\n",
    "1. **Node-Level Tasks**: Node classification, node regression (e.g., predicting a node's category).  \n",
    "2. **Edge-Level Tasks**: Link prediction, edge classification (e.g., predicting relationships between nodes).  \n",
    "3. **Graph-Level Tasks**: Graph classification or regression (e.g., predicting properties of molecules).\n",
    "\n",
    "## How Do GNNs Work?\n",
    "\n",
    "GNNs operate by iteratively passing, aggregating, and transforming information between nodes through their neighbors. This process can be summarized as:\n",
    "\n",
    "1. **Message Passing**:\n",
    "   Each node aggregates messages (features) from its neighbors using a defined aggregation function (e.g., sum, mean, max).\n",
    "\n",
    "2. **Update Function**:\n",
    "   The node updates its representation based on the aggregated messages.\n",
    "\n",
    "3. **Propagation**:\n",
    "   These steps are repeated for a fixed number of layers, enabling information to flow across the graph.\n",
    "\n",
    "### General GNN Framework:\n",
    "Given a graph \\( G = (V, E) \\):\n",
    "- \\( h_v^{(k)} \\): Node \\( v \\)'s representation at the \\( k \\)-th layer.  \n",
    "- \\( \\mathcal{N}(v) \\): Set of neighbors of node \\( v \\).  \n",
    "\n",
    "The update rule for a GNN can be expressed as:  \n",
    "\\[ \n",
    "h_v^{(k+1)} = \\text{UPDATE}\\left(h_v^{(k)}, \\text{AGGREGATE}\\left(\\{h_u^{(k)} : u \\in \\mathcal{N}(v)\\}\\right)\\right)\n",
    "\\]  \n",
    "\n",
    "Where:\n",
    "- **AGGREGATE**: Combines features from neighbors.  \n",
    "- **UPDATE**: Updates the node's feature based on the aggregated information.\n",
    "\n",
    "## Popular Variants of GNNs\n",
    "\n",
    "1. **Graph Convolutional Networks (GCNs)**:\n",
    "   Perform convolution operations on graphs, generalizing the idea of CNNs to graph data.  \n",
    "\n",
    "2. **Graph Attention Networks (GATs)**:\n",
    "   Use attention mechanisms to weigh the importance of neighbors during aggregation.  \n",
    "\n",
    "3. **GraphSAGE**:\n",
    "   Samples and aggregates features from a fixed-size neighborhood to handle large-scale graphs.  \n",
    "\n",
    "4. **Message Passing Neural Networks (MPNNs)**:\n",
    "   Generalize message-passing frameworks for learning on graphs.\n",
    "\n",
    "5. **Graph Isomorphism Networks (GINs)**:\n",
    "   Focus on learning node embeddings that are maximally expressive for graph classification tasks.\n",
    "\n",
    "## Applications of GNNs\n",
    "\n",
    "1. **Social Networks**:\n",
    "   Predicting user connections or interests.  \n",
    "2. **Molecular Biology**:\n",
    "   Modeling molecular structures for drug discovery or material science.  \n",
    "3. **Recommendation Systems**:\n",
    "   Learning item-user relationships for personalized recommendations.  \n",
    "4. **Traffic Networks**:\n",
    "   Predicting traffic flow or congestion.  \n",
    "5. **Knowledge Graphs**:\n",
    "   Inferring missing links or entity attributes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pytorch Geometric Framework\n",
    "- Understanding Message Passing Scheme in Pytorch Geometric.\n",
    "- Efficient graph data representations and paralleling minibatching graphs.\n",
    "- Showcase the implementation of **Graph Convolution Networks** (Kipf & Welling, [SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS](https://arxiv.org/abs/1609.02907), ICLR 2017), and you should implement **GraphSAGE** (Hamilton et al, [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216), NIPS 2017) in the lab based on message passing scheme.\n",
    "\n",
    "#### 2. Vertex Classification\n",
    "- Showcase a model developed based on our GCN implementation to do vertex classification on Cora dataset. \n",
    "- Develop a model with **your own** GraphSAGE (with mean/sum/max aggregation) implementation on the same dataset to get insights of difference.\n",
    "\n",
    "#### 3. Graph Classification\n",
    "- Implement **GINConv** (Xu et al, [HOW POWERFUL ARE GRAPH NEURAL NETWORKS?](https://arxiv.org/abs/1810.00826), ICLR 2019) on graph classification benchmark dataset (IMDB) and compare different aggregation functions (SUM/MEAN/MAX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up working environment\n",
    "\n",
    "For this tutorial you will need to train a large network, therefore we recommend you work with Google Colaboratory, which provides free GPU time. You will need a Google account to do so. Please log in to your account and go to the following page: https://colab.research.google.com. Then upload this notebook.For GPU support, go to \"Edit\" -> \"Notebook Settings\", and select \"Hardware accelerator\" as \"GPU\".You will need to install pytorch by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch==1.13.1+cu117 in /opt/conda/envs/torch/lib/python3.10/site-packages (1.13.1+cu117)\n",
      "Requirement already satisfied: torchvision==0.14.1+cu117 in /opt/conda/envs/torch/lib/python3.10/site-packages (0.14.1+cu117)\n",
      "Requirement already satisfied: torchaudio==0.13.1 in /opt/conda/envs/torch/lib/python3.10/site-packages (0.13.1+cu117)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch==1.13.1+cu117) (4.13.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/torch/lib/python3.10/site-packages (from torchvision==0.14.1+cu117) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/torch/lib/python3.10/site-packages (from torchvision==0.14.1+cu117) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from torchvision==0.14.1+cu117) (11.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/torch/lib/python3.10/site-packages (from requests->torchvision==0.14.1+cu117) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/torch/lib/python3.10/site-packages (from requests->torchvision==0.14.1+cu117) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/torch/lib/python3.10/site-packages (from requests->torchvision==0.14.1+cu117) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/torch/lib/python3.10/site-packages (from requests->torchvision==0.14.1+cu117) (2025.1.31)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
      "Requirement already satisfied: torch-scatter in /opt/conda/envs/torch/lib/python3.10/site-packages (2.1.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
      "Requirement already satisfied: torch-sparse in /opt/conda/envs/torch/lib/python3.10/site-packages (0.6.18)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch-sparse) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/conda/envs/torch/lib/python3.10/site-packages (from scipy->torch-sparse) (1.26.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
      "Requirement already satisfied: torch-cluster in /opt/conda/envs/torch/lib/python3.10/site-packages (1.6.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/torch/lib/python3.10/site-packages (from torch-cluster) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/conda/envs/torch/lib/python3.10/site-packages (from scipy->torch-cluster) (1.26.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
      "Requirement already satisfied: torch-spline-conv in /opt/conda/envs/torch/lib/python3.10/site-packages (1.2.2+pt113cu117)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/envs/torch/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/envs/torch/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: threadpoolctl, joblib, scikit-learn\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/opt/conda/envs/torch/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
    "!pip install torch-cluster -f  https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
    "!pip install torch-spline-conv -f  https://data.pyg.org/whl/torch-1.13.1+cu117.html\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Geometric Framework\n",
    "\n",
    "#### Generic Message Passing Scheme\n",
    "Generalizing the convolution operator to irregular domains is typically expressed as a *neighborhood aggregation* or *message passing* scheme.\n",
    "With $\\mathbf{x}^{(k-1)}_i \\in \\mathbb{R}^F$ denoting node features of node $i$ in layer $(k-1)$ and $\\mathbf{e}_{i,j} \\in \\mathbb{R}^D$ denoting (optional) edge features from node $i$ to node $j$, message passing graph neural networks can be described as\n",
    "\n",
    "$$\n",
    "  \\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{i,j}\\right) \\right)\n",
    "$$\n",
    "\n",
    "where $\\square$ denotes a differentiable, permutation invariant function, *e.g.*, sum, mean or max, and $\\gamma$ and $\\phi$ denote differentiable functions such as MLPs (Multi Layer Perceptrons).\n",
    "\n",
    "#### Graph data representations in PyG\n",
    "Given a *sparse* **Graph** $\\mathcal{G}=(\\mathbf{X}, (\\mathbf{I}, \\mathbf{E}))$ with **node features** $\\mathbf{X} \\in \\mathbb{R}^{|V| \\times F}$, **edge indices $\\mathbf{I} \\in \\{1, \\cdots, N\\}^{2 \\times |\\mathcal{E}|}$**, (optional) **edge features** $\\mathbf{E} \\in \\mathbb{R}^{|\\mathcal{E} \\times D|}$, it is described by an instance of class `torch_geometric.data.Data`, which holds the corresponding attributes.\n",
    "\n",
    "We show a simple example of an unweighted and directed graph with four nodes and three edges.\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"figures/graph_data.png\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 1], edge_index=[2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[2, 1, 3],\n",
    "                           [0, 0, 2]], dtype=torch.long)\n",
    "x = torch.tensor([[1], [1], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batching Graphs\n",
    "Neural networks are usually trained in a batch-wise fashion. Minibatch graphs can be efficiently dealt with to achieve parallelization over a mini-batch from creating sparse block diagnoal adjacency matrices and concatenating features and target matrices in the node dimension.\n",
    "\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"figures/mini_batch_graph.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Message Passing Scheme in PyG\n",
    "\n",
    "PyTorch Geometric provides the `torch_geometric.nn.MessagePassing` base class, which helps in creating such kinds of message passing graph neural networks by automatically taking care of message propagation. The implementation is decoupled into **UPDATE**, **AGGREGATION**, **MESSAGE** functions as:\n",
    "$$\n",
    "    \\mathbf{x}_i^{(k)} = \\mathrm{UPDATE} \\left( \\mathbf{x}_i, , \\mathrm{AGGR}_{j \\in \\mathcal{N}(i)} \\, \\mathrm{MESSAGE}^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{i,j}\\right) \\right)    \n",
    "$$\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"figures/message_passing.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the GCN layer (lecture)\n",
    "\n",
    "The graph convolutional operator introduced by Kipf & Welling (ICLR 2017) is defined as\n",
    "$$\n",
    "        \\mathbf{X}^{k} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X}^{k-1} \\mathbf{\\Theta},\n",
    "$$\n",
    "where $\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}$ denotes the adjacency matrix with inserted self-loops and\n",
    "$\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}$ its diagonal degree matrix. It is equivalent as:\n",
    "$$\n",
    "\\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{deg(j)}} \\cdot \\left( \\mathbf{x}_j^{(k-1)}\\mathbf{\\Theta} \\right),\n",
    "$$\n",
    "\n",
    "where neighboring node features are first transformed by a weight matrix $\\mathbf{\\Theta}$, normalized by their degree, and finally summed up.\n",
    "This formula can be divided into the following steps:\n",
    "\n",
    "1. Add self-loops to the adjacency matrix.\n",
    "2. Linearly transform node feature matrix.\n",
    "3. Normalize node features.\n",
    "4. Sum up neighboring node features.\n",
    "5. Return new node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import math\n",
    "\n",
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "        \n",
    "def add_self_loops(edge_index, num_nodes=None):\n",
    "    loop_index = torch.arange(0, num_nodes, dtype=torch.long,\n",
    "                              device=edge_index.device)\n",
    "    loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
    "\n",
    "    edge_index = torch.cat([edge_index, loop_index], dim=1)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def degree(index, num_nodes=None, dtype=None):\n",
    "    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)\n",
    "    return out.scatter_add_(0, index, out.new_ones((index.size(0))))\n",
    "        \n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        zeros(self.lin.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        \n",
    "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3-5: Start propagating messages.\n",
    "\n",
    "        return self.propagate(edge_index, x=x)\n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        # Step 3: Normalize node features.\n",
    "        row, col = edge_index\n",
    "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return norm.view(-1, 1) * x_j        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################              \n",
    "        \n",
    "\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        # Step 5: Return new node embeddings.\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing GraphSAGE (lab)\n",
    "\n",
    "The algorithm of GraphSAGE (*Inductive Representation Learning on Large Graphs (NIPS 2017)*) embedding generation is described as:\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"figures/graphsage.png\"></p>\n",
    "\n",
    "You are required to implement this algortihm with **MEAN/SUM/MAX** AGGREGATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    bound = 1.0 / math.sqrt(size)\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "class SAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, aggr):\n",
    "        super(SAGEConv, self).__init__(aggr=aggr)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(2 * in_channels, out_channels))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        uniform(self.weight.size(0), self.weight)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        return self.propagate(edge_index, x=x)        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_weight):\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        return x_j        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "        \n",
    "\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        \n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        aggr_out = torch.cat([x, aggr_out], dim=-1)\n",
    "        aggr_out = torch.matmul(aggr_out, self.weight)\n",
    "        aggr_out = F.normalize(aggr_out, p=2, dim=-1)\n",
    "\n",
    "        return aggr_out        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "path = osp.join(os.getcwd(), 'data', 'Cora')\n",
    "dataset = Planetoid(path, 'Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "\n",
    "def run(dataset, model, runs, epochs, lr, weight_decay, early_stopping):\n",
    "\n",
    "    val_losses, accs, durations = [], [], []\n",
    "    for _ in range(runs):\n",
    "        data = dataset[0]\n",
    "        data = data.to(device)\n",
    "\n",
    "        model.to(device).reset_parameters()\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_start = time.perf_counter()\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        test_acc = 0\n",
    "        val_loss_history = []\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, optimizer, data)\n",
    "            eval_info = evaluate(model, data)\n",
    "            eval_info['epoch'] = epoch\n",
    "\n",
    "            if eval_info['val_loss'] < best_val_loss:\n",
    "                best_val_loss = eval_info['val_loss']\n",
    "                test_acc = eval_info['test_acc']\n",
    "\n",
    "            val_loss_history.append(eval_info['val_loss'])\n",
    "            if early_stopping > 0 and epoch > epochs // 2:\n",
    "                tmp = tensor(val_loss_history[-(early_stopping + 1):-1])\n",
    "                if eval_info['val_loss'] > tmp.mean().item():\n",
    "                    break\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_end = time.perf_counter()\n",
    "\n",
    "        val_losses.append(best_val_loss)\n",
    "        accs.append(test_acc)\n",
    "        durations.append(t_end - t_start)\n",
    "\n",
    "    loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
    "\n",
    "    print('Val Loss: {:.4f}, Test Accuracy: {:.3f} ± {:.3f}, Duration: {:.3f}'.\n",
    "          format(loss.mean().item(),\n",
    "                 acc.mean().item(),\n",
    "                 acc.std().item(),\n",
    "                 duration.mean().item()))\n",
    "\n",
    "\n",
    "def train(model, optimizer, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(data)\n",
    "\n",
    "    outs = {}\n",
    "    for key in ['train', 'val', 'test']:\n",
    "        mask = data['{}_mask'.format(key)]\n",
    "        loss = F.nll_loss(logits[mask], data.y[mask]).item()\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "\n",
    "        outs['{}_loss'.format(key)] = loss\n",
    "        outs['{}_acc'.format(key)] = acc\n",
    "\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model with GCN on vertex classification (lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7421, Test Accuracy: 0.800 ± 0.009, Duration: 0.360\n"
     ]
    }
   ],
   "source": [
    "runs = 10\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 0.0005\n",
    "early_stopping = 10\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden)\n",
    "        self.conv2 = GCNConv(hidden, dataset.num_classes)\n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "\n",
    "    \n",
    "run(dataset, Net(dataset), runs, epochs, lr, weight_decay,\n",
    "    early_stopping)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build models with GraphSAGE on vertex classification (lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSAGE-mean\n",
      "Val Loss: 1.3722, Test Accuracy: 0.767 ± 0.021, Duration: 0.342\n",
      "GraphSAGE-add\n",
      "Val Loss: 1.3478, Test Accuracy: 0.778 ± 0.013, Duration: 0.341\n",
      "GraphSAGE-max\n",
      "Val Loss: 1.3860, Test Accuracy: 0.748 ± 0.032, Duration: 0.335\n"
     ]
    }
   ],
   "source": [
    "## define your own model\n",
    "\n",
    "class SAGENet(torch.nn.Module):\n",
    "    def __init__(self, dataset, aggr='mean'):\n",
    "        super(SAGENet, self).__init__()\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        self.conv1 = SAGEConv(dataset.num_features, hidden, aggr=aggr)\n",
    "        self.conv2 = SAGEConv(hidden, dataset.num_classes, aggr=aggr)        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "        \n",
    "\n",
    "    \n",
    "aggrs = ['mean', 'add', 'max']    \n",
    "\n",
    "for aggr in aggrs:\n",
    "    print('GraphSAGE-{}'.format(aggr))\n",
    "    run(dataset, SAGENet(dataset, aggr), runs, epochs, lr, weight_decay,\n",
    "        early_stopping)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Classification\n",
    "\n",
    "While Graph Convolutional Networks (GCN) and GraphSAGE show extraordinary performance on transductive learning and inductive learning problems resepectively, then cannot learn to distinguish certain simple graph structures. **Graph Isomorphism Network (GIN)** is the state-of-the-art graph neural networks, which is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. \n",
    "\n",
    "(Theorem 3, How powerful are graph neural networks?) *Let $\\mathcal{A} : \\mathcal{G} → \\mathbb{R}^d$ be a GNN. With a sufficient number of GNN layers, A maps any graphs $G_1$ and $G_2$ that the Weisfeiler-Lehman test of isomorphism decides as non-isomorphic, to different embeddings if the following conditions hold:*\n",
    "\n",
    "- a) *$\\mathcal{A}$ aggregates and updates node features iteratively with*\n",
    "$$\n",
    "    h{_v}^{(k)} = \\phi \\left( h_v^{(k-1)}, f(\\{ h_u^{(k-1)}: u\\in\\mathcal{N}(v) \\}) \\right)\n",
    "$$\n",
    "*where the functions $f$, which operates on multisets, and $\\phi$ are injective.*\n",
    "- b) *$\\mathcal{A}$'s graph-level readout, which operates on the multiset ofnode features $\\{ h_v^{(k)} \\}$ , is injective.*\n",
    "\n",
    "**Graph Isomorphism Network (GIN)**, that provably satisfies the conditions in Theorem 3, is defined as:\n",
    "$$\n",
    "\\mathbf{x}^{\\prime}_i = h_{\\mathbf{\\Theta}} \\left( (1 + \\epsilon) \\cdot\n",
    "        \\mathbf{x}_i + \\mathrm{AGGR}_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j \\right)\n",
    "$$\n",
    "$h_{\\mathbf{\\Theta}}$ denotes a neural network, *.i.e.* a MLP, and AGGR explicitly denotes SUM because of **higher expressive power (than MEAN/MAX)**.\n",
    "\n",
    "You should implement **GIN-0/GIn-$\\epsilon$** with **SUM/MEAN/MAX Aggregation functions** and use **MEAN Readout function** in the end of the network to obtain **graph-level representations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import degree\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "\n",
    "class NormalizedDegree(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, data):\n",
    "        deg = degree(data.edge_index[0], dtype=torch.float)\n",
    "        deg = (deg - self.mean) / self.std\n",
    "        data.x = deg.view(-1, 1)\n",
    "        return data\n",
    "\n",
    "\n",
    "def get_dataset(name, cleaned=False):\n",
    "    path = osp.join(os.getcwd(), 'data', name)\n",
    "    dataset = TUDataset(path, name, cleaned=cleaned)\n",
    "    dataset.data.edge_attr = None\n",
    "\n",
    "    if dataset.data.x is None:\n",
    "        max_degree = 0\n",
    "        degs = []\n",
    "        for data in dataset:\n",
    "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
    "            max_degree = max(max_degree, degs[-1].max().item())\n",
    "\n",
    "        if max_degree < 1000:\n",
    "            dataset.transform = T.OneHotDegree(max_degree)\n",
    "        else:\n",
    "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
    "            mean, std = deg.mean().item(), deg.std().item()\n",
    "            dataset.transform = NormalizedDegree(mean, std)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name IMDB-BINARY(1000)\n",
      "Graphs 1000\n",
      "Nodes 19.773\n",
      "Edges 96.531\n",
      "Features 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_dataset(dataset):\n",
    "    num_nodes = num_edges = 0\n",
    "    for data in dataset:\n",
    "        num_nodes += data.num_nodes\n",
    "        num_edges += data.num_edges\n",
    "\n",
    "    print('Name', dataset)\n",
    "    print('Graphs', len(dataset))\n",
    "    print('Nodes', num_nodes / len(dataset))\n",
    "    print('Edges', (num_edges // 2) / len(dataset))\n",
    "    print('Features', dataset.num_features)\n",
    "    print('Classes', dataset.num_classes)\n",
    "    print()\n",
    "\n",
    "\n",
    "for name in ['IMDB-BINARY']:\n",
    "    print_dataset(get_dataset(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
    "\n",
    "\n",
    "def cross_validation_with_val_set(dataset, model, folds, epochs, batch_size,\n",
    "                                  lr, lr_decay_factor, lr_decay_step_size,\n",
    "                                  weight_decay, logger=None):\n",
    "\n",
    "    val_losses, accs, durations = [], [], []\n",
    "    for fold, (train_idx, test_idx,\n",
    "               val_idx) in enumerate(zip(*k_fold(dataset, folds))):\n",
    "\n",
    "        train_dataset = dataset[train_idx]\n",
    "        test_dataset = dataset[test_idx]\n",
    "        val_dataset = dataset[val_idx]\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size, shuffle=False)        \n",
    "\n",
    "        model.to(device).reset_parameters()\n",
    "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_start = time.perf_counter()\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train(model, optimizer, train_loader)\n",
    "            val_losses.append(eval_loss(model, val_loader))\n",
    "            accs.append(eval_acc(model, test_loader))\n",
    "            eval_info = {\n",
    "                'fold': fold,\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_losses[-1],\n",
    "                'test_acc': accs[-1],\n",
    "            }\n",
    "\n",
    "            if logger is not None:\n",
    "                logger(eval_info)\n",
    "\n",
    "            if epoch % lr_decay_step_size == 0:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_end = time.perf_counter()\n",
    "        durations.append(t_end - t_start)\n",
    "\n",
    "    loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
    "    loss, acc = loss.view(folds, epochs), acc.view(folds, epochs)\n",
    "    loss, argmin = loss.min(dim=1)\n",
    "    acc = acc[torch.arange(folds, dtype=torch.long), argmin]\n",
    "\n",
    "    loss_mean = loss.mean().item()\n",
    "    acc_mean = acc.mean().item()\n",
    "    acc_std = acc.std().item()\n",
    "    duration_mean = duration.mean().item()\n",
    "    print('Val Loss: {:.4f}, Test Accuracy: {:.3f} ± {:.3f}, Duration: {:.3f}'.\n",
    "          format(loss_mean, acc_mean, acc_std, duration_mean))\n",
    "\n",
    "    return loss_mean, acc_mean, acc_std\n",
    "\n",
    "\n",
    "def k_fold(dataset, folds):\n",
    "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
    "\n",
    "    test_indices, train_indices = [], []\n",
    "    for _, idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
    "        test_indices.append(torch.from_numpy(idx))\n",
    "\n",
    "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
    "\n",
    "    for i in range(folds):\n",
    "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
    "        train_mask[test_indices[i]] = 0\n",
    "        train_mask[val_indices[i]] = 0\n",
    "        train_indices.append(train_mask.nonzero().view(-1))\n",
    "\n",
    "    return train_indices, test_indices, val_indices\n",
    "\n",
    "\n",
    "def num_graphs(data):\n",
    "    if data.batch is not None:\n",
    "        return data.num_graphs\n",
    "    else:\n",
    "        return data.x.size(0)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs(data)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).max(1)[1]\n",
    "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_loss(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "        loss += F.nll_loss(out, data.y.view(-1), reduction='sum').item()\n",
    "    return loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Graph Isomorphism Network (lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
    "from torch_geometric.nn import global_mean_pool, MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "\n",
    "def reset(nn):\n",
    "    def _reset(item):\n",
    "        if hasattr(item, 'reset_parameters'):\n",
    "            item.reset_parameters()\n",
    "\n",
    "    if nn is not None:\n",
    "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
    "            for item in nn.children():\n",
    "                _reset(item)\n",
    "        else:\n",
    "            _reset(nn)\n",
    "\n",
    "\n",
    "class GINConv(MessagePassing):\n",
    "    def __init__(self, nn, eps=0, train_eps=False, **kwargs):\n",
    "        super(GINConv, self).__init__(aggr='add', **kwargs)\n",
    "        self.nn = nn\n",
    "        self.initial_eps = eps\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        out = self.nn((1 + self.eps) * x + self.propagate(edge_index, x=x))\n",
    "        return out        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################            \n",
    "        \n",
    "\n",
    "\n",
    "    def message(self, x_j):\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        return x_j        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement models with GIN-0/GIN-$\\epsilon$ to perform graph classificaiton on IMDB-binary dataset (lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN0(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden):\n",
    "        super(GIN0, self).__init__()\n",
    "        self.conv1 = GINConv(Sequential(\n",
    "            Linear(dataset.num_features, hidden),\n",
    "            ReLU(),\n",
    "            Linear(hidden, hidden),\n",
    "            ReLU(),\n",
    "            BN(hidden),\n",
    "        ),\n",
    "                             train_eps=False)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                GINConv(Sequential(\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    BN(hidden),\n",
    "                ),\n",
    "                        train_eps=False))\n",
    "        self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, dataset, num_layers, hidden):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Sequential(\n",
    "            Linear(dataset.num_features, hidden),\n",
    "            ReLU(),\n",
    "            Linear(hidden, hidden),\n",
    "            ReLU(),\n",
    "            BN(hidden),\n",
    "        ),\n",
    "                             train_eps=True)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(\n",
    "                GINConv(Sequential(\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    Linear(hidden, hidden),\n",
    "                    ReLU(),\n",
    "                    BN(hidden),\n",
    "                ),\n",
    "                        train_eps=True))\n",
    "        self.lin1 = Linear(hidden, hidden)\n",
    "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.lin1.reset_parameters()\n",
    "        self.lin2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        ########################################################################\n",
    "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
    "        ########################################################################\n",
    "\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "        \n",
    "        \n",
    "        ########################################################################\n",
    "        #                             END OF YOUR CODE                         #\n",
    "        ########################################################################                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "IMDB-BINARY - GIN0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4730, Test Accuracy: 0.724 ± 0.044, Duration: 11.662\n",
      "Best result - 0.724 ± 0.044\n",
      "-----\n",
      "IMDB-BINARY - GIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4716, Test Accuracy: 0.726 ± 0.042, Duration: 12.069\n",
      "Best result - 0.726 ± 0.042\n",
      "-----\n",
      "IMDB-BINARY - GIN0(\n",
      "  (conv1): GINConv()\n",
      "  (convs): ModuleList(\n",
      "    (0): GINConv()\n",
      "    (1): GINConv()\n",
      "    (2): GINConv()\n",
      "    (3): GINConv()\n",
      "  )\n",
      "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
      "): 0.724 ± 0.044\n",
      "IMDB-BINARY - GIN(\n",
      "  (conv1): GINConv()\n",
      "  (convs): ModuleList(\n",
      "    (0): GINConv()\n",
      "    (1): GINConv()\n",
      "    (2): GINConv()\n",
      "    (3): GINConv()\n",
      "  )\n",
      "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
      "): 0.726 ± 0.042\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "lr_decay_factor = 0.5\n",
    "lr_decay_step_size = 50\n",
    "\n",
    "layers = [5]\n",
    "hiddens = [64]\n",
    "datasets = ['IMDB-BINARY']\n",
    "nets = [\n",
    "    GIN0,\n",
    "    GIN,\n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def logger(info):\n",
    "    fold, epoch = info['fold'] + 1, info['epoch']\n",
    "    val_loss, test_acc = info['val_loss'], info['test_acc']\n",
    "    print('{:02d}/{:03d}: Val Loss: {:.4f}, Test Accuracy: {:.3f}'.format(\n",
    "        fold, epoch, val_loss, test_acc))\n",
    "\n",
    "\n",
    "results = []\n",
    "for dataset_name, Net in product(datasets, nets):\n",
    "    best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "    print('-----\\n{} - {}'.format(dataset_name, Net.__name__))\n",
    "    for num_layers, hidden in product(layers, hiddens):\n",
    "        dataset = get_dataset(dataset_name)\n",
    "        model = Net(dataset, num_layers, hidden)\n",
    "        loss, acc, std = cross_validation_with_val_set(\n",
    "            dataset,\n",
    "            model,\n",
    "            folds=10,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            lr=lr,\n",
    "            lr_decay_factor=lr_decay_factor,\n",
    "            lr_decay_step_size=lr_decay_step_size,\n",
    "            weight_decay=0,\n",
    "            logger=None,\n",
    "        )\n",
    "        if loss < best_result[0]:\n",
    "            best_result = (loss, acc, std)\n",
    "\n",
    "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "    print('Best result - {}'.format(desc))\n",
    "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "IMDB-BINARY - GIN0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4656, Test Accuracy: 0.714 ± 0.037, Duration: 11.843\n",
      "Best result - 0.714 ± 0.037\n",
      "-----\n",
      "IMDB-BINARY - GIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/torch/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4536, Test Accuracy: 0.721 ± 0.043, Duration: 12.055\n",
      "Best result - 0.721 ± 0.043\n",
      "-----\n",
      "IMDB-BINARY - GIN0(\n",
      "  (conv1): GINConv()\n",
      "  (convs): ModuleList(\n",
      "    (0): GINConv()\n",
      "    (1): GINConv()\n",
      "    (2): GINConv()\n",
      "    (3): GINConv()\n",
      "  )\n",
      "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
      "): 0.714 ± 0.037\n",
      "IMDB-BINARY - GIN(\n",
      "  (conv1): GINConv()\n",
      "  (convs): ModuleList(\n",
      "    (0): GINConv()\n",
      "    (1): GINConv()\n",
      "    (2): GINConv()\n",
      "    (3): GINConv()\n",
      "  )\n",
      "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
      "): 0.721 ± 0.043\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "lr_decay_factor = 0.5\n",
    "lr_decay_step_size = 50\n",
    "\n",
    "layers = [5]\n",
    "hiddens = [64]\n",
    "datasets = ['IMDB-BINARY']\n",
    "nets = [\n",
    "    GIN0,\n",
    "    GIN,\n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def logger(info):\n",
    "    fold, epoch = info['fold'] + 1, info['epoch']\n",
    "    val_loss, test_acc = info['val_loss'], info['test_acc']\n",
    "    print('{:02d}/{:03d}: Val Loss: {:.4f}, Test Accuracy: {:.3f}'.format(\n",
    "        fold, epoch, val_loss, test_acc))\n",
    "\n",
    "\n",
    "results = []\n",
    "for dataset_name, Net in product(datasets, nets):\n",
    "    best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
    "    print('-----\\n{} - {}'.format(dataset_name, Net.__name__))\n",
    "    for num_layers, hidden in product(layers, hiddens):\n",
    "        dataset = get_dataset(dataset_name)\n",
    "        model = Net(dataset, num_layers, hidden)\n",
    "        loss, acc, std = cross_validation_with_val_set(\n",
    "            dataset,\n",
    "            model,\n",
    "            folds=10,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            lr=lr,\n",
    "            lr_decay_factor=lr_decay_factor,\n",
    "            lr_decay_step_size=lr_decay_step_size,\n",
    "            weight_decay=0,\n",
    "            logger=None,\n",
    "        )\n",
    "        if loss < best_result[0]:\n",
    "            best_result = (loss, acc, std)\n",
    "\n",
    "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
    "    print('Best result - {}'.format(desc))\n",
    "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
    "print('-----\\n{}'.format('\\n'.join(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Materials: Graph Neural Networks for Material Property Prediction\n",
    "\n",
    "In material science, GNNs have been successfully applied to predict material properties (such as formation energy, band gap, or dipole moments) from their molecular or crystal structures. In this section, we demonstrate how to extend the tutorial by using the QM9 dataset as an example.\n",
    "\n",
    "## 1. Loading the QM9 Dataset\n",
    "\n",
    "We will use PyTorch Geometric's built-in QM9 dataset, which contains molecular graphs with associated quantum chemical properties. In this example, we pick one target property (e.g., dipole moment) to perform regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
      "Extracting data/QM9/raw/qm9_v3.zip\n",
      "Processing...\n",
      "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Load QM9 dataset (this may take some time on the first run)\n",
    "dataset = QM9(root='./data/QM9')\n",
    "\n",
    "# For regression, select one target property (e.g., target_index = 4 might represent dipole moment)\n",
    "target_index = 4\n",
    "\n",
    "# Shuffle and split the dataset\n",
    "torch.manual_seed(42)\n",
    "dataset = dataset.shuffle()\n",
    "train_dataset = dataset[:int(0.8 * len(dataset))]\n",
    "val_dataset = dataset[int(0.8 * len(dataset)):int(0.9 * len(dataset))]\n",
    "test_dataset = dataset[int(0.9 * len(dataset)):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining a GCN Model for Regression\n",
    "We build a simple Graph Convolutional Network (GCN) that computes node embeddings and aggregates them into a graph-level representation to predict the property value. Note that we use global mean pooling for a fixed-size output per graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, 1)  # Regression: output is one scalar value\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # First graph convolution layer + ReLU activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Second graph convolution layer + ReLU activation\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Global pooling (mean) to obtain graph-level embedding\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # Final linear layer for regression output\n",
    "        x = self.lin(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "model = GCNModel(dataset.num_features, hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation\n",
    "Here is a training loop tailored for regression. We use the Mean Squared Error (MSE) loss to evaluate the difference between the predicted and the true target property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 2.0952, Val Loss: 1.0209\n",
      "Epoch: 002, Train Loss: 0.9924, Val Loss: 0.9731\n",
      "Epoch: 003, Train Loss: 0.9563, Val Loss: 0.9482\n",
      "Epoch: 004, Train Loss: 0.9392, Val Loss: 0.9107\n",
      "Epoch: 005, Train Loss: 0.9290, Val Loss: 0.9031\n",
      "Epoch: 006, Train Loss: 0.9197, Val Loss: 0.9038\n",
      "Epoch: 007, Train Loss: 0.9121, Val Loss: 0.9042\n",
      "Epoch: 008, Train Loss: 0.9051, Val Loss: 0.9021\n",
      "Epoch: 009, Train Loss: 0.8938, Val Loss: 0.8682\n",
      "Epoch: 010, Train Loss: 0.8815, Val Loss: 0.8666\n",
      "Epoch: 011, Train Loss: 0.8683, Val Loss: 0.8600\n",
      "Epoch: 012, Train Loss: 0.8572, Val Loss: 0.8257\n",
      "Epoch: 013, Train Loss: 0.8441, Val Loss: 0.8115\n",
      "Epoch: 014, Train Loss: 0.8265, Val Loss: 0.7973\n",
      "Epoch: 015, Train Loss: 0.8056, Val Loss: 0.7732\n",
      "Epoch: 016, Train Loss: 0.7827, Val Loss: 0.7467\n",
      "Epoch: 017, Train Loss: 0.7530, Val Loss: 0.7683\n",
      "Epoch: 018, Train Loss: 0.7187, Val Loss: 0.7030\n",
      "Epoch: 019, Train Loss: 0.6899, Val Loss: 0.6789\n",
      "Epoch: 020, Train Loss: 0.6626, Val Loss: 0.6282\n",
      "Epoch: 021, Train Loss: 0.6400, Val Loss: 0.6147\n",
      "Epoch: 022, Train Loss: 0.6238, Val Loss: 0.5981\n",
      "Epoch: 023, Train Loss: 0.6082, Val Loss: 0.5798\n",
      "Epoch: 024, Train Loss: 0.5939, Val Loss: 0.5669\n",
      "Epoch: 025, Train Loss: 0.5844, Val Loss: 0.5666\n",
      "Epoch: 026, Train Loss: 0.5727, Val Loss: 0.5489\n",
      "Epoch: 027, Train Loss: 0.5632, Val Loss: 0.5519\n",
      "Epoch: 028, Train Loss: 0.5550, Val Loss: 0.5327\n",
      "Epoch: 029, Train Loss: 0.5431, Val Loss: 0.5504\n",
      "Epoch: 030, Train Loss: 0.5350, Val Loss: 0.5475\n",
      "Epoch: 031, Train Loss: 0.5233, Val Loss: 0.5077\n",
      "Epoch: 032, Train Loss: 0.5146, Val Loss: 0.4984\n",
      "Epoch: 033, Train Loss: 0.5070, Val Loss: 0.5240\n",
      "Epoch: 034, Train Loss: 0.4998, Val Loss: 0.4800\n",
      "Epoch: 035, Train Loss: 0.4901, Val Loss: 0.4777\n",
      "Epoch: 036, Train Loss: 0.4826, Val Loss: 0.4932\n",
      "Epoch: 037, Train Loss: 0.4781, Val Loss: 0.4953\n",
      "Epoch: 038, Train Loss: 0.4693, Val Loss: 0.4554\n",
      "Epoch: 039, Train Loss: 0.4609, Val Loss: 0.4468\n",
      "Epoch: 040, Train Loss: 0.4569, Val Loss: 0.4664\n",
      "Epoch: 041, Train Loss: 0.4524, Val Loss: 0.4370\n",
      "Epoch: 042, Train Loss: 0.4483, Val Loss: 0.4235\n",
      "Epoch: 043, Train Loss: 0.4438, Val Loss: 0.4312\n",
      "Epoch: 044, Train Loss: 0.4402, Val Loss: 0.4414\n",
      "Epoch: 045, Train Loss: 0.4394, Val Loss: 0.4214\n",
      "Epoch: 046, Train Loss: 0.4328, Val Loss: 0.4154\n",
      "Epoch: 047, Train Loss: 0.4300, Val Loss: 0.4093\n",
      "Epoch: 048, Train Loss: 0.4268, Val Loss: 0.4128\n",
      "Epoch: 049, Train Loss: 0.4240, Val Loss: 0.4185\n",
      "Epoch: 050, Train Loss: 0.4200, Val Loss: 0.4387\n",
      "Epoch: 051, Train Loss: 0.4202, Val Loss: 0.4126\n",
      "Epoch: 052, Train Loss: 0.4172, Val Loss: 0.4133\n",
      "Epoch: 053, Train Loss: 0.4155, Val Loss: 0.4237\n",
      "Epoch: 054, Train Loss: 0.4129, Val Loss: 0.3976\n",
      "Epoch: 055, Train Loss: 0.4089, Val Loss: 0.3948\n",
      "Epoch: 056, Train Loss: 0.4085, Val Loss: 0.3883\n",
      "Epoch: 057, Train Loss: 0.4065, Val Loss: 0.4010\n",
      "Epoch: 058, Train Loss: 0.4056, Val Loss: 0.3957\n",
      "Epoch: 059, Train Loss: 0.4050, Val Loss: 0.3944\n",
      "Epoch: 060, Train Loss: 0.4011, Val Loss: 0.4125\n",
      "Epoch: 061, Train Loss: 0.4018, Val Loss: 0.3882\n",
      "Epoch: 062, Train Loss: 0.4002, Val Loss: 0.3872\n",
      "Epoch: 063, Train Loss: 0.4004, Val Loss: 0.4814\n",
      "Epoch: 064, Train Loss: 0.3962, Val Loss: 0.3811\n",
      "Epoch: 065, Train Loss: 0.3962, Val Loss: 0.3799\n",
      "Epoch: 066, Train Loss: 0.3944, Val Loss: 0.3930\n",
      "Epoch: 067, Train Loss: 0.3936, Val Loss: 0.3817\n",
      "Epoch: 068, Train Loss: 0.3939, Val Loss: 0.3798\n",
      "Epoch: 069, Train Loss: 0.3923, Val Loss: 0.3843\n",
      "Epoch: 070, Train Loss: 0.3899, Val Loss: 0.3776\n",
      "Epoch: 071, Train Loss: 0.3898, Val Loss: 0.3749\n",
      "Epoch: 072, Train Loss: 0.3880, Val Loss: 0.3848\n",
      "Epoch: 073, Train Loss: 0.3891, Val Loss: 0.3974\n",
      "Epoch: 074, Train Loss: 0.3864, Val Loss: 0.3722\n",
      "Epoch: 075, Train Loss: 0.3849, Val Loss: 0.3699\n",
      "Epoch: 076, Train Loss: 0.3838, Val Loss: 0.3810\n",
      "Epoch: 077, Train Loss: 0.3837, Val Loss: 0.3850\n",
      "Epoch: 078, Train Loss: 0.3807, Val Loss: 0.3758\n",
      "Epoch: 079, Train Loss: 0.3806, Val Loss: 0.3725\n",
      "Epoch: 080, Train Loss: 0.3800, Val Loss: 0.3688\n",
      "Epoch: 081, Train Loss: 0.3800, Val Loss: 0.3946\n",
      "Epoch: 082, Train Loss: 0.3784, Val Loss: 0.3645\n",
      "Epoch: 083, Train Loss: 0.3770, Val Loss: 0.3725\n",
      "Epoch: 084, Train Loss: 0.3757, Val Loss: 0.3609\n",
      "Epoch: 085, Train Loss: 0.3754, Val Loss: 0.3644\n",
      "Epoch: 086, Train Loss: 0.3743, Val Loss: 0.3827\n",
      "Epoch: 087, Train Loss: 0.3730, Val Loss: 0.3649\n",
      "Epoch: 088, Train Loss: 0.3733, Val Loss: 0.3941\n",
      "Epoch: 089, Train Loss: 0.3723, Val Loss: 0.3573\n",
      "Epoch: 090, Train Loss: 0.3721, Val Loss: 0.3902\n",
      "Epoch: 091, Train Loss: 0.3714, Val Loss: 0.3763\n",
      "Epoch: 092, Train Loss: 0.3682, Val Loss: 0.3824\n",
      "Epoch: 093, Train Loss: 0.3698, Val Loss: 0.3942\n",
      "Epoch: 094, Train Loss: 0.3668, Val Loss: 0.3655\n",
      "Epoch: 095, Train Loss: 0.3682, Val Loss: 0.3552\n",
      "Epoch: 096, Train Loss: 0.3672, Val Loss: 0.3643\n",
      "Epoch: 097, Train Loss: 0.3656, Val Loss: 0.3722\n",
      "Epoch: 098, Train Loss: 0.3659, Val Loss: 0.3554\n",
      "Epoch: 099, Train Loss: 0.3677, Val Loss: 0.3765\n",
      "Epoch: 100, Train Loss: 0.3658, Val Loss: 0.3529\n",
      "Test Loss: 0.3592\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        # Use the chosen property from the target vector for regression\n",
    "        target = data.y[:, target_index]\n",
    "        loss = F.mse_loss(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data)\n",
    "            target = data.y[:, target_index]\n",
    "            loss = F.mse_loss(out, target, reduction='sum')\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "# Train for 100 epochs and evaluate on the validation set\n",
    "for epoch in range(1, 101):\n",
    "    train_loss = train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "test_loss = evaluate(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://github.com/sw-gong/GNN-Tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
